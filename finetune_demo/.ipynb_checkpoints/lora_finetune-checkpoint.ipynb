{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {},
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-04T03:54:21.892159Z",
     "iopub.status.busy": "2024-04-04T03:54:21.891832Z",
     "iopub.status.idle": "2024-04-04T03:54:22.993853Z",
     "shell.execute_reply": "2024-04-04T03:54:22.993291Z",
     "shell.execute_reply.started": "2024-04-04T03:54:21.892141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {},
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d786e87-bbb1-46b5-9fd7-a0444aa6d176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-04T03:55:46.526139Z",
     "iopub.status.busy": "2024-04-04T03:55:46.525812Z",
     "iopub.status.idle": "2024-04-04T05:51:56.130853Z",
     "shell.execute_reply": "2024-04-04T05:51:56.130161Z",
     "shell.execute_reply.started": "2024-04-04T03:55:46.526119Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04 11:55:52.102672: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-04 11:55:52.105087: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 11:55:52.135802: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-04 11:55:52.135836: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-04 11:55:52.135857: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-04 11:55:52.142066: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 11:55:52.142272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 11:55:53.021529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:39<00:00,  5.64s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 528522.33 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 253310.68 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 391307.46 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:04<00:00, 25447.84 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1805.53 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1908.36 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.7818, 'grad_norm': 2.0437986850738525, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6401, 'grad_norm': 2.470005512237549, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4053, 'grad_norm': 2.6474568843841553, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1253, 'grad_norm': 2.3942553997039795, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9081, 'grad_norm': 2.431609869003296, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9312, 'grad_norm': 2.4616527557373047, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8018, 'grad_norm': 2.4704389572143555, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8112, 'grad_norm': 2.2830305099487305, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6913, 'grad_norm': 2.5309255123138428, 'learning_rate': 4.85e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7, 'grad_norm': 2.624610424041748, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7344, 'grad_norm': 2.7791686058044434, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6973, 'grad_norm': 2.786942958831787, 'learning_rate': 4.8e-05, 'epoch': 0.01}\n",
      "{'loss': 3.664, 'grad_norm': 3.1692004203796387, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.717, 'grad_norm': 3.2141685485839844, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7164, 'grad_norm': 3.2162787914276123, 'learning_rate': 4.75e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6728, 'grad_norm': 3.44077467918396, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5849, 'grad_norm': 3.284376621246338, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6736, 'grad_norm': 3.586770534515381, 'learning_rate': 4.7e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5701, 'grad_norm': 3.4719114303588867, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6275, 'grad_norm': 3.866074323654175, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7234, 'grad_norm': 3.914433002471924, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6544, 'grad_norm': 3.959792137145996, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5479, 'grad_norm': 3.7564408779144287, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6127, 'grad_norm': 4.44600772857666, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6489, 'grad_norm': 4.00662088394165, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5583, 'grad_norm': 4.130444049835205, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6587, 'grad_norm': 4.584769248962402, 'learning_rate': 4.55e-05, 'epoch': 0.02}\n",
      "{'loss': 3.7068, 'grad_norm': 3.7638869285583496, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5588, 'grad_norm': 3.8101017475128174, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5858, 'grad_norm': 4.79115629196167, 'learning_rate': 4.5e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5758, 'grad_norm': 4.43220329284668, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.576, 'grad_norm': 4.2356276512146, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6438, 'grad_norm': 4.062766075134277, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5542, 'grad_norm': 4.264623641967773, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4869, 'grad_norm': 4.390668869018555, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6373, 'grad_norm': 4.734661102294922, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5357, 'grad_norm': 4.415337085723877, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5609, 'grad_norm': 4.316463470458984, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6744, 'grad_norm': 4.860877513885498, 'learning_rate': 4.35e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5596, 'grad_norm': 4.97535514831543, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4413, 'grad_norm': 4.427122116088867, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5594, 'grad_norm': 5.216673374176025, 'learning_rate': 4.3e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6446, 'grad_norm': 5.027074337005615, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5146, 'grad_norm': 4.544394016265869, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4763, 'grad_norm': 5.106007099151611, 'learning_rate': 4.25e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5554, 'grad_norm': 5.283824920654297, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6405, 'grad_norm': 5.621486186981201, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5109, 'grad_norm': 5.212619781494141, 'learning_rate': 4.2e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5523, 'grad_norm': 6.43948221206665, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5474, 'grad_norm': 4.700966835021973, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.03}\n",
      " 17%|██████▎                               | 500/3000 [14:59<1:15:18,  1.81s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.88s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.39s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.63s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.546 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.793948, 'eval_rouge-2': 6.759464, 'eval_rouge-l': 25.103046, 'eval_bleu-4': 0.03242874948270053, 'eval_runtime': 37.4147, 'eval_samples_per_second': 1.336, 'eval_steps_per_second': 0.107, 'epoch': 0.03}\n",
      " 17%|██████▎                               | 500/3000 [15:37<1:15:18,  1.81s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 3.5782, 'grad_norm': 6.37362003326416, 'learning_rate': 4.15e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6479, 'grad_norm': 5.155587673187256, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4893, 'grad_norm': 5.63127326965332, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5515, 'grad_norm': 5.301176071166992, 'learning_rate': 4.1e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6151, 'grad_norm': 5.207632541656494, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5575, 'grad_norm': 6.021974086761475, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5229, 'grad_norm': 5.732546806335449, 'learning_rate': 4.05e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4673, 'grad_norm': 4.991211414337158, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5816, 'grad_norm': 4.936851978302002, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5006, 'grad_norm': 5.026504039764404, 'learning_rate': 4e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4125, 'grad_norm': 5.408780574798584, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.491, 'grad_norm': 5.472633361816406, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4789, 'grad_norm': 5.835902690887451, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.518, 'grad_norm': 5.994723320007324, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5473, 'grad_norm': 5.563441753387451, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5833, 'grad_norm': 5.540884494781494, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4645, 'grad_norm': 5.9696574211120605, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4443, 'grad_norm': 6.031214714050293, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5736, 'grad_norm': 6.734596252441406, 'learning_rate': 3.85e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4997, 'grad_norm': 4.975800037384033, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4266, 'grad_norm': 5.57794189453125, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4871, 'grad_norm': 5.97750186920166, 'learning_rate': 3.8e-05, 'epoch': 0.05}\n",
      "{'loss': 3.446, 'grad_norm': 5.448827743530273, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4858, 'grad_norm': 5.175415992736816, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5652, 'grad_norm': 5.469293594360352, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4854, 'grad_norm': 5.635575294494629, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5538, 'grad_norm': 5.998100757598877, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5647, 'grad_norm': 6.7062554359436035, 'learning_rate': 3.7e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5764, 'grad_norm': 5.698570251464844, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5747, 'grad_norm': 5.371750354766846, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4908, 'grad_norm': 6.082599639892578, 'learning_rate': 3.65e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5125, 'grad_norm': 6.340615749359131, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4506, 'grad_norm': 6.04584264755249, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5438, 'grad_norm': 5.708745956420898, 'learning_rate': 3.6e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5747, 'grad_norm': 5.8314995765686035, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.6017, 'grad_norm': 5.9321489334106445, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5232, 'grad_norm': 5.672604560852051, 'learning_rate': 3.55e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5707, 'grad_norm': 5.658790588378906, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.06}\n",
      "{'loss': 3.472, 'grad_norm': 5.606131076812744, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4833, 'grad_norm': 6.571240425109863, 'learning_rate': 3.5e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4794, 'grad_norm': 5.574256896972656, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5842, 'grad_norm': 6.17440128326416, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5622, 'grad_norm': 5.843172550201416, 'learning_rate': 3.45e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4415, 'grad_norm': 5.847398281097412, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5004, 'grad_norm': 6.0864577293396, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5554, 'grad_norm': 5.770301342010498, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5072, 'grad_norm': 6.061112403869629, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4346, 'grad_norm': 7.271158695220947, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4381, 'grad_norm': 6.1007208824157715, 'learning_rate': 3.35e-05, 'epoch': 0.07}\n",
      "{'loss': 3.565, 'grad_norm': 6.2444748878479, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.07}\n",
      " 33%|█████████████                          | 1000/3000 [30:29<59:02,  1.77s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.75s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.77s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.860394, 'eval_rouge-2': 6.402213999999999, 'eval_rouge-l': 23.996504, 'eval_bleu-4': 0.0314527646530588, 'eval_runtime': 80.4507, 'eval_samples_per_second': 0.621, 'eval_steps_per_second': 0.05, 'epoch': 0.07}\n",
      " 33%|█████████████                          | 1000/3000 [31:50<59:02,  1.77s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:54<00:00, 12.58s/it]\u001b[A\n",
      "{'loss': 3.5603, 'grad_norm': 5.852876663208008, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.6382, 'grad_norm': 6.523838996887207, 'learning_rate': 3.3e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4581, 'grad_norm': 7.634453296661377, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5497, 'grad_norm': 6.366379261016846, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5124, 'grad_norm': 6.124894618988037, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5544, 'grad_norm': 6.307173252105713, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4259, 'grad_norm': 6.254983425140381, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5778, 'grad_norm': 7.2622575759887695, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5274, 'grad_norm': 6.998595237731934, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5126, 'grad_norm': 6.096250057220459, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5449, 'grad_norm': 6.091968536376953, 'learning_rate': 3.15e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5113, 'grad_norm': 6.368655681610107, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4213, 'grad_norm': 6.090677261352539, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5754, 'grad_norm': 7.638160705566406, 'learning_rate': 3.1e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4456, 'grad_norm': 6.127885818481445, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4316, 'grad_norm': 7.065780162811279, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4787, 'grad_norm': 6.065114974975586, 'learning_rate': 3.05e-05, 'epoch': 0.08}\n",
      "{'loss': 3.435, 'grad_norm': 6.409563064575195, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5439, 'grad_norm': 6.229949474334717, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.461, 'grad_norm': 6.093557357788086, 'learning_rate': 3e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4841, 'grad_norm': 6.720696449279785, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4912, 'grad_norm': 6.505699157714844, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4139, 'grad_norm': 6.3939032554626465, 'learning_rate': 2.95e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4677, 'grad_norm': 6.187898635864258, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5565, 'grad_norm': 7.141350269317627, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4363, 'grad_norm': 6.9024434089660645, 'learning_rate': 2.9e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4256, 'grad_norm': 6.519431114196777, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5059, 'grad_norm': 6.369292259216309, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.09}\n",
      "{'loss': 3.6021, 'grad_norm': 6.094115734100342, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5358, 'grad_norm': 6.663750648498535, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5031, 'grad_norm': 6.53918981552124, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.09}\n",
      "{'loss': 3.6039, 'grad_norm': 6.415428161621094, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4988, 'grad_norm': 6.480123996734619, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4478, 'grad_norm': 6.293306350708008, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5773, 'grad_norm': 6.4289021492004395, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4509, 'grad_norm': 6.566956996917725, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5028, 'grad_norm': 6.687005519866943, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5461, 'grad_norm': 6.600275993347168, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4229, 'grad_norm': 6.0643815994262695, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3716, 'grad_norm': 6.57555627822876, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5173, 'grad_norm': 6.529566287994385, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5339, 'grad_norm': 6.452078819274902, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4997, 'grad_norm': 6.801543235778809, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5521, 'grad_norm': 7.534966468811035, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4325, 'grad_norm': 6.621831893920898, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3913, 'grad_norm': 6.163074016571045, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4213, 'grad_norm': 6.48969030380249, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4023, 'grad_norm': 6.87962007522583, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5671, 'grad_norm': 7.949827671051025, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5231, 'grad_norm': 6.449795246124268, 'learning_rate': 2.5e-05, 'epoch': 0.1}\n",
      " 50%|███████████████████▌                   | 1500/3000 [46:43<42:07,  1.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.61s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:11, 11.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.039370000000005, 'eval_rouge-2': 6.736248000000002, 'eval_rouge-l': 23.117626000000005, 'eval_bleu-4': 0.03054829964378878, 'eval_runtime': 74.5937, 'eval_samples_per_second': 0.67, 'eval_steps_per_second': 0.054, 'epoch': 0.1}\n",
      " 50%|███████████████████▌                   | 1500/3000 [47:57<42:07,  1.69s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 14.24s/it]\u001b[A\n",
      "{'loss': 3.5396, 'grad_norm': 6.876465797424316, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4203, 'grad_norm': 6.795841217041016, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5842, 'grad_norm': 6.785796165466309, 'learning_rate': 2.45e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4293, 'grad_norm': 8.048306465148926, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4104, 'grad_norm': 6.0386962890625, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4579, 'grad_norm': 7.7320733070373535, 'learning_rate': 2.4e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4582, 'grad_norm': 8.90543270111084, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.11}\n",
      "{'loss': 3.561, 'grad_norm': 6.902184963226318, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4787, 'grad_norm': 7.361254692077637, 'learning_rate': 2.35e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5132, 'grad_norm': 6.529755115509033, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5057, 'grad_norm': 6.770949840545654, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4723, 'grad_norm': 6.593822956085205, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5119, 'grad_norm': 7.39378023147583, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4726, 'grad_norm': 6.608827590942383, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4747, 'grad_norm': 6.433361530303955, 'learning_rate': 2.25e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4203, 'grad_norm': 7.012224197387695, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.12}\n",
      "{'loss': 3.507, 'grad_norm': 6.413757801055908, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4718, 'grad_norm': 6.558558940887451, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4755, 'grad_norm': 7.035560131072998, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4469, 'grad_norm': 6.41768217086792, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3679, 'grad_norm': 6.996922492980957, 'learning_rate': 2.15e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4468, 'grad_norm': 6.786187171936035, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4487, 'grad_norm': 7.3013224601745605, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3977, 'grad_norm': 8.180988311767578, 'learning_rate': 2.1e-05, 'epoch': 0.12}\n",
      "{'loss': 3.5679, 'grad_norm': 7.8481926918029785, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4847, 'grad_norm': 7.368406295776367, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4282, 'grad_norm': 7.22657585144043, 'learning_rate': 2.05e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4134, 'grad_norm': 7.242148399353027, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4931, 'grad_norm': 9.987663269042969, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3365, 'grad_norm': 7.023622989654541, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5325, 'grad_norm': 6.949524402618408, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4134, 'grad_norm': 7.156066417694092, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3728, 'grad_norm': 7.486799240112305, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4648, 'grad_norm': 6.807130336761475, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5185, 'grad_norm': 7.470948696136475, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4923, 'grad_norm': 7.526678085327148, 'learning_rate': 1.9e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4038, 'grad_norm': 7.334163188934326, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3345, 'grad_norm': 8.02619457244873, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3837, 'grad_norm': 6.837758541107178, 'learning_rate': 1.85e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4902, 'grad_norm': 6.728013515472412, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4003, 'grad_norm': 7.312056541442871, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4744, 'grad_norm': 7.376815319061279, 'learning_rate': 1.8e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4345, 'grad_norm': 7.2866339683532715, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5902, 'grad_norm': 8.195247650146484, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5603, 'grad_norm': 6.802951812744141, 'learning_rate': 1.75e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4197, 'grad_norm': 7.891188621520996, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5648, 'grad_norm': 7.594467639923096, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4376, 'grad_norm': 7.027124881744385, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5386, 'grad_norm': 7.414944171905518, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5446, 'grad_norm': 7.51305627822876, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.14}\n",
      " 67%|████████████████████████▋            | 2000/3000 [1:02:48<28:32,  1.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.28s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.91s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.39069599999999, 'eval_rouge-2': 7.030192, 'eval_rouge-l': 25.301407999999995, 'eval_bleu-4': 0.03543318124431213, 'eval_runtime': 38.116, 'eval_samples_per_second': 1.312, 'eval_steps_per_second': 0.105, 'epoch': 0.14}\n",
      " 67%|████████████████████████▋            | 2000/3000 [1:03:26<28:32,  1.71s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.12s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4336, 'grad_norm': 6.833144664764404, 'learning_rate': 1.65e-05, 'epoch': 0.14}\n",
      "{'loss': 3.534, 'grad_norm': 7.859468460083008, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4066, 'grad_norm': 7.661405563354492, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4143, 'grad_norm': 7.449788570404053, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5146, 'grad_norm': 7.896629810333252, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.14}\n",
      "{'loss': 3.461, 'grad_norm': 7.242011070251465, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4739, 'grad_norm': 6.556521415710449, 'learning_rate': 1.55e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4649, 'grad_norm': 7.916108131408691, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4152, 'grad_norm': 6.925540447235107, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.15}\n",
      "{'loss': 3.5438, 'grad_norm': 6.833171844482422, 'learning_rate': 1.5e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4487, 'grad_norm': 6.8021135330200195, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.15}\n",
      "{'loss': 3.5187, 'grad_norm': 6.466027736663818, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4966, 'grad_norm': 6.668020725250244, 'learning_rate': 1.45e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3283, 'grad_norm': 7.504470348358154, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4692, 'grad_norm': 7.592623710632324, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4131, 'grad_norm': 7.236384391784668, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4035, 'grad_norm': 7.218069553375244, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4103, 'grad_norm': 7.536242485046387, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4956, 'grad_norm': 7.382458209991455, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3686, 'grad_norm': 6.770516395568848, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4253, 'grad_norm': 7.788748264312744, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3932, 'grad_norm': 7.474444389343262, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4272, 'grad_norm': 7.9033308029174805, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5286, 'grad_norm': 7.114306926727295, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4668, 'grad_norm': 7.6436052322387695, 'learning_rate': 1.25e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4433, 'grad_norm': 8.110224723815918, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4842, 'grad_norm': 7.410526752471924, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3833, 'grad_norm': 8.358258247375488, 'learning_rate': 1.2e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4458, 'grad_norm': 7.264891624450684, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4321, 'grad_norm': 7.380011081695557, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4407, 'grad_norm': 7.733465194702148, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4705, 'grad_norm': 8.303135871887207, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 3.483, 'grad_norm': 7.1070427894592285, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5928, 'grad_norm': 7.475180625915527, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5446, 'grad_norm': 7.815522193908691, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4972, 'grad_norm': 7.818418025970459, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4685, 'grad_norm': 7.366806983947754, 'learning_rate': 1.05e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4986, 'grad_norm': 7.465664386749268, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4564, 'grad_norm': 7.4612250328063965, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5267, 'grad_norm': 8.247980117797852, 'learning_rate': 1e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5414, 'grad_norm': 8.415897369384766, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.17}\n",
      "{'loss': 3.3349, 'grad_norm': 7.9090447425842285, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.17}\n",
      "{'loss': 3.3874, 'grad_norm': 7.59743070602417, 'learning_rate': 9.5e-06, 'epoch': 0.17}\n",
      "{'loss': 3.4572, 'grad_norm': 7.77655029296875, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.17}\n",
      "{'loss': 3.4788, 'grad_norm': 7.356769561767578, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.17}\n",
      "{'loss': 3.4441, 'grad_norm': 8.009040832519531, 'learning_rate': 9e-06, 'epoch': 0.17}\n",
      "{'loss': 3.3853, 'grad_norm': 8.015044212341309, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.17}\n",
      "{'loss': 3.4937, 'grad_norm': 7.35114049911499, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.17}\n",
      "{'loss': 3.4251, 'grad_norm': 7.779502868652344, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.17}\n",
      "{'loss': 3.3214, 'grad_norm': 9.70203971862793, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.17}\n",
      " 83%|██████████████████████████████▊      | 2500/3000 [1:18:18<14:18,  1.72s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.76s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.78s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.851698000000006, 'eval_rouge-2': 7.096985999999999, 'eval_rouge-l': 23.826258, 'eval_bleu-4': 0.03176535991207183, 'eval_runtime': 78.8849, 'eval_samples_per_second': 0.634, 'eval_steps_per_second': 0.051, 'epoch': 0.17}\n",
      " 83%|██████████████████████████████▊      | 2500/3000 [1:19:37<14:18,  1.72s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:52<00:00, 11.96s/it]\u001b[A\n",
      "{'loss': 3.4828, 'grad_norm': 7.928711414337158, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4069, 'grad_norm': 8.555063247680664, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4686, 'grad_norm': 8.17979621887207, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.18}\n",
      "{'loss': 3.5096, 'grad_norm': 7.143539905548096, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4832, 'grad_norm': 7.150292873382568, 'learning_rate': 7.5e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4498, 'grad_norm': 7.071715354919434, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.18}\n",
      "{'loss': 3.3767, 'grad_norm': 7.487540245056152, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4327, 'grad_norm': 7.3917131423950195, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4212, 'grad_norm': 7.744691371917725, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.18}\n",
      "{'loss': 3.5056, 'grad_norm': 7.855419158935547, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4912, 'grad_norm': 7.124057292938232, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.18}\n",
      "{'loss': 3.5299, 'grad_norm': 8.347439765930176, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.18}\n",
      "{'loss': 3.3633, 'grad_norm': 8.829699516296387, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.18}\n",
      "{'loss': 3.2628, 'grad_norm': 7.341890335083008, 'learning_rate': 6e-06, 'epoch': 0.18}\n",
      "{'loss': 3.3444, 'grad_norm': 7.324737071990967, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.18}\n",
      "{'loss': 3.418, 'grad_norm': 7.533356666564941, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.19}\n",
      "{'loss': 3.3985, 'grad_norm': 7.306253433227539, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.19}\n",
      "{'loss': 3.3805, 'grad_norm': 7.043768882751465, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4483, 'grad_norm': 7.464650630950928, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.19}\n",
      "{'loss': 3.3701, 'grad_norm': 7.89226770401001, 'learning_rate': 5e-06, 'epoch': 0.19}\n",
      "{'loss': 3.5036, 'grad_norm': 7.650650501251221, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4069, 'grad_norm': 8.11743450164795, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.19}\n",
      "{'loss': 3.5026, 'grad_norm': 8.355544090270996, 'learning_rate': 4.5e-06, 'epoch': 0.19}\n",
      "{'loss': 3.3531, 'grad_norm': 7.54362154006958, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4198, 'grad_norm': 7.276082515716553, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4813, 'grad_norm': 7.467063903808594, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}\n",
      "{'loss': 3.5477, 'grad_norm': 8.305357933044434, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4019, 'grad_norm': 7.575416088104248, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4127, 'grad_norm': 7.630129814147949, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.19}\n",
      "{'loss': 3.4355, 'grad_norm': 7.679064750671387, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4301, 'grad_norm': 8.939208984375, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4721, 'grad_norm': 7.8377366065979, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 3.5225, 'grad_norm': 7.257197856903076, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4829, 'grad_norm': 8.730218887329102, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4326, 'grad_norm': 8.39306926727295, 'learning_rate': 2.5e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4766, 'grad_norm': 7.98246431350708, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4879, 'grad_norm': 7.323619842529297, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.2}\n",
      "{'loss': 3.5094, 'grad_norm': 7.659107685089111, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 3.4156, 'grad_norm': 8.470523834228516, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.2}\n",
      "{'loss': 3.5321, 'grad_norm': 7.983297824859619, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3813, 'grad_norm': 7.912066459655762, 'learning_rate': 1.5e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3861, 'grad_norm': 9.201478958129883, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3533, 'grad_norm': 7.017385005950928, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.2}\n",
      "{'loss': 3.3713, 'grad_norm': 7.204711437225342, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3653, 'grad_norm': 7.3236165046691895, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.21}\n",
      "{'loss': 3.6042, 'grad_norm': 7.810524940490723, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.21}\n",
      "{'loss': 3.496, 'grad_norm': 7.358755111694336, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.21}\n",
      "{'loss': 3.5481, 'grad_norm': 8.93406867980957, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.21}\n",
      "{'loss': 3.486, 'grad_norm': 7.5468668937683105, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.21}\n",
      "{'loss': 3.506, 'grad_norm': 7.4330735206604, 'learning_rate': 0.0, 'epoch': 0.21}\n",
      "100%|█████████████████████████████████████| 3000/3000 [1:34:37<00:00,  1.85s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             {'eval_rouge-1': 33.119144, 'eval_rouge-2': 7.501474000000001, 'eval_rouge-l': 25.227052, 'eval_bleu-4': 0.03582971228187696, 'eval_runtime': 37.0415, 'eval_samples_per_second': 1.35, 'eval_steps_per_second': 0.108, 'epoch': 0.21}\n",
      "100%|█████████████████████████████████████| 3000/3000 [1:35:14<00:00,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.73s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5715.109, 'train_samples_per_second': 4.199, 'train_steps_per_second': 0.525, 'train_loss': 3.5137314453125, 'epoch': 0.21}\n",
      "100%|█████████████████████████████████████| 3000/3000 [1:35:15<00:00,  1.91s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [19:19<00:00, 17.30s/it]\n"
     ]
    }
   ],
   "source": [
    "!python finetune_hf.py  data/AdvertiseGen_fix ../../chatglm3-6b configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98af6059-3275-476a-929e-b995c6b8abf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T06:26:22.477607Z",
     "iopub.status.busy": "2024-04-04T06:26:22.477264Z",
     "iopub.status.idle": "2024-04-04T17:11:36.789684Z",
     "shell.execute_reply": "2024-04-04T17:11:36.788837Z",
     "shell.execute_reply.started": "2024-04-04T06:26:22.477588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04 14:26:29.696509: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-04 14:26:29.709869: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 14:26:29.745771: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-04 14:26:29.745823: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-04 14:26:29.745852: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-04 14:26:29.868899: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-04 14:26:29.869168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 14:26:30.787478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:14<00:00,  2.10s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 20,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.782, 'grad_norm': 2.1204299926757812, 'learning_rate': 4.9975e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6372, 'grad_norm': 2.480602264404297, 'learning_rate': 4.995e-05, 'epoch': 0.0}\n",
      "{'loss': 4.3969, 'grad_norm': 2.7176406383514404, 'learning_rate': 4.992500000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1137, 'grad_norm': 2.4353606700897217, 'learning_rate': 4.99e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8987, 'grad_norm': 2.4388978481292725, 'learning_rate': 4.9875000000000006e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9195, 'grad_norm': 2.4612107276916504, 'learning_rate': 4.9850000000000006e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7975, 'grad_norm': 2.509828567504883, 'learning_rate': 4.9825000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8057, 'grad_norm': 2.3183088302612305, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6892, 'grad_norm': 2.530475616455078, 'learning_rate': 4.9775000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6988, 'grad_norm': 2.6621177196502686, 'learning_rate': 4.975e-05, 'epoch': 0.01}\n",
      "{'loss': 3.73, 'grad_norm': 2.8096985816955566, 'learning_rate': 4.9725e-05, 'epoch': 0.01}\n",
      "{'loss': 3.696, 'grad_norm': 2.818150043487549, 'learning_rate': 4.97e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6629, 'grad_norm': 3.2313499450683594, 'learning_rate': 4.967500000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 3.71, 'grad_norm': 3.285785436630249, 'learning_rate': 4.965e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7096, 'grad_norm': 3.268613338470459, 'learning_rate': 4.962500000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6688, 'grad_norm': 3.476339817047119, 'learning_rate': 4.96e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5799, 'grad_norm': 3.3597824573516846, 'learning_rate': 4.9575000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.667, 'grad_norm': 3.6185667514801025, 'learning_rate': 4.9550000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.567, 'grad_norm': 3.545269250869751, 'learning_rate': 4.9525000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6227, 'grad_norm': 3.895204544067383, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7165, 'grad_norm': 3.9551072120666504, 'learning_rate': 4.9475e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6503, 'grad_norm': 4.017728328704834, 'learning_rate': 4.945e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5459, 'grad_norm': 3.8832695484161377, 'learning_rate': 4.9425e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6076, 'grad_norm': 4.482609748840332, 'learning_rate': 4.94e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6431, 'grad_norm': 4.067645072937012, 'learning_rate': 4.937500000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5513, 'grad_norm': 4.10300350189209, 'learning_rate': 4.935e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6532, 'grad_norm': 4.511022567749023, 'learning_rate': 4.9325000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.7016, 'grad_norm': 3.7989981174468994, 'learning_rate': 4.93e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5568, 'grad_norm': 3.7975053787231445, 'learning_rate': 4.9275000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5816, 'grad_norm': 5.002889156341553, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5753, 'grad_norm': 4.581648826599121, 'learning_rate': 4.9225000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5718, 'grad_norm': 4.384933948516846, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6412, 'grad_norm': 4.150382041931152, 'learning_rate': 4.9175e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5501, 'grad_norm': 4.39354133605957, 'learning_rate': 4.915e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4856, 'grad_norm': 4.508529186248779, 'learning_rate': 4.9125e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6354, 'grad_norm': 4.910836696624756, 'learning_rate': 4.91e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5318, 'grad_norm': 4.565152168273926, 'learning_rate': 4.907500000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5576, 'grad_norm': 4.469936370849609, 'learning_rate': 4.905e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6717, 'grad_norm': 4.9243597984313965, 'learning_rate': 4.9025000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5581, 'grad_norm': 5.073676586151123, 'learning_rate': 4.9e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4375, 'grad_norm': 4.506387233734131, 'learning_rate': 4.8975000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.552, 'grad_norm': 5.265570163726807, 'learning_rate': 4.8950000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6401, 'grad_norm': 5.181767463684082, 'learning_rate': 4.8925e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5115, 'grad_norm': 4.63200569152832, 'learning_rate': 4.89e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4751, 'grad_norm': 5.245683670043945, 'learning_rate': 4.8875e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5539, 'grad_norm': 5.3010406494140625, 'learning_rate': 4.885e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6368, 'grad_norm': 5.644524097442627, 'learning_rate': 4.8825e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5078, 'grad_norm': 5.282111644744873, 'learning_rate': 4.88e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5487, 'grad_norm': 6.296285152435303, 'learning_rate': 4.8775000000000007e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5438, 'grad_norm': 4.743957996368408, 'learning_rate': 4.875e-05, 'epoch': 0.03}\n",
      "  2%|▉                                    | 500/20000 [15:01<9:48:15,  1.81s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.88s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.50s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.46s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.619 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.817312, 'eval_rouge-2': 6.511339999999999, 'eval_rouge-l': 25.322345999999996, 'eval_bleu-4': 0.03498835139362386, 'eval_runtime': 14.5298, 'eval_samples_per_second': 3.441, 'eval_steps_per_second': 0.275, 'epoch': 0.03}\n",
      "  2%|▉                                    | 500/20000 [15:15<9:48:15,  1.81s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.46s/it]\u001b[A\n",
      "{'loss': 3.5739, 'grad_norm': 6.439197063446045, 'learning_rate': 4.8725000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6448, 'grad_norm': 5.10688591003418, 'learning_rate': 4.87e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4914, 'grad_norm': 5.641813278198242, 'learning_rate': 4.8675000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5445, 'grad_norm': 5.201046943664551, 'learning_rate': 4.8650000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6159, 'grad_norm': 5.186396598815918, 'learning_rate': 4.8625e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5551, 'grad_norm': 6.15217399597168, 'learning_rate': 4.86e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5142, 'grad_norm': 5.848851203918457, 'learning_rate': 4.8575e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4648, 'grad_norm': 5.07790470123291, 'learning_rate': 4.855e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5766, 'grad_norm': 5.05804967880249, 'learning_rate': 4.8525e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4968, 'grad_norm': 5.14670467376709, 'learning_rate': 4.85e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4134, 'grad_norm': 5.377557277679443, 'learning_rate': 4.8475000000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4829, 'grad_norm': 5.473511695861816, 'learning_rate': 4.845e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4742, 'grad_norm': 5.948910236358643, 'learning_rate': 4.8425000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5136, 'grad_norm': 6.37531042098999, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5438, 'grad_norm': 5.577256202697754, 'learning_rate': 4.8375000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5864, 'grad_norm': 5.536460876464844, 'learning_rate': 4.835e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4545, 'grad_norm': 6.024771690368652, 'learning_rate': 4.8325e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4458, 'grad_norm': 6.055814743041992, 'learning_rate': 4.83e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5704, 'grad_norm': 7.076323509216309, 'learning_rate': 4.8275e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4998, 'grad_norm': 4.867742538452148, 'learning_rate': 4.825e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4253, 'grad_norm': 5.614609718322754, 'learning_rate': 4.822500000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4838, 'grad_norm': 5.7970757484436035, 'learning_rate': 4.82e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4379, 'grad_norm': 5.483625411987305, 'learning_rate': 4.8175000000000005e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4843, 'grad_norm': 5.442440986633301, 'learning_rate': 4.815e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5634, 'grad_norm': 5.463184833526611, 'learning_rate': 4.8125000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4783, 'grad_norm': 5.744521617889404, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5478, 'grad_norm': 6.044450283050537, 'learning_rate': 4.8075e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5617, 'grad_norm': 6.474924087524414, 'learning_rate': 4.805e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5731, 'grad_norm': 5.796173095703125, 'learning_rate': 4.8025e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5716, 'grad_norm': 5.458268642425537, 'learning_rate': 4.8e-05, 'epoch': 0.06}\n",
      "{'loss': 3.482, 'grad_norm': 5.918163299560547, 'learning_rate': 4.7975e-05, 'epoch': 0.06}\n",
      "{'loss': 3.506, 'grad_norm': 6.280702590942383, 'learning_rate': 4.795e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4512, 'grad_norm': 6.126393795013428, 'learning_rate': 4.7925000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5439, 'grad_norm': 5.743447780609131, 'learning_rate': 4.79e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5706, 'grad_norm': 5.833982944488525, 'learning_rate': 4.7875000000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5974, 'grad_norm': 5.752176284790039, 'learning_rate': 4.785e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5236, 'grad_norm': 5.76030158996582, 'learning_rate': 4.7825000000000004e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5688, 'grad_norm': 5.6488356590271, 'learning_rate': 4.78e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4745, 'grad_norm': 5.620138168334961, 'learning_rate': 4.7775e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4904, 'grad_norm': 6.816155433654785, 'learning_rate': 4.775e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4708, 'grad_norm': 5.535476207733154, 'learning_rate': 4.7725e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5816, 'grad_norm': 6.384890079498291, 'learning_rate': 4.77e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5564, 'grad_norm': 5.62965726852417, 'learning_rate': 4.7675e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4388, 'grad_norm': 5.781065464019775, 'learning_rate': 4.765e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4994, 'grad_norm': 6.063292980194092, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5477, 'grad_norm': 5.781582355499268, 'learning_rate': 4.76e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5051, 'grad_norm': 6.2390265464782715, 'learning_rate': 4.7575000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4318, 'grad_norm': 7.021505355834961, 'learning_rate': 4.755e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4268, 'grad_norm': 6.04427433013916, 'learning_rate': 4.7525e-05, 'epoch': 0.07}\n",
      "{'loss': 3.562, 'grad_norm': 6.379775047302246, 'learning_rate': 4.75e-05, 'epoch': 0.07}\n",
      "  5%|█▊                                  | 1000/20000 [30:09<9:20:44,  1.77s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.136135999999997, 'eval_rouge-2': 6.266456, 'eval_rouge-l': 21.835322, 'eval_bleu-4': 0.02846895845753463, 'eval_runtime': 95.2278, 'eval_samples_per_second': 0.525, 'eval_steps_per_second': 0.042, 'epoch': 0.07}\n",
      "  5%|█▊                                  | 1000/20000 [31:44<9:20:44,  1.77s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:08<00:00, 18.00s/it]\u001b[A\n",
      "{'loss': 3.5604, 'grad_norm': 5.91240930557251, 'learning_rate': 4.7475e-05, 'epoch': 0.07}\n",
      "{'loss': 3.644, 'grad_norm': 6.550025463104248, 'learning_rate': 4.745e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4573, 'grad_norm': 7.2451491355896, 'learning_rate': 4.7425e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5499, 'grad_norm': 6.161859035491943, 'learning_rate': 4.74e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5049, 'grad_norm': 5.986932277679443, 'learning_rate': 4.7375e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5574, 'grad_norm': 6.226944923400879, 'learning_rate': 4.735e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4238, 'grad_norm': 6.294554710388184, 'learning_rate': 4.7325000000000005e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5757, 'grad_norm': 7.3968329429626465, 'learning_rate': 4.73e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5237, 'grad_norm': 6.803350448608398, 'learning_rate': 4.7275000000000004e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5098, 'grad_norm': 6.178763389587402, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5447, 'grad_norm': 6.1669511795043945, 'learning_rate': 4.7225e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5049, 'grad_norm': 6.26861047744751, 'learning_rate': 4.72e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4182, 'grad_norm': 6.062897682189941, 'learning_rate': 4.7175e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5707, 'grad_norm': 7.040592670440674, 'learning_rate': 4.715e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4444, 'grad_norm': 5.937472820281982, 'learning_rate': 4.7125e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4362, 'grad_norm': 7.035297870635986, 'learning_rate': 4.71e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4708, 'grad_norm': 5.979449272155762, 'learning_rate': 4.7075e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4381, 'grad_norm': 6.728111743927002, 'learning_rate': 4.705e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5482, 'grad_norm': 6.3097381591796875, 'learning_rate': 4.7025000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.449, 'grad_norm': 5.803990840911865, 'learning_rate': 4.7e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4862, 'grad_norm': 6.29902458190918, 'learning_rate': 4.6975000000000003e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4883, 'grad_norm': 6.561037540435791, 'learning_rate': 4.695e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4064, 'grad_norm': 6.0955328941345215, 'learning_rate': 4.6925e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4722, 'grad_norm': 6.154017925262451, 'learning_rate': 4.69e-05, 'epoch': 0.09}\n",
      "{'loss': 3.554, 'grad_norm': 7.029185771942139, 'learning_rate': 4.6875e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4233, 'grad_norm': 6.707267761230469, 'learning_rate': 4.685000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4219, 'grad_norm': 6.933850288391113, 'learning_rate': 4.6825e-05, 'epoch': 0.09}\n",
      "{'loss': 3.507, 'grad_norm': 6.459427833557129, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5965, 'grad_norm': 6.103348731994629, 'learning_rate': 4.6775000000000005e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5397, 'grad_norm': 6.607172012329102, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4963, 'grad_norm': 6.2446465492248535, 'learning_rate': 4.6725000000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 3.6099, 'grad_norm': 6.388039588928223, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4897, 'grad_norm': 6.369032382965088, 'learning_rate': 4.6675e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4469, 'grad_norm': 6.149190902709961, 'learning_rate': 4.665e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5783, 'grad_norm': 6.577157020568848, 'learning_rate': 4.6625e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4518, 'grad_norm': 6.4019060134887695, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 3.499, 'grad_norm': 6.767665386199951, 'learning_rate': 4.6575e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5396, 'grad_norm': 6.69582986831665, 'learning_rate': 4.655000000000001e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4229, 'grad_norm': 6.09252405166626, 'learning_rate': 4.6525e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3762, 'grad_norm': 6.759308815002441, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5101, 'grad_norm': 6.451384544372559, 'learning_rate': 4.6475000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5354, 'grad_norm': 6.543117046356201, 'learning_rate': 4.6450000000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4925, 'grad_norm': 6.810155868530273, 'learning_rate': 4.6425000000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5532, 'grad_norm': 7.361645698547363, 'learning_rate': 4.64e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4336, 'grad_norm': 6.637885093688965, 'learning_rate': 4.6375e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3817, 'grad_norm': 6.221179485321045, 'learning_rate': 4.635e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4252, 'grad_norm': 6.633926868438721, 'learning_rate': 4.6325e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4016, 'grad_norm': 6.9073920249938965, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5671, 'grad_norm': 7.652199745178223, 'learning_rate': 4.6275e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5249, 'grad_norm': 5.912661075592041, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.1}\n",
      "  8%|██▋                                 | 1500/20000 [46:39<8:40:20,  1.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.528505999999997, 'eval_rouge-2': 6.618673999999999, 'eval_rouge-l': 21.911162, 'eval_bleu-4': 0.028974387368174634, 'eval_runtime': 95.2286, 'eval_samples_per_second': 0.525, 'eval_steps_per_second': 0.042, 'epoch': 0.1}\n",
      "  8%|██▋                                 | 1500/20000 [48:14<8:40:20,  1.69s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:08<00:00, 17.99s/it]\u001b[A\n",
      "{'loss': 3.5366, 'grad_norm': 6.872135162353516, 'learning_rate': 4.6225e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4083, 'grad_norm': 6.919486999511719, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5915, 'grad_norm': 6.582271099090576, 'learning_rate': 4.6175000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4208, 'grad_norm': 7.805014133453369, 'learning_rate': 4.6150000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4165, 'grad_norm': 6.100283145904541, 'learning_rate': 4.6125e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4484, 'grad_norm': 7.5095038414001465, 'learning_rate': 4.61e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4608, 'grad_norm': 8.49410629272461, 'learning_rate': 4.6075e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5706, 'grad_norm': 6.69224214553833, 'learning_rate': 4.605e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4732, 'grad_norm': 6.796002388000488, 'learning_rate': 4.6025e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5122, 'grad_norm': 6.446697235107422, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5041, 'grad_norm': 6.799267768859863, 'learning_rate': 4.5975e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4724, 'grad_norm': 6.626651763916016, 'learning_rate': 4.5950000000000006e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5073, 'grad_norm': 7.4156575202941895, 'learning_rate': 4.5925e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4694, 'grad_norm': 6.156412601470947, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4711, 'grad_norm': 6.090411186218262, 'learning_rate': 4.5875000000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4134, 'grad_norm': 6.99143123626709, 'learning_rate': 4.585e-05, 'epoch': 0.12}\n",
      "{'loss': 3.5062, 'grad_norm': 6.332850933074951, 'learning_rate': 4.5825e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4809, 'grad_norm': 6.558451175689697, 'learning_rate': 4.58e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4694, 'grad_norm': 6.578976631164551, 'learning_rate': 4.5775e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4435, 'grad_norm': 6.248669147491455, 'learning_rate': 4.575e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3665, 'grad_norm': 7.5487589836120605, 'learning_rate': 4.5725e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4454, 'grad_norm': 6.75459098815918, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4532, 'grad_norm': 6.942879676818848, 'learning_rate': 4.5675e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3929, 'grad_norm': 7.989406108856201, 'learning_rate': 4.5650000000000005e-05, 'epoch': 0.12}\n",
      "{'loss': 3.5587, 'grad_norm': 7.573115825653076, 'learning_rate': 4.5625e-05, 'epoch': 0.12}\n",
      "{'loss': 3.482, 'grad_norm': 7.2356486320495605, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4196, 'grad_norm': 7.203944683074951, 'learning_rate': 4.5575e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4082, 'grad_norm': 6.869022846221924, 'learning_rate': 4.555e-05, 'epoch': 0.12}\n",
      "{'loss': 3.5028, 'grad_norm': 8.070042610168457, 'learning_rate': 4.5525e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3308, 'grad_norm': 6.453763008117676, 'learning_rate': 4.55e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5292, 'grad_norm': 6.956363201141357, 'learning_rate': 4.5475e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4083, 'grad_norm': 6.737707614898682, 'learning_rate': 4.545000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3756, 'grad_norm': 7.0556230545043945, 'learning_rate': 4.5425e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4608, 'grad_norm': 6.62035608291626, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5142, 'grad_norm': 7.315000534057617, 'learning_rate': 4.5375e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4864, 'grad_norm': 7.43198823928833, 'learning_rate': 4.5350000000000005e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4122, 'grad_norm': 7.338084697723389, 'learning_rate': 4.5325000000000004e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3282, 'grad_norm': 7.738790035247803, 'learning_rate': 4.53e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3745, 'grad_norm': 6.609130382537842, 'learning_rate': 4.5275e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4924, 'grad_norm': 6.797285079956055, 'learning_rate': 4.525e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3995, 'grad_norm': 6.941409587860107, 'learning_rate': 4.5225e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4726, 'grad_norm': 6.802133560180664, 'learning_rate': 4.52e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4305, 'grad_norm': 6.969637393951416, 'learning_rate': 4.5175e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5948, 'grad_norm': 6.941435813903809, 'learning_rate': 4.5150000000000006e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5486, 'grad_norm': 6.510318279266357, 'learning_rate': 4.5125e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4189, 'grad_norm': 7.550617694854736, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5682, 'grad_norm': 6.953962326049805, 'learning_rate': 4.5075e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4315, 'grad_norm': 6.8527302742004395, 'learning_rate': 4.5050000000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5315, 'grad_norm': 7.1190290451049805, 'learning_rate': 4.5025000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5366, 'grad_norm': 7.435640811920166, 'learning_rate': 4.5e-05, 'epoch': 0.14}\n",
      " 10%|███▍                              | 2000/20000 [1:03:06<8:34:44,  1.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:28<00:08,  8.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.717793999999998, 'eval_rouge-2': 6.7762139999999995, 'eval_rouge-l': 25.046156, 'eval_bleu-4': 0.031812746544877736, 'eval_runtime': 58.7778, 'eval_samples_per_second': 0.851, 'eval_steps_per_second': 0.068, 'epoch': 0.14}\n",
      " 10%|███▍                              | 2000/20000 [1:04:05<8:34:44,  1.72s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.83s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4202, 'grad_norm': 6.919673442840576, 'learning_rate': 4.4975e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5403, 'grad_norm': 7.982364654541016, 'learning_rate': 4.495e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4052, 'grad_norm': 7.349168300628662, 'learning_rate': 4.4925e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4049, 'grad_norm': 7.810097694396973, 'learning_rate': 4.49e-05, 'epoch': 0.14}\n",
      "{'loss': 3.5044, 'grad_norm': 7.2665886878967285, 'learning_rate': 4.4875e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4678, 'grad_norm': 7.0645833015441895, 'learning_rate': 4.4850000000000006e-05, 'epoch': 0.14}\n",
      "{'loss': 3.464, 'grad_norm': 6.317969799041748, 'learning_rate': 4.4825e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4664, 'grad_norm': 7.24451208114624, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4062, 'grad_norm': 6.501794338226318, 'learning_rate': 4.4775e-05, 'epoch': 0.15}\n",
      "{'loss': 3.5385, 'grad_norm': 6.545888900756836, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4472, 'grad_norm': 6.770245552062988, 'learning_rate': 4.4725e-05, 'epoch': 0.15}\n",
      "{'loss': 3.5055, 'grad_norm': 6.324234485626221, 'learning_rate': 4.47e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4976, 'grad_norm': 6.746654510498047, 'learning_rate': 4.4675e-05, 'epoch': 0.15}\n",
      "{'loss': 3.326, 'grad_norm': 7.283712387084961, 'learning_rate': 4.465e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4706, 'grad_norm': 7.684628963470459, 'learning_rate': 4.4625e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4075, 'grad_norm': 7.360445976257324, 'learning_rate': 4.46e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4131, 'grad_norm': 6.754322528839111, 'learning_rate': 4.4575e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4091, 'grad_norm': 6.968858242034912, 'learning_rate': 4.4550000000000005e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4911, 'grad_norm': 6.901885986328125, 'learning_rate': 4.4525e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3768, 'grad_norm': 6.344217777252197, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4344, 'grad_norm': 7.602787494659424, 'learning_rate': 4.4475e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3967, 'grad_norm': 7.112104892730713, 'learning_rate': 4.445e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4234, 'grad_norm': 7.64246129989624, 'learning_rate': 4.4425e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5247, 'grad_norm': 6.73600435256958, 'learning_rate': 4.44e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4631, 'grad_norm': 7.074652194976807, 'learning_rate': 4.4375e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4527, 'grad_norm': 7.937557220458984, 'learning_rate': 4.435e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4792, 'grad_norm': 6.823251724243164, 'learning_rate': 4.4325e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3854, 'grad_norm': 8.154444694519043, 'learning_rate': 4.43e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4604, 'grad_norm': 6.919170379638672, 'learning_rate': 4.4275e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4264, 'grad_norm': 6.725033283233643, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4343, 'grad_norm': 7.535030364990234, 'learning_rate': 4.4225e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4729, 'grad_norm': 7.870820045471191, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4721, 'grad_norm': 7.370500087738037, 'learning_rate': 4.4174999999999996e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5919, 'grad_norm': 6.834732532501221, 'learning_rate': 4.415e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5472, 'grad_norm': 7.561237335205078, 'learning_rate': 4.4125e-05, 'epoch': 0.16}\n",
      "{'loss': 3.505, 'grad_norm': 7.563777446746826, 'learning_rate': 4.41e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4617, 'grad_norm': 7.083509922027588, 'learning_rate': 4.4075e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4864, 'grad_norm': 7.099149227142334, 'learning_rate': 4.405e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4521, 'grad_norm': 6.970762252807617, 'learning_rate': 4.4025e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5167, 'grad_norm': 7.284651756286621, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5376, 'grad_norm': 8.154348373413086, 'learning_rate': 4.3975e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3233, 'grad_norm': 7.252498626708984, 'learning_rate': 4.3950000000000004e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3898, 'grad_norm': 7.185070514678955, 'learning_rate': 4.3925e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4437, 'grad_norm': 6.555518627166748, 'learning_rate': 4.39e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4763, 'grad_norm': 6.671214580535889, 'learning_rate': 4.3875e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4459, 'grad_norm': 7.543887138366699, 'learning_rate': 4.385e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3752, 'grad_norm': 7.3382086753845215, 'learning_rate': 4.3825e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4841, 'grad_norm': 6.608920574188232, 'learning_rate': 4.38e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4229, 'grad_norm': 6.949357032775879, 'learning_rate': 4.3775e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3073, 'grad_norm': 8.384902000427246, 'learning_rate': 4.375e-05, 'epoch': 0.17}\n",
      " 12%|████▎                             | 2500/20000 [1:18:58<8:22:15,  1.72s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.329946, 'eval_rouge-2': 6.39887, 'eval_rouge-l': 22.997734, 'eval_bleu-4': 0.03011133678822916, 'eval_runtime': 79.4849, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.05, 'epoch': 0.17}\n",
      " 12%|████▎                             | 2500/20000 [1:20:18<8:22:15,  1.72s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.13s/it]\u001b[A\n",
      "{'loss': 3.478, 'grad_norm': 7.279073238372803, 'learning_rate': 4.3725000000000006e-05, 'epoch': 0.18}\n",
      "{'loss': 3.413, 'grad_norm': 8.064878463745117, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4561, 'grad_norm': 7.839511871337891, 'learning_rate': 4.3675000000000005e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4943, 'grad_norm': 6.841652870178223, 'learning_rate': 4.3650000000000004e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4916, 'grad_norm': 6.87311315536499, 'learning_rate': 4.3625e-05, 'epoch': 0.18}\n",
      "{'loss': 3.441, 'grad_norm': 6.8257927894592285, 'learning_rate': 4.36e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3664, 'grad_norm': 7.316271781921387, 'learning_rate': 4.3575e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4287, 'grad_norm': 7.5564284324646, 'learning_rate': 4.355e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4187, 'grad_norm': 7.164698600769043, 'learning_rate': 4.352500000000001e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4953, 'grad_norm': 6.793464183807373, 'learning_rate': 4.35e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4879, 'grad_norm': 6.815323352813721, 'learning_rate': 4.3475000000000006e-05, 'epoch': 0.18}\n",
      "{'loss': 3.5114, 'grad_norm': 7.3381428718566895, 'learning_rate': 4.345e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3588, 'grad_norm': 7.784074783325195, 'learning_rate': 4.3425000000000005e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2484, 'grad_norm': 7.273670196533203, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3397, 'grad_norm': 6.805823802947998, 'learning_rate': 4.3375000000000004e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4112, 'grad_norm': 6.993881702423096, 'learning_rate': 4.335e-05, 'epoch': 0.19}\n",
      "{'loss': 3.377, 'grad_norm': 6.817543983459473, 'learning_rate': 4.3325e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3671, 'grad_norm': 6.535529136657715, 'learning_rate': 4.33e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4521, 'grad_norm': 7.486639499664307, 'learning_rate': 4.3275e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3682, 'grad_norm': 7.107226371765137, 'learning_rate': 4.325e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4858, 'grad_norm': 7.165095329284668, 'learning_rate': 4.322500000000001e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3906, 'grad_norm': 7.243063449859619, 'learning_rate': 4.32e-05, 'epoch': 0.19}\n",
      "{'loss': 3.5035, 'grad_norm': 9.828459739685059, 'learning_rate': 4.3175000000000006e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3429, 'grad_norm': 7.254584789276123, 'learning_rate': 4.315e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4154, 'grad_norm': 6.681764602661133, 'learning_rate': 4.3125000000000005e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4743, 'grad_norm': 7.062891483306885, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.19}\n",
      "{'loss': 3.5286, 'grad_norm': 7.730734348297119, 'learning_rate': 4.3075000000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3854, 'grad_norm': 7.411584854125977, 'learning_rate': 4.305e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4024, 'grad_norm': 7.588590621948242, 'learning_rate': 4.3025e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4312, 'grad_norm': 7.277734756469727, 'learning_rate': 4.3e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4177, 'grad_norm': 9.194900512695312, 'learning_rate': 4.2975e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4549, 'grad_norm': 7.497389316558838, 'learning_rate': 4.295e-05, 'epoch': 0.2}\n",
      "{'loss': 3.5053, 'grad_norm': 7.250667095184326, 'learning_rate': 4.2925000000000007e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4679, 'grad_norm': 7.333057403564453, 'learning_rate': 4.29e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4286, 'grad_norm': 8.652995109558105, 'learning_rate': 4.2875000000000005e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4742, 'grad_norm': 7.642717361450195, 'learning_rate': 4.285e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4793, 'grad_norm': 7.3031840324401855, 'learning_rate': 4.2825000000000004e-05, 'epoch': 0.2}\n",
      "{'loss': 3.5057, 'grad_norm': 7.185006141662598, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4032, 'grad_norm': 7.9919843673706055, 'learning_rate': 4.2775e-05, 'epoch': 0.2}\n",
      "{'loss': 3.5113, 'grad_norm': 7.4869585037231445, 'learning_rate': 4.275e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3608, 'grad_norm': 7.616255283355713, 'learning_rate': 4.2725e-05, 'epoch': 0.2}\n",
      "{'loss': 3.375, 'grad_norm': 8.890353202819824, 'learning_rate': 4.27e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3553, 'grad_norm': 6.647554397583008, 'learning_rate': 4.2675e-05, 'epoch': 0.2}\n",
      "{'loss': 3.359, 'grad_norm': 6.789359092712402, 'learning_rate': 4.265e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3401, 'grad_norm': 7.098024845123291, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.21}\n",
      "{'loss': 3.5904, 'grad_norm': 7.320459842681885, 'learning_rate': 4.26e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4798, 'grad_norm': 7.0262885093688965, 'learning_rate': 4.2575000000000005e-05, 'epoch': 0.21}\n",
      "{'loss': 3.5455, 'grad_norm': 8.77715015411377, 'learning_rate': 4.2550000000000004e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4844, 'grad_norm': 7.30494499206543, 'learning_rate': 4.2525000000000004e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4972, 'grad_norm': 7.395715236663818, 'learning_rate': 4.25e-05, 'epoch': 0.21}\n",
      " 15%|█████                             | 3000/20000 [1:35:19<8:44:54,  1.85s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:08<00:08,  4.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:33<00:12, 12.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.321416, 'eval_rouge-2': 7.126091999999999, 'eval_rouge-l': 25.212464, 'eval_bleu-4': 0.03589140605942784, 'eval_runtime': 62.5694, 'eval_samples_per_second': 0.799, 'eval_steps_per_second': 0.064, 'epoch': 0.21}\n",
      " 15%|█████                             | 3000/20000 [1:36:22<8:44:54,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:36<00:00,  9.06s/it]\u001b[A\n",
      "{'loss': 3.3935, 'grad_norm': 7.230053424835205, 'learning_rate': 4.2475e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3027, 'grad_norm': 7.769696235656738, 'learning_rate': 4.245e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4412, 'grad_norm': 8.1829195022583, 'learning_rate': 4.2425e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4567, 'grad_norm': 8.016676902770996, 'learning_rate': 4.24e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4675, 'grad_norm': 7.290591716766357, 'learning_rate': 4.237500000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3944, 'grad_norm': 8.541641235351562, 'learning_rate': 4.235e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3815, 'grad_norm': 6.883636951446533, 'learning_rate': 4.2325000000000006e-05, 'epoch': 0.21}\n",
      "{'loss': 3.5196, 'grad_norm': 7.355725288391113, 'learning_rate': 4.23e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4559, 'grad_norm': 7.827169895172119, 'learning_rate': 4.2275000000000004e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3665, 'grad_norm': 7.220537185668945, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4021, 'grad_norm': 8.069292068481445, 'learning_rate': 4.2225e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4643, 'grad_norm': 6.642880916595459, 'learning_rate': 4.22e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4402, 'grad_norm': 8.26063346862793, 'learning_rate': 4.2175e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3782, 'grad_norm': 7.04740047454834, 'learning_rate': 4.215e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4267, 'grad_norm': 7.653355121612549, 'learning_rate': 4.2125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.5104, 'grad_norm': 7.703267574310303, 'learning_rate': 4.21e-05, 'epoch': 0.22}\n",
      "{'loss': 3.464, 'grad_norm': 8.384134292602539, 'learning_rate': 4.2075000000000006e-05, 'epoch': 0.22}\n",
      "{'loss': 3.5308, 'grad_norm': 8.706306457519531, 'learning_rate': 4.205e-05, 'epoch': 0.22}\n",
      "{'loss': 3.5472, 'grad_norm': 7.345625877380371, 'learning_rate': 4.2025000000000005e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4652, 'grad_norm': 7.370922088623047, 'learning_rate': 4.2e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4437, 'grad_norm': 7.733022689819336, 'learning_rate': 4.1975000000000004e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4895, 'grad_norm': 7.079156875610352, 'learning_rate': 4.195e-05, 'epoch': 0.22}\n",
      "{'loss': 3.407, 'grad_norm': 7.840794563293457, 'learning_rate': 4.1925e-05, 'epoch': 0.23}\n",
      "{'loss': 3.5316, 'grad_norm': 7.19880485534668, 'learning_rate': 4.19e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4305, 'grad_norm': 7.641568660736084, 'learning_rate': 4.1875e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4273, 'grad_norm': 7.772340297698975, 'learning_rate': 4.185e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3782, 'grad_norm': 6.680289268493652, 'learning_rate': 4.1825e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3915, 'grad_norm': 8.457769393920898, 'learning_rate': 4.18e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4937, 'grad_norm': 7.45513916015625, 'learning_rate': 4.1775000000000006e-05, 'epoch': 0.23}\n",
      "{'loss': 3.5238, 'grad_norm': 7.359285354614258, 'learning_rate': 4.175e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3868, 'grad_norm': 6.675346374511719, 'learning_rate': 4.1725000000000005e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3836, 'grad_norm': 7.987618923187256, 'learning_rate': 4.17e-05, 'epoch': 0.23}\n",
      "{'loss': 3.345, 'grad_norm': 7.72022008895874, 'learning_rate': 4.1675e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4076, 'grad_norm': 7.157039642333984, 'learning_rate': 4.165e-05, 'epoch': 0.23}\n",
      "{'loss': 3.436, 'grad_norm': 7.272836208343506, 'learning_rate': 4.1625e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4995, 'grad_norm': 7.417486190795898, 'learning_rate': 4.16e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4124, 'grad_norm': 6.680798530578613, 'learning_rate': 4.1575e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4339, 'grad_norm': 7.818140983581543, 'learning_rate': 4.155e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4366, 'grad_norm': 7.430485248565674, 'learning_rate': 4.1525e-05, 'epoch': 0.24}\n",
      "{'loss': 3.375, 'grad_norm': 7.806196212768555, 'learning_rate': 4.15e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4894, 'grad_norm': 8.685870170593262, 'learning_rate': 4.1475000000000005e-05, 'epoch': 0.24}\n",
      "{'loss': 3.419, 'grad_norm': 7.9091877937316895, 'learning_rate': 4.145e-05, 'epoch': 0.24}\n",
      "{'loss': 3.5417, 'grad_norm': 7.337635517120361, 'learning_rate': 4.1425000000000004e-05, 'epoch': 0.24}\n",
      "{'loss': 3.402, 'grad_norm': 8.542226791381836, 'learning_rate': 4.14e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3147, 'grad_norm': 7.394015789031982, 'learning_rate': 4.1375e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4253, 'grad_norm': 9.170624732971191, 'learning_rate': 4.135e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4245, 'grad_norm': 8.161306381225586, 'learning_rate': 4.1325e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3798, 'grad_norm': 7.982314586639404, 'learning_rate': 4.13e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4615, 'grad_norm': 7.875446796417236, 'learning_rate': 4.1275e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4408, 'grad_norm': 6.48390007019043, 'learning_rate': 4.125e-05, 'epoch': 0.24}\n",
      " 18%|█████▉                            | 3500/20000 [1:51:20<8:21:30,  1.82s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:09,  9.06s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.165738, 'eval_rouge-2': 6.67346, 'eval_rouge-l': 24.912432000000003, 'eval_bleu-4': 0.033257279249921924, 'eval_runtime': 58.6132, 'eval_samples_per_second': 0.853, 'eval_steps_per_second': 0.068, 'epoch': 0.24}\n",
      " 18%|█████▉                            | 3500/20000 [1:52:19<8:21:30,  1.82s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.73s/it]\u001b[A\n",
      "{'loss': 3.618, 'grad_norm': 7.930517196655273, 'learning_rate': 4.1225e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2428, 'grad_norm': 7.325198173522949, 'learning_rate': 4.12e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3905, 'grad_norm': 7.940271377563477, 'learning_rate': 4.1175000000000005e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4733, 'grad_norm': 8.809427261352539, 'learning_rate': 4.115e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4255, 'grad_norm': 7.309786319732666, 'learning_rate': 4.1125000000000004e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4192, 'grad_norm': 8.231523513793945, 'learning_rate': 4.11e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4822, 'grad_norm': 8.016133308410645, 'learning_rate': 4.1075e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4098, 'grad_norm': 9.000072479248047, 'learning_rate': 4.105e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4663, 'grad_norm': 8.115458488464355, 'learning_rate': 4.1025e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4282, 'grad_norm': 8.100341796875, 'learning_rate': 4.1e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3248, 'grad_norm': 9.263501167297363, 'learning_rate': 4.0975e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3169, 'grad_norm': 8.410959243774414, 'learning_rate': 4.095e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4736, 'grad_norm': 7.5609283447265625, 'learning_rate': 4.0925000000000005e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4323, 'grad_norm': 6.945135593414307, 'learning_rate': 4.09e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4209, 'grad_norm': 7.692669868469238, 'learning_rate': 4.0875000000000004e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4771, 'grad_norm': 7.202565670013428, 'learning_rate': 4.085e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4581, 'grad_norm': 7.859777450561523, 'learning_rate': 4.0825e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3415, 'grad_norm': 7.6503167152404785, 'learning_rate': 4.08e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3661, 'grad_norm': 8.379520416259766, 'learning_rate': 4.0775e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3819, 'grad_norm': 8.485718727111816, 'learning_rate': 4.075e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4908, 'grad_norm': 7.499400615692139, 'learning_rate': 4.0725e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4485, 'grad_norm': 7.79388952255249, 'learning_rate': 4.07e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3491, 'grad_norm': 7.538565635681152, 'learning_rate': 4.0675e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3829, 'grad_norm': 7.85508918762207, 'learning_rate': 4.065e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2433, 'grad_norm': 8.024264335632324, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4183, 'grad_norm': 7.7842607498168945, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4581, 'grad_norm': 7.671616077423096, 'learning_rate': 4.0575000000000004e-05, 'epoch': 0.26}\n",
      "{'loss': 3.485, 'grad_norm': 8.091123580932617, 'learning_rate': 4.055e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4404, 'grad_norm': 10.241312980651855, 'learning_rate': 4.0525e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4994, 'grad_norm': 7.760661602020264, 'learning_rate': 4.05e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3813, 'grad_norm': 8.131750106811523, 'learning_rate': 4.0475e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3898, 'grad_norm': 7.49351167678833, 'learning_rate': 4.045000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 3.379, 'grad_norm': 8.418067932128906, 'learning_rate': 4.0425e-05, 'epoch': 0.27}\n",
      "{'loss': 3.461, 'grad_norm': 7.488457202911377, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4057, 'grad_norm': 7.918309688568115, 'learning_rate': 4.0375e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4513, 'grad_norm': 7.561319351196289, 'learning_rate': 4.0350000000000005e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4326, 'grad_norm': 7.820744514465332, 'learning_rate': 4.0325000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4983, 'grad_norm': 11.543146133422852, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3839, 'grad_norm': 7.500174522399902, 'learning_rate': 4.0275e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3423, 'grad_norm': 7.059051036834717, 'learning_rate': 4.025e-05, 'epoch': 0.27}\n",
      "{'loss': 3.488, 'grad_norm': 7.324711322784424, 'learning_rate': 4.0225e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4632, 'grad_norm': 7.649487018585205, 'learning_rate': 4.02e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4276, 'grad_norm': 7.426252841949463, 'learning_rate': 4.0175e-05, 'epoch': 0.27}\n",
      "{'loss': 3.332, 'grad_norm': 7.544555187225342, 'learning_rate': 4.015000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3586, 'grad_norm': 8.132863998413086, 'learning_rate': 4.0125e-05, 'epoch': 0.28}\n",
      "{'loss': 3.5086, 'grad_norm': 7.695252895355225, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3774, 'grad_norm': 8.171452522277832, 'learning_rate': 4.0075e-05, 'epoch': 0.28}\n",
      "{'loss': 3.38, 'grad_norm': 7.091436862945557, 'learning_rate': 4.0050000000000004e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4615, 'grad_norm': 7.488702297210693, 'learning_rate': 4.0025000000000004e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3563, 'grad_norm': 7.346038341522217, 'learning_rate': 4e-05, 'epoch': 0.28}\n",
      " 20%|██████▊                           | 4000/20000 [2:07:14<7:27:04,  1.68s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.583028, 'eval_rouge-2': 7.317724000000001, 'eval_rouge-l': 23.408089999999998, 'eval_bleu-4': 0.03259258990624442, 'eval_runtime': 79.0859, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.051, 'epoch': 0.28}\n",
      " 20%|██████▊                           | 4000/20000 [2:08:33<7:27:04,  1.68s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:52<00:00, 11.97s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3762, 'grad_norm': 8.763251304626465, 'learning_rate': 3.9975e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3526, 'grad_norm': 8.169045448303223, 'learning_rate': 3.995e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3836, 'grad_norm': 7.406705379486084, 'learning_rate': 3.9925e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4113, 'grad_norm': 7.1255621910095215, 'learning_rate': 3.99e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3854, 'grad_norm': 7.8749613761901855, 'learning_rate': 3.9875e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4186, 'grad_norm': 7.501633167266846, 'learning_rate': 3.9850000000000006e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4985, 'grad_norm': 8.273098945617676, 'learning_rate': 3.9825e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3915, 'grad_norm': 8.637242317199707, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4116, 'grad_norm': 7.698669910430908, 'learning_rate': 3.9775e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4788, 'grad_norm': 8.334736824035645, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4146, 'grad_norm': 8.658397674560547, 'learning_rate': 3.9725e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3993, 'grad_norm': 7.791739463806152, 'learning_rate': 3.97e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4682, 'grad_norm': 8.209036827087402, 'learning_rate': 3.9675e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3528, 'grad_norm': 7.1237406730651855, 'learning_rate': 3.965e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4239, 'grad_norm': 7.544717788696289, 'learning_rate': 3.9625e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4426, 'grad_norm': 7.5047607421875, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4485, 'grad_norm': 8.547266960144043, 'learning_rate': 3.9575e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3424, 'grad_norm': 8.079249382019043, 'learning_rate': 3.9550000000000006e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3966, 'grad_norm': 7.913451194763184, 'learning_rate': 3.9525e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3645, 'grad_norm': 8.147515296936035, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3964, 'grad_norm': 7.207703113555908, 'learning_rate': 3.9475000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 3.439, 'grad_norm': 8.376558303833008, 'learning_rate': 3.9450000000000003e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3015, 'grad_norm': 7.746705532073975, 'learning_rate': 3.9425e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4065, 'grad_norm': 8.00832748413086, 'learning_rate': 3.94e-05, 'epoch': 0.3}\n",
      "{'loss': 3.425, 'grad_norm': 7.156189918518066, 'learning_rate': 3.9375e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3612, 'grad_norm': 7.613673686981201, 'learning_rate': 3.935e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4422, 'grad_norm': 9.03586196899414, 'learning_rate': 3.9325e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3758, 'grad_norm': 8.736234664916992, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4669, 'grad_norm': 8.253713607788086, 'learning_rate': 3.9275e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3838, 'grad_norm': 9.216925621032715, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4416, 'grad_norm': 7.4655327796936035, 'learning_rate': 3.9225e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4606, 'grad_norm': 7.690245628356934, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4702, 'grad_norm': 8.430072784423828, 'learning_rate': 3.9175000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3953, 'grad_norm': 8.809013366699219, 'learning_rate': 3.915e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4067, 'grad_norm': 7.6866374015808105, 'learning_rate': 3.9125e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4868, 'grad_norm': 8.322807312011719, 'learning_rate': 3.91e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3166, 'grad_norm': 8.014163970947266, 'learning_rate': 3.9075e-05, 'epoch': 0.31}\n",
      "{'loss': 3.402, 'grad_norm': 7.344560623168945, 'learning_rate': 3.905e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3631, 'grad_norm': 7.478900909423828, 'learning_rate': 3.9025e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4598, 'grad_norm': 8.059187889099121, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4048, 'grad_norm': 7.147975444793701, 'learning_rate': 3.8975e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4066, 'grad_norm': 8.165555953979492, 'learning_rate': 3.8950000000000005e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3861, 'grad_norm': 7.985406875610352, 'learning_rate': 3.8925e-05, 'epoch': 0.31}\n",
      "{'loss': 3.464, 'grad_norm': 7.601638317108154, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3858, 'grad_norm': 7.922405242919922, 'learning_rate': 3.8875e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3219, 'grad_norm': 7.429653644561768, 'learning_rate': 3.885e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4952, 'grad_norm': 7.7808356285095215, 'learning_rate': 3.8825e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4756, 'grad_norm': 8.309332847595215, 'learning_rate': 3.88e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4296, 'grad_norm': 8.108105659484863, 'learning_rate': 3.8775e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4251, 'grad_norm': 8.299306869506836, 'learning_rate': 3.875e-05, 'epoch': 0.31}\n",
      " 22%|███████▋                          | 4500/20000 [2:23:31<7:55:18,  1.84s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:41<00:14, 14.01s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.842336, 'eval_rouge-2': 6.78279, 'eval_rouge-l': 23.951216, 'eval_bleu-4': 0.033117785495225066, 'eval_runtime': 69.9326, 'eval_samples_per_second': 0.715, 'eval_steps_per_second': 0.057, 'epoch': 0.31}\n",
      " 22%|███████▋                          | 4500/20000 [2:24:40<7:55:18,  1.84s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:43<00:00,  9.61s/it]\u001b[A\n",
      "{'loss': 3.4243, 'grad_norm': 7.714871883392334, 'learning_rate': 3.8725e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3164, 'grad_norm': 7.28920841217041, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3724, 'grad_norm': 7.140926361083984, 'learning_rate': 3.8675e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4712, 'grad_norm': 7.8592753410339355, 'learning_rate': 3.8650000000000004e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3997, 'grad_norm': 8.438334465026855, 'learning_rate': 3.8625e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3488, 'grad_norm': 7.20979118347168, 'learning_rate': 3.86e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3996, 'grad_norm': 8.271614074707031, 'learning_rate': 3.8575e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2219, 'grad_norm': 8.009960174560547, 'learning_rate': 3.855e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4438, 'grad_norm': 7.742457389831543, 'learning_rate': 3.8525e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3847, 'grad_norm': 8.458789825439453, 'learning_rate': 3.85e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4542, 'grad_norm': 8.025766372680664, 'learning_rate': 3.8475e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3331, 'grad_norm': 7.9046149253845215, 'learning_rate': 3.845e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3738, 'grad_norm': 9.867218017578125, 'learning_rate': 3.8425e-05, 'epoch': 0.32}\n",
      "{'loss': 3.418, 'grad_norm': 7.294676303863525, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4135, 'grad_norm': 7.593705654144287, 'learning_rate': 3.8375e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4332, 'grad_norm': 8.793498039245605, 'learning_rate': 3.8350000000000004e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3836, 'grad_norm': 7.559849739074707, 'learning_rate': 3.8324999999999996e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3727, 'grad_norm': 8.599677085876465, 'learning_rate': 3.83e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4021, 'grad_norm': 8.248480796813965, 'learning_rate': 3.8275e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4007, 'grad_norm': 8.010116577148438, 'learning_rate': 3.825e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2943, 'grad_norm': 7.786677837371826, 'learning_rate': 3.8225e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3265, 'grad_norm': 8.336036682128906, 'learning_rate': 3.82e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4045, 'grad_norm': 8.208819389343262, 'learning_rate': 3.8175e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3826, 'grad_norm': 7.283139705657959, 'learning_rate': 3.8150000000000006e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2965, 'grad_norm': 7.565293788909912, 'learning_rate': 3.8125e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4281, 'grad_norm': 8.218655586242676, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.33}\n",
      "{'loss': 3.399, 'grad_norm': 8.670109748840332, 'learning_rate': 3.8075e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3985, 'grad_norm': 8.168771743774414, 'learning_rate': 3.805e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3577, 'grad_norm': 7.628873825073242, 'learning_rate': 3.8025e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4969, 'grad_norm': 8.230005264282227, 'learning_rate': 3.8e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4172, 'grad_norm': 7.731106758117676, 'learning_rate': 3.7975e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4099, 'grad_norm': 7.696306228637695, 'learning_rate': 3.795e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4604, 'grad_norm': 8.018366813659668, 'learning_rate': 3.7925e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3888, 'grad_norm': 8.027125358581543, 'learning_rate': 3.79e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4223, 'grad_norm': 7.962060451507568, 'learning_rate': 3.7875e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4996, 'grad_norm': 8.24238395690918, 'learning_rate': 3.7850000000000005e-05, 'epoch': 0.34}\n",
      "{'loss': 3.5187, 'grad_norm': 8.58227252960205, 'learning_rate': 3.7825e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4762, 'grad_norm': 8.052850723266602, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3928, 'grad_norm': 8.047124862670898, 'learning_rate': 3.7775e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3522, 'grad_norm': 7.813356876373291, 'learning_rate': 3.775e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3441, 'grad_norm': 7.40317440032959, 'learning_rate': 3.7725e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3295, 'grad_norm': 7.791759490966797, 'learning_rate': 3.77e-05, 'epoch': 0.34}\n",
      "{'loss': 3.1627, 'grad_norm': 8.966075897216797, 'learning_rate': 3.7675e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4554, 'grad_norm': 7.697432518005371, 'learning_rate': 3.765e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3961, 'grad_norm': 7.87242317199707, 'learning_rate': 3.7625e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4171, 'grad_norm': 8.867837905883789, 'learning_rate': 3.76e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4096, 'grad_norm': 8.378774642944336, 'learning_rate': 3.7575e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4284, 'grad_norm': 8.389702796936035, 'learning_rate': 3.7550000000000005e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3829, 'grad_norm': 8.080229759216309, 'learning_rate': 3.7525e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2978, 'grad_norm': 7.944150924682617, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.35}\n",
      " 25%|████████▌                         | 5000/20000 [2:39:33<7:35:03,  1.82s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.639916, 'eval_rouge-2': 7.770286000000001, 'eval_rouge-l': 24.206592000000004, 'eval_bleu-4': 0.03419798294781152, 'eval_runtime': 79.2496, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.05, 'epoch': 0.35}\n",
      " 25%|████████▌                         | 5000/20000 [2:40:53<7:35:03,  1.82s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.05s/it]\u001b[A\n",
      "{'loss': 3.4717, 'grad_norm': 8.332745552062988, 'learning_rate': 3.7475e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4115, 'grad_norm': 7.619295597076416, 'learning_rate': 3.745e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3821, 'grad_norm': 7.860531330108643, 'learning_rate': 3.7425e-05, 'epoch': 0.35}\n",
      "{'loss': 3.347, 'grad_norm': 8.22801685333252, 'learning_rate': 3.74e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3523, 'grad_norm': 8.075170516967773, 'learning_rate': 3.737500000000001e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4232, 'grad_norm': 8.175348281860352, 'learning_rate': 3.735e-05, 'epoch': 0.35}\n",
      "{'loss': 3.426, 'grad_norm': 7.708315372467041, 'learning_rate': 3.7325000000000006e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4085, 'grad_norm': 8.812823295593262, 'learning_rate': 3.73e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3284, 'grad_norm': 8.183734893798828, 'learning_rate': 3.7275000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 3.422, 'grad_norm': 7.647735118865967, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3981, 'grad_norm': 7.8858232498168945, 'learning_rate': 3.7225000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4219, 'grad_norm': 8.076746940612793, 'learning_rate': 3.72e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3262, 'grad_norm': 7.55964469909668, 'learning_rate': 3.7175e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3906, 'grad_norm': 7.8277716636657715, 'learning_rate': 3.715e-05, 'epoch': 0.36}\n",
      "{'loss': 3.5274, 'grad_norm': 8.07495403289795, 'learning_rate': 3.7125e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3522, 'grad_norm': 7.425179958343506, 'learning_rate': 3.71e-05, 'epoch': 0.36}\n",
      "{'loss': 3.515, 'grad_norm': 7.969293117523193, 'learning_rate': 3.707500000000001e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3827, 'grad_norm': 7.70602560043335, 'learning_rate': 3.705e-05, 'epoch': 0.36}\n",
      "{'loss': 3.424, 'grad_norm': 7.87637186050415, 'learning_rate': 3.7025000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 3.297, 'grad_norm': 10.404633522033691, 'learning_rate': 3.7e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3728, 'grad_norm': 8.551767349243164, 'learning_rate': 3.6975000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3448, 'grad_norm': 7.7427849769592285, 'learning_rate': 3.6950000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3356, 'grad_norm': 8.460753440856934, 'learning_rate': 3.6925e-05, 'epoch': 0.37}\n",
      "{'loss': 3.4346, 'grad_norm': 7.615028381347656, 'learning_rate': 3.69e-05, 'epoch': 0.37}\n",
      "{'loss': 3.4556, 'grad_norm': 7.912057876586914, 'learning_rate': 3.6875e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3722, 'grad_norm': 9.362802505493164, 'learning_rate': 3.685e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3367, 'grad_norm': 8.622333526611328, 'learning_rate': 3.6825e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3953, 'grad_norm': 7.743772029876709, 'learning_rate': 3.68e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3403, 'grad_norm': 8.024919509887695, 'learning_rate': 3.6775000000000006e-05, 'epoch': 0.37}\n",
      "{'loss': 3.4065, 'grad_norm': 7.184396743774414, 'learning_rate': 3.675e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3945, 'grad_norm': 7.929767608642578, 'learning_rate': 3.6725000000000005e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3888, 'grad_norm': 7.887243747711182, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3395, 'grad_norm': 8.81053352355957, 'learning_rate': 3.6675000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3558, 'grad_norm': 7.663697242736816, 'learning_rate': 3.665e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3919, 'grad_norm': 8.238089561462402, 'learning_rate': 3.6625e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3328, 'grad_norm': 8.061209678649902, 'learning_rate': 3.66e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3202, 'grad_norm': 7.710418701171875, 'learning_rate': 3.6575e-05, 'epoch': 0.37}\n",
      "{'loss': 3.424, 'grad_norm': 8.754114151000977, 'learning_rate': 3.655e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3648, 'grad_norm': 7.569508075714111, 'learning_rate': 3.652500000000001e-05, 'epoch': 0.38}\n",
      "{'loss': 3.5031, 'grad_norm': 8.638154029846191, 'learning_rate': 3.65e-05, 'epoch': 0.38}\n",
      "{'loss': 3.437, 'grad_norm': 7.969354152679443, 'learning_rate': 3.6475000000000006e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3077, 'grad_norm': 7.993767738342285, 'learning_rate': 3.645e-05, 'epoch': 0.38}\n",
      "{'loss': 3.4487, 'grad_norm': 9.051705360412598, 'learning_rate': 3.6425000000000004e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3689, 'grad_norm': 8.841314315795898, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3942, 'grad_norm': 7.883800983428955, 'learning_rate': 3.6375e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3141, 'grad_norm': 8.041330337524414, 'learning_rate': 3.635e-05, 'epoch': 0.38}\n",
      "{'loss': 3.402, 'grad_norm': 8.369523048400879, 'learning_rate': 3.6325e-05, 'epoch': 0.38}\n",
      "{'loss': 3.4262, 'grad_norm': 8.02194881439209, 'learning_rate': 3.63e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3301, 'grad_norm': 8.610970497131348, 'learning_rate': 3.6275e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2338, 'grad_norm': 7.754545211791992, 'learning_rate': 3.625e-05, 'epoch': 0.38}\n",
      " 28%|█████████▎                        | 5500/20000 [2:55:46<7:13:53,  1.80s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:16<00:05,  5.34s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.4219, 'eval_rouge-2': 7.759519999999998, 'eval_rouge-l': 25.234868000000002, 'eval_bleu-4': 0.035627135792353354, 'eval_runtime': 22.9511, 'eval_samples_per_second': 2.179, 'eval_steps_per_second': 0.174, 'epoch': 0.38}\n",
      " 28%|█████████▎                        | 5500/20000 [2:56:09<7:13:53,  1.80s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:18<00:00,  4.22s/it]\u001b[A\n",
      "{'loss': 3.3727, 'grad_norm': 8.388625144958496, 'learning_rate': 3.6225000000000006e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3395, 'grad_norm': 8.310654640197754, 'learning_rate': 3.62e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4894, 'grad_norm': 7.678810119628906, 'learning_rate': 3.6175000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3295, 'grad_norm': 7.867332935333252, 'learning_rate': 3.615e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4131, 'grad_norm': 8.070096015930176, 'learning_rate': 3.6125000000000004e-05, 'epoch': 0.39}\n",
      "{'loss': 3.404, 'grad_norm': 7.521617412567139, 'learning_rate': 3.61e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3449, 'grad_norm': 7.78309440612793, 'learning_rate': 3.6075e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3506, 'grad_norm': 8.778867721557617, 'learning_rate': 3.605e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3292, 'grad_norm': 7.811316013336182, 'learning_rate': 3.6025e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3602, 'grad_norm': 8.262928009033203, 'learning_rate': 3.6e-05, 'epoch': 0.39}\n",
      "{'loss': 3.424, 'grad_norm': 7.741294860839844, 'learning_rate': 3.5975e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4063, 'grad_norm': 7.257336616516113, 'learning_rate': 3.595e-05, 'epoch': 0.39}\n",
      "{'loss': 3.371, 'grad_norm': 8.686779975891113, 'learning_rate': 3.5925000000000006e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3693, 'grad_norm': 9.22645092010498, 'learning_rate': 3.59e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3818, 'grad_norm': 9.351831436157227, 'learning_rate': 3.5875000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4608, 'grad_norm': 8.117581367492676, 'learning_rate': 3.585e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2872, 'grad_norm': 8.193443298339844, 'learning_rate': 3.5825000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3627, 'grad_norm': 7.889092445373535, 'learning_rate': 3.58e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3521, 'grad_norm': 9.32773494720459, 'learning_rate': 3.5775e-05, 'epoch': 0.4}\n",
      "{'loss': 3.426, 'grad_norm': 8.073026657104492, 'learning_rate': 3.575e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4036, 'grad_norm': 8.42110824584961, 'learning_rate': 3.5725e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3948, 'grad_norm': 8.256237983703613, 'learning_rate': 3.57e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2742, 'grad_norm': 8.015971183776855, 'learning_rate': 3.5675e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3943, 'grad_norm': 7.564082145690918, 'learning_rate': 3.565e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3153, 'grad_norm': 8.110819816589355, 'learning_rate': 3.5625000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 3.5049, 'grad_norm': 8.38450813293457, 'learning_rate': 3.56e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4004, 'grad_norm': 8.324982643127441, 'learning_rate': 3.5575000000000004e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4926, 'grad_norm': 7.876395225524902, 'learning_rate': 3.555e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4135, 'grad_norm': 8.580841064453125, 'learning_rate': 3.5525e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4228, 'grad_norm': 7.523461818695068, 'learning_rate': 3.55e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3655, 'grad_norm': 8.111004829406738, 'learning_rate': 3.5475e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3594, 'grad_norm': 8.607561111450195, 'learning_rate': 3.545e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3004, 'grad_norm': 8.557439804077148, 'learning_rate': 3.5425e-05, 'epoch': 0.41}\n",
      "{'loss': 3.326, 'grad_norm': 7.619649887084961, 'learning_rate': 3.54e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3632, 'grad_norm': 8.038881301879883, 'learning_rate': 3.5375e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2217, 'grad_norm': 8.368083953857422, 'learning_rate': 3.535e-05, 'epoch': 0.41}\n",
      "{'loss': 3.4188, 'grad_norm': 8.136106491088867, 'learning_rate': 3.5325000000000005e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3856, 'grad_norm': 8.541719436645508, 'learning_rate': 3.53e-05, 'epoch': 0.41}\n",
      "{'loss': 3.4661, 'grad_norm': 9.922223091125488, 'learning_rate': 3.5275000000000004e-05, 'epoch': 0.41}\n",
      "{'loss': 3.4332, 'grad_norm': 7.685575008392334, 'learning_rate': 3.525e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3214, 'grad_norm': 8.762831687927246, 'learning_rate': 3.5225e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3937, 'grad_norm': 8.732568740844727, 'learning_rate': 3.52e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3287, 'grad_norm': 7.572319984436035, 'learning_rate': 3.5175e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3614, 'grad_norm': 7.456002235412598, 'learning_rate': 3.515e-05, 'epoch': 0.41}\n",
      "{'loss': 3.33, 'grad_norm': 8.194451332092285, 'learning_rate': 3.5125e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2668, 'grad_norm': 7.632400035858154, 'learning_rate': 3.51e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3715, 'grad_norm': 8.1400785446167, 'learning_rate': 3.5075000000000006e-05, 'epoch': 0.42}\n",
      "{'loss': 3.4003, 'grad_norm': 7.452674388885498, 'learning_rate': 3.505e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2865, 'grad_norm': 8.533934593200684, 'learning_rate': 3.5025000000000004e-05, 'epoch': 0.42}\n",
      "{'loss': 3.4245, 'grad_norm': 8.414352416992188, 'learning_rate': 3.5e-05, 'epoch': 0.42}\n",
      " 30%|██████████▏                       | 6000/20000 [3:10:59<7:01:01,  1.80s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.44294, 'eval_rouge-2': 7.891246, 'eval_rouge-l': 24.360124, 'eval_bleu-4': 0.036303295544596347, 'eval_runtime': 95.2699, 'eval_samples_per_second': 0.525, 'eval_steps_per_second': 0.042, 'epoch': 0.42}\n",
      " 30%|██████████▏                       | 6000/20000 [3:12:34<7:01:01,  1.80s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:09<00:00, 18.01s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-6000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3884, 'grad_norm': 9.045351028442383, 'learning_rate': 3.4975e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3804, 'grad_norm': 7.94701623916626, 'learning_rate': 3.495e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3125, 'grad_norm': 8.674416542053223, 'learning_rate': 3.4925e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3432, 'grad_norm': 7.332756519317627, 'learning_rate': 3.49e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3258, 'grad_norm': 8.44258975982666, 'learning_rate': 3.4875e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3154, 'grad_norm': 8.405652046203613, 'learning_rate': 3.485e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2979, 'grad_norm': 8.323708534240723, 'learning_rate': 3.4825e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3819, 'grad_norm': 7.754278659820557, 'learning_rate': 3.48e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2744, 'grad_norm': 8.046521186828613, 'learning_rate': 3.4775000000000005e-05, 'epoch': 0.43}\n",
      "{'loss': 3.4173, 'grad_norm': 7.953609943389893, 'learning_rate': 3.475e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3102, 'grad_norm': 8.684337615966797, 'learning_rate': 3.4725000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3803, 'grad_norm': 7.844711780548096, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3128, 'grad_norm': 8.3018798828125, 'learning_rate': 3.4675e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3496, 'grad_norm': 8.288976669311523, 'learning_rate': 3.465e-05, 'epoch': 0.43}\n",
      "{'loss': 3.4501, 'grad_norm': 7.964346408843994, 'learning_rate': 3.4625e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3849, 'grad_norm': 8.219644546508789, 'learning_rate': 3.46e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3992, 'grad_norm': 7.616069316864014, 'learning_rate': 3.4575e-05, 'epoch': 0.43}\n",
      "{'loss': 3.4321, 'grad_norm': 8.63440990447998, 'learning_rate': 3.455e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3034, 'grad_norm': 7.941137790679932, 'learning_rate': 3.4525e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3013, 'grad_norm': 7.884331703186035, 'learning_rate': 3.45e-05, 'epoch': 0.43}\n",
      "{'loss': 3.31, 'grad_norm': 8.093180656433105, 'learning_rate': 3.4475000000000005e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3753, 'grad_norm': 8.021859169006348, 'learning_rate': 3.445e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3828, 'grad_norm': 8.070144653320312, 'learning_rate': 3.4425e-05, 'epoch': 0.43}\n",
      "{'loss': 3.4284, 'grad_norm': 8.841700553894043, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4347, 'grad_norm': 10.902763366699219, 'learning_rate': 3.4375e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3052, 'grad_norm': 8.344420433044434, 'learning_rate': 3.435e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4252, 'grad_norm': 8.739508628845215, 'learning_rate': 3.4325e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4206, 'grad_norm': 8.350203514099121, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2893, 'grad_norm': 8.066590309143066, 'learning_rate': 3.4275e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3543, 'grad_norm': 8.34689712524414, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.44}\n",
      "{'loss': 3.33, 'grad_norm': 7.955122470855713, 'learning_rate': 3.4225e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3157, 'grad_norm': 7.699036598205566, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4709, 'grad_norm': 7.835043430328369, 'learning_rate': 3.4175000000000004e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3148, 'grad_norm': 7.992209434509277, 'learning_rate': 3.415e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3386, 'grad_norm': 8.650045394897461, 'learning_rate': 3.4125e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2721, 'grad_norm': 7.546984672546387, 'learning_rate': 3.41e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3774, 'grad_norm': 7.801555633544922, 'learning_rate': 3.4075e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3514, 'grad_norm': 8.102373123168945, 'learning_rate': 3.405e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3877, 'grad_norm': 8.898599624633789, 'learning_rate': 3.4025e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3641, 'grad_norm': 8.015669822692871, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3728, 'grad_norm': 8.772544860839844, 'learning_rate': 3.3975e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3335, 'grad_norm': 8.546341896057129, 'learning_rate': 3.3950000000000005e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3914, 'grad_norm': 8.61876392364502, 'learning_rate': 3.3925e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2575, 'grad_norm': 8.713712692260742, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3571, 'grad_norm': 8.776305198669434, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3675, 'grad_norm': 9.283767700195312, 'learning_rate': 3.385e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2721, 'grad_norm': 9.018941879272461, 'learning_rate': 3.3825e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3107, 'grad_norm': 7.484601020812988, 'learning_rate': 3.38e-05, 'epoch': 0.45}\n",
      "{'loss': 3.4123, 'grad_norm': 7.599757671356201, 'learning_rate': 3.3775e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3161, 'grad_norm': 9.320785522460938, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.45}\n",
      " 32%|███████████                       | 6500/20000 [3:27:29<6:55:34,  1.85s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.897080000000003, 'eval_rouge-2': 7.087737999999999, 'eval_rouge-l': 20.752154, 'eval_bleu-4': 0.028807117336006098, 'eval_runtime': 82.0938, 'eval_samples_per_second': 0.609, 'eval_steps_per_second': 0.049, 'epoch': 0.45}\n",
      " 32%|███████████                       | 6500/20000 [3:28:51<6:55:34,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:55<00:00, 13.08s/it]\u001b[A\n",
      "{'loss': 3.3568, 'grad_norm': 8.459827423095703, 'learning_rate': 3.3725e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3661, 'grad_norm': 8.193608283996582, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3536, 'grad_norm': 7.787384033203125, 'learning_rate': 3.3675e-05, 'epoch': 0.46}\n",
      "{'loss': 3.296, 'grad_norm': 8.141520500183105, 'learning_rate': 3.3650000000000005e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3511, 'grad_norm': 9.344317436218262, 'learning_rate': 3.3625000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2892, 'grad_norm': 7.996683597564697, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4475, 'grad_norm': 8.668688774108887, 'learning_rate': 3.3575e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3509, 'grad_norm': 8.18570613861084, 'learning_rate': 3.355e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3066, 'grad_norm': 8.75417423248291, 'learning_rate': 3.3525e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4633, 'grad_norm': 7.839199066162109, 'learning_rate': 3.35e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3121, 'grad_norm': 7.980897426605225, 'learning_rate': 3.3475e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3384, 'grad_norm': 8.886621475219727, 'learning_rate': 3.345000000000001e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4133, 'grad_norm': 8.787954330444336, 'learning_rate': 3.3425e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3569, 'grad_norm': 9.47188949584961, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2625, 'grad_norm': 8.077886581420898, 'learning_rate': 3.3375e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3818, 'grad_norm': 7.564040660858154, 'learning_rate': 3.3350000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4252, 'grad_norm': 8.11255931854248, 'learning_rate': 3.3325000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3699, 'grad_norm': 9.557330131530762, 'learning_rate': 3.33e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3186, 'grad_norm': 8.234079360961914, 'learning_rate': 3.3275e-05, 'epoch': 0.47}\n",
      "{'loss': 3.4087, 'grad_norm': 8.297719955444336, 'learning_rate': 3.325e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3813, 'grad_norm': 7.860575199127197, 'learning_rate': 3.3225e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3638, 'grad_norm': 8.40881633758545, 'learning_rate': 3.32e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3198, 'grad_norm': 9.222090721130371, 'learning_rate': 3.3175e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3693, 'grad_norm': 8.198563575744629, 'learning_rate': 3.3150000000000006e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3718, 'grad_norm': 8.153855323791504, 'learning_rate': 3.3125e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3595, 'grad_norm': 8.555546760559082, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2658, 'grad_norm': 10.135126113891602, 'learning_rate': 3.3075e-05, 'epoch': 0.47}\n",
      "{'loss': 3.5583, 'grad_norm': 7.49044942855835, 'learning_rate': 3.3050000000000004e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3541, 'grad_norm': 9.620129585266113, 'learning_rate': 3.3025e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3295, 'grad_norm': 7.765844821929932, 'learning_rate': 3.3e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3265, 'grad_norm': 8.174901962280273, 'learning_rate': 3.2975e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2366, 'grad_norm': 8.443802833557129, 'learning_rate': 3.295e-05, 'epoch': 0.48}\n",
      "{'loss': 3.4217, 'grad_norm': 8.78215503692627, 'learning_rate': 3.2925e-05, 'epoch': 0.48}\n",
      "{'loss': 3.4392, 'grad_norm': 7.835170269012451, 'learning_rate': 3.29e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3623, 'grad_norm': 9.191332817077637, 'learning_rate': 3.2875e-05, 'epoch': 0.48}\n",
      "{'loss': 3.384, 'grad_norm': 8.063167572021484, 'learning_rate': 3.2850000000000006e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3283, 'grad_norm': 8.145094871520996, 'learning_rate': 3.2825e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2763, 'grad_norm': 8.120984077453613, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.48}\n",
      "{'loss': 3.431, 'grad_norm': 7.297832489013672, 'learning_rate': 3.2775e-05, 'epoch': 0.48}\n",
      "{'loss': 3.276, 'grad_norm': 8.970710754394531, 'learning_rate': 3.275e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3993, 'grad_norm': 8.029308319091797, 'learning_rate': 3.2725e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3555, 'grad_norm': 8.301970481872559, 'learning_rate': 3.27e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3161, 'grad_norm': 7.792013645172119, 'learning_rate': 3.2675e-05, 'epoch': 0.48}\n",
      "{'loss': 3.325, 'grad_norm': 8.013378143310547, 'learning_rate': 3.265e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3723, 'grad_norm': 8.235671043395996, 'learning_rate': 3.2625e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3934, 'grad_norm': 8.070089340209961, 'learning_rate': 3.26e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2919, 'grad_norm': 9.23603343963623, 'learning_rate': 3.2575e-05, 'epoch': 0.49}\n",
      "{'loss': 3.4139, 'grad_norm': 7.7876200675964355, 'learning_rate': 3.2550000000000005e-05, 'epoch': 0.49}\n",
      "{'loss': 3.4424, 'grad_norm': 7.807687282562256, 'learning_rate': 3.2525e-05, 'epoch': 0.49}\n",
      "{'loss': 3.4176, 'grad_norm': 7.779654502868652, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.49}\n",
      " 35%|███████████▉                      | 7000/20000 [3:43:48<6:18:08,  1.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:09,  9.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.523694000000006, 'eval_rouge-2': 7.719352, 'eval_rouge-l': 22.957192, 'eval_bleu-4': 0.034678600316578594, 'eval_runtime': 59.3622, 'eval_samples_per_second': 0.842, 'eval_steps_per_second': 0.067, 'epoch': 0.49}\n",
      " 35%|███████████▉                      | 7000/20000 [3:44:48<6:18:08,  1.75s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  6.86s/it]\u001b[A\n",
      "{'loss': 3.2568, 'grad_norm': 9.061079025268555, 'learning_rate': 3.2474999999999997e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3041, 'grad_norm': 9.211423873901367, 'learning_rate': 3.245e-05, 'epoch': 0.49}\n",
      "{'loss': 3.4346, 'grad_norm': 8.145801544189453, 'learning_rate': 3.2425e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2673, 'grad_norm': 8.31454849243164, 'learning_rate': 3.24e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2889, 'grad_norm': 8.762796401977539, 'learning_rate': 3.2375e-05, 'epoch': 0.49}\n",
      "{'loss': 3.333, 'grad_norm': 8.43170166015625, 'learning_rate': 3.235e-05, 'epoch': 0.49}\n",
      "{'loss': 3.4093, 'grad_norm': 9.558470726013184, 'learning_rate': 3.2325e-05, 'epoch': 0.49}\n",
      "{'loss': 3.318, 'grad_norm': 8.959807395935059, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2587, 'grad_norm': 8.568557739257812, 'learning_rate': 3.2275e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3763, 'grad_norm': 9.086116790771484, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3058, 'grad_norm': 8.5892915725708, 'learning_rate': 3.2225e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3123, 'grad_norm': 8.77329158782959, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2411, 'grad_norm': 7.904527187347412, 'learning_rate': 3.2175e-05, 'epoch': 0.5}\n",
      "{'loss': 3.4782, 'grad_norm': 8.55823040008545, 'learning_rate': 3.215e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3629, 'grad_norm': 8.618391036987305, 'learning_rate': 3.2125e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3689, 'grad_norm': 8.28107738494873, 'learning_rate': 3.21e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2636, 'grad_norm': 8.3314208984375, 'learning_rate': 3.2075e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3302, 'grad_norm': 8.048778533935547, 'learning_rate': 3.205e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3337, 'grad_norm': 8.342767715454102, 'learning_rate': 3.2025e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3322, 'grad_norm': 9.237153053283691, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.5}\n",
      "{'loss': 3.4389, 'grad_norm': 9.140952110290527, 'learning_rate': 3.1975e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2944, 'grad_norm': 9.028986930847168, 'learning_rate': 3.1950000000000004e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3197, 'grad_norm': 8.031917572021484, 'learning_rate': 3.1925e-05, 'epoch': 0.5}\n",
      "{'loss': 3.4075, 'grad_norm': 9.606276512145996, 'learning_rate': 3.19e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3519, 'grad_norm': 7.795090198516846, 'learning_rate': 3.1875e-05, 'epoch': 0.51}\n",
      "{'loss': 3.329, 'grad_norm': 8.72628402709961, 'learning_rate': 3.185e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3113, 'grad_norm': 8.101726531982422, 'learning_rate': 3.1825e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3978, 'grad_norm': 8.140707969665527, 'learning_rate': 3.18e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3571, 'grad_norm': 8.119874954223633, 'learning_rate': 3.1775e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3123, 'grad_norm': 8.869121551513672, 'learning_rate': 3.175e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3016, 'grad_norm': 7.8883256912231445, 'learning_rate': 3.1725e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2093, 'grad_norm': 9.052746772766113, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2921, 'grad_norm': 8.984456062316895, 'learning_rate': 3.1675e-05, 'epoch': 0.51}\n",
      "{'loss': 3.4629, 'grad_norm': 9.376075744628906, 'learning_rate': 3.1650000000000004e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2481, 'grad_norm': 8.080801963806152, 'learning_rate': 3.1624999999999996e-05, 'epoch': 0.51}\n",
      "{'loss': 3.4027, 'grad_norm': 8.253775596618652, 'learning_rate': 3.16e-05, 'epoch': 0.51}\n",
      "{'loss': 3.4633, 'grad_norm': 9.112552642822266, 'learning_rate': 3.1575e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3391, 'grad_norm': 8.890084266662598, 'learning_rate': 3.155e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3272, 'grad_norm': 8.756084442138672, 'learning_rate': 3.1525e-05, 'epoch': 0.52}\n",
      "{'loss': 3.461, 'grad_norm': 8.726670265197754, 'learning_rate': 3.15e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3928, 'grad_norm': 8.208523750305176, 'learning_rate': 3.1475e-05, 'epoch': 0.52}\n",
      "{'loss': 3.4129, 'grad_norm': 7.6033806800842285, 'learning_rate': 3.145e-05, 'epoch': 0.52}\n",
      "{'loss': 3.312, 'grad_norm': 9.246601104736328, 'learning_rate': 3.1425e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3667, 'grad_norm': 9.26123046875, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2772, 'grad_norm': 8.576687812805176, 'learning_rate': 3.1375e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3492, 'grad_norm': 8.204608917236328, 'learning_rate': 3.135e-05, 'epoch': 0.52}\n",
      "{'loss': 3.4149, 'grad_norm': 9.051748275756836, 'learning_rate': 3.1324999999999996e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3562, 'grad_norm': 8.878213882446289, 'learning_rate': 3.13e-05, 'epoch': 0.52}\n",
      "{'loss': 3.347, 'grad_norm': 8.938578605651855, 'learning_rate': 3.1275e-05, 'epoch': 0.52}\n",
      "{'loss': 3.4582, 'grad_norm': 8.06739616394043, 'learning_rate': 3.125e-05, 'epoch': 0.52}\n",
      " 38%|████████████▊                     | 7500/20000 [3:59:38<6:11:39,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.92823, 'eval_rouge-2': 7.130878, 'eval_rouge-l': 23.481788, 'eval_bleu-4': 0.03285366380480382, 'eval_runtime': 79.1652, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.051, 'epoch': 0.52}\n",
      " 38%|████████████▊                     | 7500/20000 [4:00:58<6:11:39,  1.78s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:52<00:00, 12.01s/it]\u001b[A\n",
      "{'loss': 3.4564, 'grad_norm': 9.066588401794434, 'learning_rate': 3.122500000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2252, 'grad_norm': 8.215049743652344, 'learning_rate': 3.12e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3638, 'grad_norm': 9.709692001342773, 'learning_rate': 3.1175000000000006e-05, 'epoch': 0.53}\n",
      "{'loss': 3.347, 'grad_norm': 8.426335334777832, 'learning_rate': 3.115e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3363, 'grad_norm': 8.371664047241211, 'learning_rate': 3.1125000000000004e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3201, 'grad_norm': 8.408634185791016, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.53}\n",
      "{'loss': 3.201, 'grad_norm': 8.347785949707031, 'learning_rate': 3.1075e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3577, 'grad_norm': 8.329673767089844, 'learning_rate': 3.105e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3066, 'grad_norm': 8.828568458557129, 'learning_rate': 3.1025e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3435, 'grad_norm': 8.714613914489746, 'learning_rate': 3.1e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3374, 'grad_norm': 7.790698528289795, 'learning_rate': 3.0975e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3809, 'grad_norm': 8.699039459228516, 'learning_rate': 3.095e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3241, 'grad_norm': 8.70656967163086, 'learning_rate': 3.0925000000000006e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3568, 'grad_norm': 8.277366638183594, 'learning_rate': 3.09e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2728, 'grad_norm': 8.910622596740723, 'learning_rate': 3.0875000000000005e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3668, 'grad_norm': 8.058675765991211, 'learning_rate': 3.0850000000000004e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3776, 'grad_norm': 9.450594902038574, 'learning_rate': 3.0825000000000004e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3448, 'grad_norm': 8.00060749053955, 'learning_rate': 3.08e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2261, 'grad_norm': 7.908261299133301, 'learning_rate': 3.0775e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3617, 'grad_norm': 8.438284873962402, 'learning_rate': 3.075e-05, 'epoch': 0.54}\n",
      "{'loss': 3.4922, 'grad_norm': 8.396750450134277, 'learning_rate': 3.0725e-05, 'epoch': 0.54}\n",
      "{'loss': 3.4209, 'grad_norm': 9.469639778137207, 'learning_rate': 3.07e-05, 'epoch': 0.54}\n",
      "{'loss': 3.4887, 'grad_norm': 10.080883979797363, 'learning_rate': 3.067500000000001e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2699, 'grad_norm': 8.408269882202148, 'learning_rate': 3.065e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3572, 'grad_norm': 8.063340187072754, 'learning_rate': 3.0625000000000006e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3287, 'grad_norm': 8.752450942993164, 'learning_rate': 3.06e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3203, 'grad_norm': 8.71566104888916, 'learning_rate': 3.0575000000000005e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2744, 'grad_norm': 8.621907234191895, 'learning_rate': 3.0550000000000004e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3519, 'grad_norm': 8.581875801086426, 'learning_rate': 3.0525e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2584, 'grad_norm': 8.856552124023438, 'learning_rate': 3.05e-05, 'epoch': 0.54}\n",
      "{'loss': 3.4751, 'grad_norm': 9.567561149597168, 'learning_rate': 3.0475000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3439, 'grad_norm': 8.807259559631348, 'learning_rate': 3.045e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3745, 'grad_norm': 9.12044620513916, 'learning_rate': 3.0425000000000004e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3253, 'grad_norm': 9.389494895935059, 'learning_rate': 3.04e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2798, 'grad_norm': 8.950623512268066, 'learning_rate': 3.0375000000000003e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3793, 'grad_norm': 8.185713768005371, 'learning_rate': 3.035e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3499, 'grad_norm': 8.734782218933105, 'learning_rate': 3.0325000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3743, 'grad_norm': 9.045351028442383, 'learning_rate': 3.03e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3968, 'grad_norm': 8.759092330932617, 'learning_rate': 3.0275000000000004e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3104, 'grad_norm': 8.264174461364746, 'learning_rate': 3.025e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3202, 'grad_norm': 7.418924808502197, 'learning_rate': 3.0225000000000003e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2965, 'grad_norm': 8.09614372253418, 'learning_rate': 3.02e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3629, 'grad_norm': 7.838380336761475, 'learning_rate': 3.0175e-05, 'epoch': 0.55}\n",
      "{'loss': 3.4046, 'grad_norm': 7.877453804016113, 'learning_rate': 3.015e-05, 'epoch': 0.55}\n",
      "{'loss': 3.4782, 'grad_norm': 9.558450698852539, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3111, 'grad_norm': 8.898823738098145, 'learning_rate': 3.01e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3885, 'grad_norm': 10.540473937988281, 'learning_rate': 3.0075000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3516, 'grad_norm': 9.48790168762207, 'learning_rate': 3.0050000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1783, 'grad_norm': 8.079160690307617, 'learning_rate': 3.0025000000000005e-05, 'epoch': 0.56}\n",
      "{'loss': 3.4053, 'grad_norm': 7.881382942199707, 'learning_rate': 3e-05, 'epoch': 0.56}\n",
      " 40%|█████████████▌                    | 8000/20000 [4:15:47<5:49:03,  1.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.954284, 'eval_rouge-2': 7.575356, 'eval_rouge-l': 25.402130000000003, 'eval_bleu-4': 0.03727669515261808, 'eval_runtime': 36.6557, 'eval_samples_per_second': 1.364, 'eval_steps_per_second': 0.109, 'epoch': 0.56}\n",
      " 40%|█████████████▌                    | 8000/20000 [4:16:24<5:49:03,  1.75s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.54s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-8000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2186, 'grad_norm': 8.509381294250488, 'learning_rate': 2.9975000000000004e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2837, 'grad_norm': 8.089774131774902, 'learning_rate': 2.995e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3742, 'grad_norm': 8.074478149414062, 'learning_rate': 2.9925000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3717, 'grad_norm': 9.945598602294922, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3216, 'grad_norm': 8.457122802734375, 'learning_rate': 2.9875000000000004e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3017, 'grad_norm': 8.826691627502441, 'learning_rate': 2.985e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2376, 'grad_norm': 8.676111221313477, 'learning_rate': 2.9825000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2038, 'grad_norm': 8.434569358825684, 'learning_rate': 2.98e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2561, 'grad_norm': 8.468635559082031, 'learning_rate': 2.9775000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2666, 'grad_norm': 8.178831100463867, 'learning_rate': 2.975e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2533, 'grad_norm': 9.05565357208252, 'learning_rate': 2.9725000000000004e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3088, 'grad_norm': 8.76010799407959, 'learning_rate': 2.97e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2939, 'grad_norm': 8.690807342529297, 'learning_rate': 2.9675000000000003e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3438, 'grad_norm': 8.771352767944336, 'learning_rate': 2.965e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3401, 'grad_norm': 8.398795127868652, 'learning_rate': 2.9625000000000002e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3459, 'grad_norm': 8.670751571655273, 'learning_rate': 2.96e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3754, 'grad_norm': 8.52798843383789, 'learning_rate': 2.9575000000000004e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3923, 'grad_norm': 8.477265357971191, 'learning_rate': 2.955e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2554, 'grad_norm': 7.9835968017578125, 'learning_rate': 2.9525000000000003e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3837, 'grad_norm': 8.676758766174316, 'learning_rate': 2.95e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3024, 'grad_norm': 7.88584041595459, 'learning_rate': 2.9475e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3184, 'grad_norm': 8.642976760864258, 'learning_rate': 2.945e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3607, 'grad_norm': 9.369315147399902, 'learning_rate': 2.9425000000000004e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2511, 'grad_norm': 8.6898193359375, 'learning_rate': 2.94e-05, 'epoch': 0.58}\n",
      "{'loss': 3.4205, 'grad_norm': 8.614339828491211, 'learning_rate': 2.9375000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3248, 'grad_norm': 8.896505355834961, 'learning_rate': 2.935e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2634, 'grad_norm': 8.970025062561035, 'learning_rate': 2.9325e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3328, 'grad_norm': 8.46622371673584, 'learning_rate': 2.93e-05, 'epoch': 0.58}\n",
      "{'loss': 3.254, 'grad_norm': 9.199069023132324, 'learning_rate': 2.9275000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3352, 'grad_norm': 8.848882675170898, 'learning_rate': 2.925e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3775, 'grad_norm': 8.688691139221191, 'learning_rate': 2.9225000000000002e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3448, 'grad_norm': 8.688804626464844, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3155, 'grad_norm': 9.849897384643555, 'learning_rate': 2.9175e-05, 'epoch': 0.58}\n",
      "{'loss': 3.4307, 'grad_norm': 8.6506986618042, 'learning_rate': 2.915e-05, 'epoch': 0.58}\n",
      "{'loss': 3.435, 'grad_norm': 8.666376113891602, 'learning_rate': 2.9125000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3854, 'grad_norm': 9.522918701171875, 'learning_rate': 2.91e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3411, 'grad_norm': 9.147846221923828, 'learning_rate': 2.9075000000000002e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3874, 'grad_norm': 7.884590148925781, 'learning_rate': 2.9049999999999998e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2272, 'grad_norm': 9.09542465209961, 'learning_rate': 2.9025e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4503, 'grad_norm': 9.513216018676758, 'learning_rate': 2.9e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2634, 'grad_norm': 10.063080787658691, 'learning_rate': 2.8975000000000003e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4188, 'grad_norm': 9.236730575561523, 'learning_rate': 2.895e-05, 'epoch': 0.59}\n",
      "{'loss': 3.257, 'grad_norm': 8.929304122924805, 'learning_rate': 2.8925000000000002e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3243, 'grad_norm': 9.4480562210083, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.59}\n",
      "{'loss': 3.395, 'grad_norm': 9.16240119934082, 'learning_rate': 2.8875e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2859, 'grad_norm': 8.107505798339844, 'learning_rate': 2.885e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3733, 'grad_norm': 8.65893840789795, 'learning_rate': 2.8825000000000003e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4243, 'grad_norm': 9.38355827331543, 'learning_rate': 2.88e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4571, 'grad_norm': 10.869622230529785, 'learning_rate': 2.8775e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3519, 'grad_norm': 8.716512680053711, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.59}\n",
      " 42%|██████████████▍                   | 8500/20000 [4:31:21<5:52:57,  1.84s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.581956, 'eval_rouge-2': 7.497706000000001, 'eval_rouge-l': 24.145972, 'eval_bleu-4': 0.03452454207729247, 'eval_runtime': 80.1185, 'eval_samples_per_second': 0.624, 'eval_steps_per_second': 0.05, 'epoch': 0.59}\n",
      " 42%|██████████████▍                   | 8500/20000 [4:32:42<5:52:57,  1.84s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.36s/it]\u001b[A\n",
      "{'loss': 3.3295, 'grad_norm': 9.04923152923584, 'learning_rate': 2.8725e-05, 'epoch': 0.59}\n",
      "{'loss': 3.25, 'grad_norm': 8.550016403198242, 'learning_rate': 2.87e-05, 'epoch': 0.59}\n",
      "{'loss': 3.4032, 'grad_norm': 8.845670700073242, 'learning_rate': 2.8675000000000002e-05, 'epoch': 0.6}\n",
      "{'loss': 3.392, 'grad_norm': 9.16170883178711, 'learning_rate': 2.865e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3724, 'grad_norm': 8.984169960021973, 'learning_rate': 2.8625e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3445, 'grad_norm': 8.188672065734863, 'learning_rate': 2.86e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3279, 'grad_norm': 8.888473510742188, 'learning_rate': 2.8575000000000003e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2554, 'grad_norm': 8.969303131103516, 'learning_rate': 2.855e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3321, 'grad_norm': 8.531739234924316, 'learning_rate': 2.8525000000000002e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3082, 'grad_norm': 9.406132698059082, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1697, 'grad_norm': 11.093727111816406, 'learning_rate': 2.8475e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3081, 'grad_norm': 8.113755226135254, 'learning_rate': 2.845e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3311, 'grad_norm': 7.939051151275635, 'learning_rate': 2.8425000000000003e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3819, 'grad_norm': 9.471796035766602, 'learning_rate': 2.84e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2709, 'grad_norm': 8.885051727294922, 'learning_rate': 2.8375000000000002e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3197, 'grad_norm': 8.066082954406738, 'learning_rate': 2.8349999999999998e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3418, 'grad_norm': 9.562944412231445, 'learning_rate': 2.8325e-05, 'epoch': 0.61}\n",
      "{'loss': 3.4142, 'grad_norm': 8.449078559875488, 'learning_rate': 2.83e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3745, 'grad_norm': 9.752581596374512, 'learning_rate': 2.8275000000000003e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3321, 'grad_norm': 9.356995582580566, 'learning_rate': 2.825e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2584, 'grad_norm': 9.369159698486328, 'learning_rate': 2.8225e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3546, 'grad_norm': 8.656876564025879, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3991, 'grad_norm': 9.188427925109863, 'learning_rate': 2.8175e-05, 'epoch': 0.61}\n",
      "{'loss': 3.388, 'grad_norm': 9.357714653015137, 'learning_rate': 2.815e-05, 'epoch': 0.61}\n",
      "{'loss': 3.4846, 'grad_norm': 9.493529319763184, 'learning_rate': 2.8125000000000003e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3608, 'grad_norm': 10.067540168762207, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3038, 'grad_norm': 8.664525032043457, 'learning_rate': 2.8075e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2386, 'grad_norm': 8.309734344482422, 'learning_rate': 2.8050000000000004e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3951, 'grad_norm': 8.460983276367188, 'learning_rate': 2.8025e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2482, 'grad_norm': 8.683759689331055, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2906, 'grad_norm': 8.680440902709961, 'learning_rate': 2.7975000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3399, 'grad_norm': 8.588883399963379, 'learning_rate': 2.7950000000000005e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2534, 'grad_norm': 8.919196128845215, 'learning_rate': 2.7925e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3884, 'grad_norm': 8.45510482788086, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2845, 'grad_norm': 8.512483596801758, 'learning_rate': 2.7875e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3867, 'grad_norm': 9.959497451782227, 'learning_rate': 2.7850000000000003e-05, 'epoch': 0.62}\n",
      "{'loss': 3.233, 'grad_norm': 8.642938613891602, 'learning_rate': 2.7825000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3957, 'grad_norm': 8.060338973999023, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.62}\n",
      "{'loss': 3.413, 'grad_norm': 8.406253814697266, 'learning_rate': 2.7775e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3694, 'grad_norm': 9.244855880737305, 'learning_rate': 2.7750000000000004e-05, 'epoch': 0.62}\n",
      "{'loss': 3.36, 'grad_norm': 8.640358924865723, 'learning_rate': 2.7725e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3937, 'grad_norm': 8.896697044372559, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3779, 'grad_norm': 7.966984748840332, 'learning_rate': 2.7675000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.4098, 'grad_norm': 9.29605770111084, 'learning_rate': 2.7650000000000005e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2753, 'grad_norm': 8.662135124206543, 'learning_rate': 2.7625e-05, 'epoch': 0.62}\n",
      "{'loss': 3.4086, 'grad_norm': 8.366379737854004, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.63}\n",
      "{'loss': 3.262, 'grad_norm': 8.768576622009277, 'learning_rate': 2.7575e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2433, 'grad_norm': 9.309447288513184, 'learning_rate': 2.7550000000000002e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3083, 'grad_norm': 8.272226333618164, 'learning_rate': 2.7525e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3582, 'grad_norm': 9.10496997833252, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.63}\n",
      " 45%|███████████████▎                  | 9000/20000 [4:47:34<5:26:54,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.004292, 'eval_rouge-2': 7.634744, 'eval_rouge-l': 22.988681999999997, 'eval_bleu-4': 0.03607763141863782, 'eval_runtime': 79.6709, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.05, 'epoch': 0.63}\n",
      " 45%|███████████████▎                  | 9000/20000 [4:48:53<5:26:54,  1.78s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.19s/it]\u001b[A\n",
      "{'loss': 3.3923, 'grad_norm': 7.985833644866943, 'learning_rate': 2.7475e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4551, 'grad_norm': 9.75617790222168, 'learning_rate': 2.7450000000000003e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4104, 'grad_norm': 9.09487247467041, 'learning_rate': 2.7425e-05, 'epoch': 0.63}\n",
      "{'loss': 3.5079, 'grad_norm': 8.847451210021973, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3425, 'grad_norm': 8.335692405700684, 'learning_rate': 2.7375e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3404, 'grad_norm': 8.733609199523926, 'learning_rate': 2.7350000000000004e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3827, 'grad_norm': 9.211060523986816, 'learning_rate': 2.7325e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4006, 'grad_norm': 9.307429313659668, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3637, 'grad_norm': 8.58265495300293, 'learning_rate': 2.7275e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3144, 'grad_norm': 8.620497703552246, 'learning_rate': 2.725e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3896, 'grad_norm': 8.320236206054688, 'learning_rate': 2.7225e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3288, 'grad_norm': 8.438514709472656, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3752, 'grad_norm': 8.538013458251953, 'learning_rate': 2.7175e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3493, 'grad_norm': 8.564205169677734, 'learning_rate': 2.7150000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3506, 'grad_norm': 9.0146484375, 'learning_rate': 2.7125000000000002e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2363, 'grad_norm': 9.464719772338867, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3804, 'grad_norm': 8.684515953063965, 'learning_rate': 2.7075e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3414, 'grad_norm': 8.863544464111328, 'learning_rate': 2.7050000000000004e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3414, 'grad_norm': 8.780891418457031, 'learning_rate': 2.7025e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2787, 'grad_norm': 9.365216255187988, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3783, 'grad_norm': 9.226901054382324, 'learning_rate': 2.6975000000000002e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3591, 'grad_norm': 9.391069412231445, 'learning_rate': 2.6950000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3825, 'grad_norm': 9.036478996276855, 'learning_rate': 2.6925e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3051, 'grad_norm': 9.288731575012207, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3206, 'grad_norm': 8.312165260314941, 'learning_rate': 2.6875e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2973, 'grad_norm': 8.916117668151855, 'learning_rate': 2.6850000000000002e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2569, 'grad_norm': 8.977853775024414, 'learning_rate': 2.6825e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3443, 'grad_norm': 11.014246940612793, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3385, 'grad_norm': 9.553597450256348, 'learning_rate': 2.6775e-05, 'epoch': 0.65}\n",
      "{'loss': 3.4034, 'grad_norm': 8.93054485321045, 'learning_rate': 2.6750000000000003e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2283, 'grad_norm': 9.859461784362793, 'learning_rate': 2.6725e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3594, 'grad_norm': 9.351571083068848, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3269, 'grad_norm': 8.563721656799316, 'learning_rate': 2.6675e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3687, 'grad_norm': 9.417141914367676, 'learning_rate': 2.6650000000000004e-05, 'epoch': 0.65}\n",
      "{'loss': 3.313, 'grad_norm': 8.515019416809082, 'learning_rate': 2.6625e-05, 'epoch': 0.65}\n",
      "{'loss': 3.322, 'grad_norm': 9.386852264404297, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3286, 'grad_norm': 8.34044361114502, 'learning_rate': 2.6575e-05, 'epoch': 0.65}\n",
      "{'loss': 3.378, 'grad_norm': 9.646300315856934, 'learning_rate': 2.655e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2683, 'grad_norm': 8.062396049499512, 'learning_rate': 2.6525e-05, 'epoch': 0.66}\n",
      "{'loss': 3.341, 'grad_norm': 9.465981483459473, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3643, 'grad_norm': 9.12081527709961, 'learning_rate': 2.6475e-05, 'epoch': 0.66}\n",
      "{'loss': 3.4025, 'grad_norm': 8.181328773498535, 'learning_rate': 2.6450000000000003e-05, 'epoch': 0.66}\n",
      "{'loss': 3.289, 'grad_norm': 8.494585990905762, 'learning_rate': 2.6425e-05, 'epoch': 0.66}\n",
      "{'loss': 3.403, 'grad_norm': 8.58458423614502, 'learning_rate': 2.64e-05, 'epoch': 0.66}\n",
      "{'loss': 3.4041, 'grad_norm': 8.897603988647461, 'learning_rate': 2.6375e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2191, 'grad_norm': 9.443633079528809, 'learning_rate': 2.6350000000000004e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3186, 'grad_norm': 9.133365631103516, 'learning_rate': 2.6325e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3281, 'grad_norm': 9.894105911254883, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2508, 'grad_norm': 8.20612907409668, 'learning_rate': 2.6275e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3451, 'grad_norm': 9.74429702758789, 'learning_rate': 2.625e-05, 'epoch': 0.66}\n",
      " 48%|████████████████▏                 | 9500/20000 [5:03:39<4:58:33,  1.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.05s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.568144000000004, 'eval_rouge-2': 7.432932000000001, 'eval_rouge-l': 25.931638000000003, 'eval_bleu-4': 0.03633132958685731, 'eval_runtime': 16.084, 'eval_samples_per_second': 3.109, 'eval_steps_per_second': 0.249, 'epoch': 0.66}\n",
      " 48%|████████████████▏                 | 9500/20000 [5:03:55<4:58:33,  1.71s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.94s/it]\u001b[A\n",
      "{'loss': 3.327, 'grad_norm': 8.987643241882324, 'learning_rate': 2.6225e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3494, 'grad_norm': 8.61202621459961, 'learning_rate': 2.6200000000000003e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3628, 'grad_norm': 14.11870288848877, 'learning_rate': 2.6175e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3744, 'grad_norm': 7.7115702629089355, 'learning_rate': 2.6150000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2585, 'grad_norm': 8.0686616897583, 'learning_rate': 2.6124999999999998e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2538, 'grad_norm': 9.327679634094238, 'learning_rate': 2.61e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3199, 'grad_norm': 8.792790412902832, 'learning_rate': 2.6075e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3172, 'grad_norm': 8.414822578430176, 'learning_rate': 2.6050000000000003e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2795, 'grad_norm': 7.885467052459717, 'learning_rate': 2.6025e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3842, 'grad_norm': 9.425503730773926, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3909, 'grad_norm': 10.4076566696167, 'learning_rate': 2.5974999999999998e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3073, 'grad_norm': 9.823517799377441, 'learning_rate': 2.595e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2909, 'grad_norm': 9.12685489654541, 'learning_rate': 2.5925e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2932, 'grad_norm': 8.613577842712402, 'learning_rate': 2.5900000000000003e-05, 'epoch': 0.67}\n",
      "{'loss': 3.4228, 'grad_norm': 8.739267349243164, 'learning_rate': 2.5875e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2681, 'grad_norm': 8.896674156188965, 'learning_rate': 2.585e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3669, 'grad_norm': 8.617151260375977, 'learning_rate': 2.5824999999999998e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3036, 'grad_norm': 10.229119300842285, 'learning_rate': 2.58e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2733, 'grad_norm': 8.659751892089844, 'learning_rate': 2.5775e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2827, 'grad_norm': 8.513205528259277, 'learning_rate': 2.5750000000000002e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2563, 'grad_norm': 9.069902420043945, 'learning_rate': 2.5725e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3398, 'grad_norm': 10.573490142822266, 'learning_rate': 2.57e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3774, 'grad_norm': 9.257004737854004, 'learning_rate': 2.5675e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2659, 'grad_norm': 10.058613777160645, 'learning_rate': 2.5650000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2463, 'grad_norm': 8.522985458374023, 'learning_rate': 2.5625e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2847, 'grad_norm': 9.682344436645508, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3649, 'grad_norm': 9.551178932189941, 'learning_rate': 2.5574999999999998e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2797, 'grad_norm': 8.478852272033691, 'learning_rate': 2.555e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2984, 'grad_norm': 8.602294921875, 'learning_rate': 2.5525e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2672, 'grad_norm': 8.394404411315918, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 3.303, 'grad_norm': 8.838970184326172, 'learning_rate': 2.5475e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3702, 'grad_norm': 9.449905395507812, 'learning_rate': 2.5450000000000002e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3424, 'grad_norm': 9.542975425720215, 'learning_rate': 2.5424999999999998e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3776, 'grad_norm': 11.256928443908691, 'learning_rate': 2.54e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3741, 'grad_norm': 9.418084144592285, 'learning_rate': 2.5375e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3024, 'grad_norm': 9.080520629882812, 'learning_rate': 2.5350000000000003e-05, 'epoch': 0.69}\n",
      "{'loss': 3.26, 'grad_norm': 9.278982162475586, 'learning_rate': 2.5325e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3238, 'grad_norm': 10.263582229614258, 'learning_rate': 2.5300000000000002e-05, 'epoch': 0.69}\n",
      "{'loss': 3.4529, 'grad_norm': 8.598548889160156, 'learning_rate': 2.5274999999999998e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3312, 'grad_norm': 9.007479667663574, 'learning_rate': 2.525e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2827, 'grad_norm': 9.127653121948242, 'learning_rate': 2.5225e-05, 'epoch': 0.69}\n",
      "{'loss': 3.288, 'grad_norm': 9.389535903930664, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3114, 'grad_norm': 9.682470321655273, 'learning_rate': 2.5175e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3689, 'grad_norm': 8.380278587341309, 'learning_rate': 2.515e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3527, 'grad_norm': 8.559598922729492, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3021, 'grad_norm': 9.465970039367676, 'learning_rate': 2.51e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2817, 'grad_norm': 9.519747734069824, 'learning_rate': 2.5075e-05, 'epoch': 0.7}\n",
      "{'loss': 3.4231, 'grad_norm': 11.385214805603027, 'learning_rate': 2.5050000000000002e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3727, 'grad_norm': 10.598737716674805, 'learning_rate': 2.5025e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2728, 'grad_norm': 10.041461944580078, 'learning_rate': 2.5e-05, 'epoch': 0.7}\n",
      " 50%|████████████████▌                | 10000/20000 [5:18:56<4:41:33,  1.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:08,  8.98s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.691578, 'eval_rouge-2': 7.569412, 'eval_rouge-l': 24.165357999999998, 'eval_bleu-4': 0.0335176297364349, 'eval_runtime': 58.8889, 'eval_samples_per_second': 0.849, 'eval_steps_per_second': 0.068, 'epoch': 0.7}\n",
      " 50%|████████████████▌                | 10000/20000 [5:19:54<4:41:33,  1.69s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.83s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-10000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3601, 'grad_norm': 8.404804229736328, 'learning_rate': 2.4975e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3047, 'grad_norm': 10.543424606323242, 'learning_rate': 2.495e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3007, 'grad_norm': 8.578962326049805, 'learning_rate': 2.4925000000000003e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3892, 'grad_norm': 8.60184383392334, 'learning_rate': 2.4900000000000002e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3829, 'grad_norm': 9.023423194885254, 'learning_rate': 2.4875e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3064, 'grad_norm': 9.841702461242676, 'learning_rate': 2.485e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2196, 'grad_norm': 10.346033096313477, 'learning_rate': 2.4825e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3566, 'grad_norm': 10.97315788269043, 'learning_rate': 2.48e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2104, 'grad_norm': 8.982351303100586, 'learning_rate': 2.4775000000000003e-05, 'epoch': 0.7}\n",
      "{'loss': 3.4341, 'grad_norm': 8.210248947143555, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3888, 'grad_norm': 8.747343063354492, 'learning_rate': 2.4725e-05, 'epoch': 0.71}\n",
      "{'loss': 3.394, 'grad_norm': 8.08596134185791, 'learning_rate': 2.47e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3377, 'grad_norm': 9.757302284240723, 'learning_rate': 2.4675e-05, 'epoch': 0.71}\n",
      "{'loss': 3.4118, 'grad_norm': 9.11500358581543, 'learning_rate': 2.465e-05, 'epoch': 0.71}\n",
      "{'loss': 3.4398, 'grad_norm': 8.195119857788086, 'learning_rate': 2.4625000000000002e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2741, 'grad_norm': 9.689894676208496, 'learning_rate': 2.46e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3573, 'grad_norm': 8.877565383911133, 'learning_rate': 2.4575e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2354, 'grad_norm': 9.354802131652832, 'learning_rate': 2.455e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3362, 'grad_norm': 9.0706787109375, 'learning_rate': 2.4525e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2986, 'grad_norm': 8.958946228027344, 'learning_rate': 2.45e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3595, 'grad_norm': 10.437993049621582, 'learning_rate': 2.4475000000000002e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3419, 'grad_norm': 9.521345138549805, 'learning_rate': 2.445e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3329, 'grad_norm': 8.9264554977417, 'learning_rate': 2.4425e-05, 'epoch': 0.71}\n",
      "{'loss': 3.275, 'grad_norm': 8.673101425170898, 'learning_rate': 2.44e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2571, 'grad_norm': 9.196338653564453, 'learning_rate': 2.4375e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3747, 'grad_norm': 9.098873138427734, 'learning_rate': 2.435e-05, 'epoch': 0.72}\n",
      "{'loss': 3.4054, 'grad_norm': 9.283742904663086, 'learning_rate': 2.4325000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 3.27, 'grad_norm': 8.875818252563477, 'learning_rate': 2.43e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2356, 'grad_norm': 8.648608207702637, 'learning_rate': 2.4275e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2713, 'grad_norm': 9.068361282348633, 'learning_rate': 2.425e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2671, 'grad_norm': 8.266111373901367, 'learning_rate': 2.4225e-05, 'epoch': 0.72}\n",
      "{'loss': 3.363, 'grad_norm': 8.198685646057129, 'learning_rate': 2.4200000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3426, 'grad_norm': 9.191526412963867, 'learning_rate': 2.4175e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3159, 'grad_norm': 8.903918266296387, 'learning_rate': 2.415e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2498, 'grad_norm': 8.542436599731445, 'learning_rate': 2.4125e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2571, 'grad_norm': 8.940831184387207, 'learning_rate': 2.41e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3358, 'grad_norm': 9.051064491271973, 'learning_rate': 2.4075e-05, 'epoch': 0.72}\n",
      "{'loss': 3.401, 'grad_norm': 8.673885345458984, 'learning_rate': 2.4050000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3515, 'grad_norm': 9.021082878112793, 'learning_rate': 2.4025e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3499, 'grad_norm': 8.124639511108398, 'learning_rate': 2.4e-05, 'epoch': 0.73}\n",
      "{'loss': 3.328, 'grad_norm': 9.544861793518066, 'learning_rate': 2.3975e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2873, 'grad_norm': 11.235461235046387, 'learning_rate': 2.395e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2606, 'grad_norm': 9.439313888549805, 'learning_rate': 2.3925e-05, 'epoch': 0.73}\n",
      "{'loss': 3.4122, 'grad_norm': 9.415196418762207, 'learning_rate': 2.39e-05, 'epoch': 0.73}\n",
      "{'loss': 3.4185, 'grad_norm': 9.476935386657715, 'learning_rate': 2.3875e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3958, 'grad_norm': 9.331331253051758, 'learning_rate': 2.385e-05, 'epoch': 0.73}\n",
      "{'loss': 3.314, 'grad_norm': 8.263710021972656, 'learning_rate': 2.3825e-05, 'epoch': 0.73}\n",
      "{'loss': 3.4168, 'grad_norm': 8.823966979980469, 'learning_rate': 2.38e-05, 'epoch': 0.73}\n",
      "{'loss': 3.4284, 'grad_norm': 8.564638137817383, 'learning_rate': 2.3775e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3326, 'grad_norm': 10.548271179199219, 'learning_rate': 2.375e-05, 'epoch': 0.73}\n",
      " 52%|█████████████████▎               | 10500/20000 [5:34:47<4:22:18,  1.66s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.64s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:11, 11.91s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.721884, 'eval_rouge-2': 8.591232, 'eval_rouge-l': 26.132236000000002, 'eval_bleu-4': 0.03750819118783283, 'eval_runtime': 59.5308, 'eval_samples_per_second': 0.84, 'eval_steps_per_second': 0.067, 'epoch': 0.73}\n",
      " 52%|█████████████████▎               | 10500/20000 [5:35:47<4:22:18,  1.66s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.57s/it]\u001b[A\n",
      "{'loss': 3.3119, 'grad_norm': 9.6748628616333, 'learning_rate': 2.3725e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3677, 'grad_norm': 9.469958305358887, 'learning_rate': 2.37e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3272, 'grad_norm': 9.177736282348633, 'learning_rate': 2.3675e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2709, 'grad_norm': 9.602718353271484, 'learning_rate': 2.365e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3999, 'grad_norm': 8.97397518157959, 'learning_rate': 2.3624999999999998e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2953, 'grad_norm': 9.01484489440918, 'learning_rate': 2.36e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2288, 'grad_norm': 8.817191123962402, 'learning_rate': 2.3575e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3548, 'grad_norm': 8.764972686767578, 'learning_rate': 2.355e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3618, 'grad_norm': 8.624176979064941, 'learning_rate': 2.3525e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3373, 'grad_norm': 9.747809410095215, 'learning_rate': 2.35e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2819, 'grad_norm': 9.409271240234375, 'learning_rate': 2.3475e-05, 'epoch': 0.74}\n",
      "{'loss': 3.304, 'grad_norm': 10.667795181274414, 'learning_rate': 2.345e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3659, 'grad_norm': 8.867452621459961, 'learning_rate': 2.3425000000000004e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2343, 'grad_norm': 10.094764709472656, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3208, 'grad_norm': 8.826553344726562, 'learning_rate': 2.3375000000000002e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2328, 'grad_norm': 8.90722942352295, 'learning_rate': 2.3350000000000002e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2639, 'grad_norm': 9.834299087524414, 'learning_rate': 2.3325e-05, 'epoch': 0.74}\n",
      "{'loss': 3.4046, 'grad_norm': 9.342419624328613, 'learning_rate': 2.3300000000000004e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2628, 'grad_norm': 8.602041244506836, 'learning_rate': 2.3275000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3123, 'grad_norm': 8.916040420532227, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2614, 'grad_norm': 9.009937286376953, 'learning_rate': 2.3225000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 3.4147, 'grad_norm': 9.78149700164795, 'learning_rate': 2.32e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3076, 'grad_norm': 9.3082275390625, 'learning_rate': 2.3175e-05, 'epoch': 0.75}\n",
      "{'loss': 3.4281, 'grad_norm': 9.04173469543457, 'learning_rate': 2.3150000000000004e-05, 'epoch': 0.75}\n",
      "{'loss': 3.257, 'grad_norm': 8.703675270080566, 'learning_rate': 2.3125000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3383, 'grad_norm': 8.306562423706055, 'learning_rate': 2.3100000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2393, 'grad_norm': 8.572412490844727, 'learning_rate': 2.3075000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3596, 'grad_norm': 9.351666450500488, 'learning_rate': 2.305e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3598, 'grad_norm': 8.821188926696777, 'learning_rate': 2.3025e-05, 'epoch': 0.75}\n",
      "{'loss': 3.4598, 'grad_norm': 8.080714225769043, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3017, 'grad_norm': 10.639318466186523, 'learning_rate': 2.2975000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3834, 'grad_norm': 9.560458183288574, 'learning_rate': 2.2950000000000002e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2622, 'grad_norm': 10.827791213989258, 'learning_rate': 2.2925e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2116, 'grad_norm': 9.28830623626709, 'learning_rate': 2.29e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3487, 'grad_norm': 9.825658798217773, 'learning_rate': 2.2875e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3253, 'grad_norm': 9.136488914489746, 'learning_rate': 2.2850000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3415, 'grad_norm': 8.909989356994629, 'learning_rate': 2.2825000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2637, 'grad_norm': 8.903738021850586, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.76}\n",
      "{'loss': 3.444, 'grad_norm': 9.21462345123291, 'learning_rate': 2.2775e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2941, 'grad_norm': 8.88494873046875, 'learning_rate': 2.275e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2812, 'grad_norm': 9.178667068481445, 'learning_rate': 2.2725000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3986, 'grad_norm': 9.096091270446777, 'learning_rate': 2.2700000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3487, 'grad_norm': 9.314355850219727, 'learning_rate': 2.2675000000000002e-05, 'epoch': 0.76}\n",
      "{'loss': 3.277, 'grad_norm': 11.154829978942871, 'learning_rate': 2.265e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3511, 'grad_norm': 9.815876007080078, 'learning_rate': 2.2625e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3761, 'grad_norm': 9.598634719848633, 'learning_rate': 2.26e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2548, 'grad_norm': 9.12088394165039, 'learning_rate': 2.2575000000000003e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3324, 'grad_norm': 9.59311580657959, 'learning_rate': 2.2550000000000003e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3396, 'grad_norm': 8.361433029174805, 'learning_rate': 2.2525000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 3.359, 'grad_norm': 9.864377975463867, 'learning_rate': 2.25e-05, 'epoch': 0.77}\n",
      " 55%|██████████████████▏              | 11000/20000 [5:50:41<4:18:25,  1.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.85547, 'eval_rouge-2': 8.780154, 'eval_rouge-l': 24.165566000000002, 'eval_bleu-4': 0.03889692188310009, 'eval_runtime': 79.6265, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.05, 'epoch': 0.77}\n",
      " 55%|██████████████████▏              | 11000/20000 [5:52:00<4:18:25,  1.72s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.18s/it]\u001b[A\n",
      "{'loss': 3.2608, 'grad_norm': 10.670891761779785, 'learning_rate': 2.2475e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3646, 'grad_norm': 9.068182945251465, 'learning_rate': 2.245e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2638, 'grad_norm': 9.403502464294434, 'learning_rate': 2.2425000000000003e-05, 'epoch': 0.77}\n",
      "{'loss': 3.4004, 'grad_norm': 8.71131706237793, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 3.392, 'grad_norm': 8.981731414794922, 'learning_rate': 2.2375000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3758, 'grad_norm': 10.56971263885498, 'learning_rate': 2.235e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2841, 'grad_norm': 9.545713424682617, 'learning_rate': 2.2325e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2968, 'grad_norm': 8.494715690612793, 'learning_rate': 2.23e-05, 'epoch': 0.77}\n",
      "{'loss': 3.288, 'grad_norm': 9.736089706420898, 'learning_rate': 2.2275000000000003e-05, 'epoch': 0.77}\n",
      "{'loss': 3.332, 'grad_norm': 8.591307640075684, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2696, 'grad_norm': 8.933323860168457, 'learning_rate': 2.2225e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3215, 'grad_norm': 9.246564865112305, 'learning_rate': 2.22e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3275, 'grad_norm': 10.027700424194336, 'learning_rate': 2.2175e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3532, 'grad_norm': 9.592093467712402, 'learning_rate': 2.215e-05, 'epoch': 0.78}\n",
      "{'loss': 3.4172, 'grad_norm': 8.053393363952637, 'learning_rate': 2.2125000000000002e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3203, 'grad_norm': 8.442978858947754, 'learning_rate': 2.2100000000000002e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3391, 'grad_norm': 9.621173858642578, 'learning_rate': 2.2075e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3084, 'grad_norm': 8.766250610351562, 'learning_rate': 2.205e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3397, 'grad_norm': 9.621162414550781, 'learning_rate': 2.2025e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2652, 'grad_norm': 8.562920570373535, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3761, 'grad_norm': 10.463420867919922, 'learning_rate': 2.1975000000000002e-05, 'epoch': 0.78}\n",
      "{'loss': 3.5132, 'grad_norm': 9.578080177307129, 'learning_rate': 2.195e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3977, 'grad_norm': 9.065707206726074, 'learning_rate': 2.1925e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2341, 'grad_norm': 9.427718162536621, 'learning_rate': 2.19e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2084, 'grad_norm': 9.997620582580566, 'learning_rate': 2.1875e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3259, 'grad_norm': 10.135310173034668, 'learning_rate': 2.1850000000000003e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2275, 'grad_norm': 9.975637435913086, 'learning_rate': 2.1825000000000002e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3002, 'grad_norm': 9.373650550842285, 'learning_rate': 2.18e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3556, 'grad_norm': 8.764290809631348, 'learning_rate': 2.1775e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2683, 'grad_norm': 9.165616989135742, 'learning_rate': 2.175e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1886, 'grad_norm': 9.399636268615723, 'learning_rate': 2.1725e-05, 'epoch': 0.79}\n",
      "{'loss': 3.4021, 'grad_norm': 10.641663551330566, 'learning_rate': 2.1700000000000002e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1965, 'grad_norm': 9.453164100646973, 'learning_rate': 2.1675e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2958, 'grad_norm': 10.010629653930664, 'learning_rate': 2.165e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3687, 'grad_norm': 9.435681343078613, 'learning_rate': 2.1625e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3483, 'grad_norm': 9.180628776550293, 'learning_rate': 2.16e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3355, 'grad_norm': 9.47465991973877, 'learning_rate': 2.1575e-05, 'epoch': 0.79}\n",
      "{'loss': 3.4147, 'grad_norm': 9.062487602233887, 'learning_rate': 2.1550000000000002e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2387, 'grad_norm': 9.133707046508789, 'learning_rate': 2.1525e-05, 'epoch': 0.8}\n",
      "{'loss': 3.4122, 'grad_norm': 9.570176124572754, 'learning_rate': 2.15e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2607, 'grad_norm': 10.366138458251953, 'learning_rate': 2.1475e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3198, 'grad_norm': 9.173773765563965, 'learning_rate': 2.145e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3287, 'grad_norm': 9.169341087341309, 'learning_rate': 2.1425e-05, 'epoch': 0.8}\n",
      "{'loss': 3.271, 'grad_norm': 8.485101699829102, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2856, 'grad_norm': 10.022675514221191, 'learning_rate': 2.1375e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2528, 'grad_norm': 8.68523120880127, 'learning_rate': 2.135e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2727, 'grad_norm': 9.133323669433594, 'learning_rate': 2.1325e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2613, 'grad_norm': 8.663824081420898, 'learning_rate': 2.13e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3585, 'grad_norm': 9.398835182189941, 'learning_rate': 2.1275000000000002e-05, 'epoch': 0.8}\n",
      "{'loss': 3.351, 'grad_norm': 8.798792839050293, 'learning_rate': 2.125e-05, 'epoch': 0.8}\n",
      " 57%|██████████████████▉              | 11500/20000 [6:06:51<4:18:39,  1.83s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.935122, 'eval_rouge-2': 8.049602, 'eval_rouge-l': 23.168208, 'eval_bleu-4': 0.03538890594785265, 'eval_runtime': 79.4849, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.05, 'epoch': 0.8}\n",
      " 57%|██████████████████▉              | 11500/20000 [6:08:10<4:18:39,  1.83s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.13s/it]\u001b[A\n",
      "{'loss': 3.3759, 'grad_norm': 9.53986930847168, 'learning_rate': 2.1225e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2741, 'grad_norm': 8.716947555541992, 'learning_rate': 2.12e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2979, 'grad_norm': 9.615489959716797, 'learning_rate': 2.1175e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3933, 'grad_norm': 9.07070255279541, 'learning_rate': 2.115e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3369, 'grad_norm': 9.579345703125, 'learning_rate': 2.1125000000000002e-05, 'epoch': 0.81}\n",
      "{'loss': 3.4082, 'grad_norm': 8.813885688781738, 'learning_rate': 2.11e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2157, 'grad_norm': 9.080644607543945, 'learning_rate': 2.1075e-05, 'epoch': 0.81}\n",
      "{'loss': 3.405, 'grad_norm': 10.164144515991211, 'learning_rate': 2.105e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2952, 'grad_norm': 8.866450309753418, 'learning_rate': 2.1025e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3126, 'grad_norm': 8.529068946838379, 'learning_rate': 2.1e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2125, 'grad_norm': 8.846824645996094, 'learning_rate': 2.0975e-05, 'epoch': 0.81}\n",
      "{'loss': 3.4073, 'grad_norm': 9.794808387756348, 'learning_rate': 2.095e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3636, 'grad_norm': 8.495920181274414, 'learning_rate': 2.0925e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2551, 'grad_norm': 9.353663444519043, 'learning_rate': 2.09e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3557, 'grad_norm': 10.430427551269531, 'learning_rate': 2.0875e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2981, 'grad_norm': 9.594667434692383, 'learning_rate': 2.085e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3159, 'grad_norm': 8.949263572692871, 'learning_rate': 2.0825e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3004, 'grad_norm': 11.320396423339844, 'learning_rate': 2.08e-05, 'epoch': 0.82}\n",
      "{'loss': 3.319, 'grad_norm': 9.027929306030273, 'learning_rate': 2.0775e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2873, 'grad_norm': 9.287087440490723, 'learning_rate': 2.075e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3993, 'grad_norm': 9.084497451782227, 'learning_rate': 2.0725e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3696, 'grad_norm': 9.215984344482422, 'learning_rate': 2.07e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2675, 'grad_norm': 9.38969898223877, 'learning_rate': 2.0675e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2126, 'grad_norm': 9.899369239807129, 'learning_rate': 2.065e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3866, 'grad_norm': 10.083069801330566, 'learning_rate': 2.0625e-05, 'epoch': 0.82}\n",
      "{'loss': 3.266, 'grad_norm': 9.74122142791748, 'learning_rate': 2.06e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3503, 'grad_norm': 12.75369644165039, 'learning_rate': 2.0575e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3559, 'grad_norm': 9.476607322692871, 'learning_rate': 2.055e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3081, 'grad_norm': 8.400245666503906, 'learning_rate': 2.0525e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2508, 'grad_norm': 9.449850082397461, 'learning_rate': 2.05e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3786, 'grad_norm': 9.999091148376465, 'learning_rate': 2.0475e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3632, 'grad_norm': 9.987335205078125, 'learning_rate': 2.045e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2548, 'grad_norm': 9.501914024353027, 'learning_rate': 2.0425e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3227, 'grad_norm': 9.313612937927246, 'learning_rate': 2.04e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3933, 'grad_norm': 9.366479873657227, 'learning_rate': 2.0375e-05, 'epoch': 0.83}\n",
      "{'loss': 3.4619, 'grad_norm': 9.80705738067627, 'learning_rate': 2.035e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3048, 'grad_norm': 10.45674991607666, 'learning_rate': 2.0325e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2938, 'grad_norm': 9.743902206420898, 'learning_rate': 2.0300000000000002e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2799, 'grad_norm': 9.009968757629395, 'learning_rate': 2.0275e-05, 'epoch': 0.83}\n",
      "{'loss': 3.367, 'grad_norm': 9.963912963867188, 'learning_rate': 2.025e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2633, 'grad_norm': 9.120579719543457, 'learning_rate': 2.0225000000000004e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2916, 'grad_norm': 9.504438400268555, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1708, 'grad_norm': 9.584486961364746, 'learning_rate': 2.0175000000000003e-05, 'epoch': 0.83}\n",
      "{'loss': 3.4112, 'grad_norm': 8.920381546020508, 'learning_rate': 2.0150000000000002e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3306, 'grad_norm': 9.8732271194458, 'learning_rate': 2.0125e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2375, 'grad_norm': 9.812725067138672, 'learning_rate': 2.01e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3014, 'grad_norm': 9.107992172241211, 'learning_rate': 2.0075000000000003e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2818, 'grad_norm': 8.056083679199219, 'learning_rate': 2.0050000000000003e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2494, 'grad_norm': 9.610452651977539, 'learning_rate': 2.0025000000000002e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3753, 'grad_norm': 10.14433765411377, 'learning_rate': 2e-05, 'epoch': 0.84}\n",
      " 60%|███████████████████▊             | 12000/20000 [6:23:04<4:07:40,  1.86s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.66s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.25s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.004177999999996, 'eval_rouge-2': 7.879305999999999, 'eval_rouge-l': 25.210582000000002, 'eval_bleu-4': 0.03588249420471887, 'eval_runtime': 38.5034, 'eval_samples_per_second': 1.299, 'eval_steps_per_second': 0.104, 'epoch': 0.84}\n",
      " 60%|███████████████████▊             | 12000/20000 [6:23:42<4:07:40,  1.86s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.04s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-12000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2854, 'grad_norm': 11.230826377868652, 'learning_rate': 1.9975e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3297, 'grad_norm': 9.143268585205078, 'learning_rate': 1.995e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2853, 'grad_norm': 9.57160758972168, 'learning_rate': 1.9925000000000003e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2991, 'grad_norm': 9.707172393798828, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2025, 'grad_norm': 9.24172306060791, 'learning_rate': 1.9875000000000002e-05, 'epoch': 0.84}\n",
      "{'loss': 3.4135, 'grad_norm': 8.812664031982422, 'learning_rate': 1.985e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3027, 'grad_norm': 9.814167976379395, 'learning_rate': 1.9825e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3083, 'grad_norm': 9.60274600982666, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2038, 'grad_norm': 9.513715744018555, 'learning_rate': 1.9775000000000003e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2751, 'grad_norm': 9.26496696472168, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.84}\n",
      "{'loss': 3.4165, 'grad_norm': 8.95362663269043, 'learning_rate': 1.9725000000000002e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3527, 'grad_norm': 9.430614471435547, 'learning_rate': 1.97e-05, 'epoch': 0.85}\n",
      "{'loss': 3.4022, 'grad_norm': 9.10355281829834, 'learning_rate': 1.9675e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3812, 'grad_norm': 9.759068489074707, 'learning_rate': 1.9650000000000003e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3188, 'grad_norm': 9.398042678833008, 'learning_rate': 1.9625000000000003e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2117, 'grad_norm': 9.8120698928833, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2587, 'grad_norm': 8.702398300170898, 'learning_rate': 1.9575e-05, 'epoch': 0.85}\n",
      "{'loss': 3.4515, 'grad_norm': 8.766351699829102, 'learning_rate': 1.955e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2502, 'grad_norm': 8.292562484741211, 'learning_rate': 1.9525e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3012, 'grad_norm': 10.031862258911133, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3265, 'grad_norm': 9.75501537322998, 'learning_rate': 1.9475000000000002e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3478, 'grad_norm': 8.8474702835083, 'learning_rate': 1.9450000000000002e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3314, 'grad_norm': 8.496329307556152, 'learning_rate': 1.9425e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2403, 'grad_norm': 8.856201171875, 'learning_rate': 1.94e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3531, 'grad_norm': 9.905200958251953, 'learning_rate': 1.9375e-05, 'epoch': 0.86}\n",
      "{'loss': 3.265, 'grad_norm': 8.70988655090332, 'learning_rate': 1.9350000000000003e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3241, 'grad_norm': 8.758049011230469, 'learning_rate': 1.9325000000000002e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3209, 'grad_norm': 9.841083526611328, 'learning_rate': 1.93e-05, 'epoch': 0.86}\n",
      "{'loss': 3.276, 'grad_norm': 9.6285400390625, 'learning_rate': 1.9275e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2116, 'grad_norm': 9.601194381713867, 'learning_rate': 1.925e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2972, 'grad_norm': 8.716890335083008, 'learning_rate': 1.9225e-05, 'epoch': 0.86}\n",
      "{'loss': 3.4187, 'grad_norm': 9.333951950073242, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2923, 'grad_norm': 8.463289260864258, 'learning_rate': 1.9175000000000002e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2183, 'grad_norm': 8.897947311401367, 'learning_rate': 1.915e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2554, 'grad_norm': 10.75386905670166, 'learning_rate': 1.9125e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3638, 'grad_norm': 9.114072799682617, 'learning_rate': 1.91e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3391, 'grad_norm': 9.365671157836914, 'learning_rate': 1.9075000000000003e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3541, 'grad_norm': 9.880481719970703, 'learning_rate': 1.9050000000000002e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3023, 'grad_norm': 9.185317993164062, 'learning_rate': 1.9025e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3196, 'grad_norm': 9.899970054626465, 'learning_rate': 1.9e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2424, 'grad_norm': 10.778473854064941, 'learning_rate': 1.8975e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3355, 'grad_norm': 9.337600708007812, 'learning_rate': 1.895e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3934, 'grad_norm': 10.325112342834473, 'learning_rate': 1.8925000000000003e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3033, 'grad_norm': 9.713517189025879, 'learning_rate': 1.8900000000000002e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2846, 'grad_norm': 9.562943458557129, 'learning_rate': 1.8875e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3089, 'grad_norm': 9.308013916015625, 'learning_rate': 1.885e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2907, 'grad_norm': 9.487149238586426, 'learning_rate': 1.8825e-05, 'epoch': 0.87}\n",
      "{'loss': 3.33, 'grad_norm': 9.823296546936035, 'learning_rate': 1.88e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2359, 'grad_norm': 9.24136734008789, 'learning_rate': 1.8775000000000002e-05, 'epoch': 0.87}\n",
      "{'loss': 3.4042, 'grad_norm': 9.51218032836914, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.87}\n",
      " 62%|████████████████████▋            | 12500/20000 [6:38:39<3:36:50,  1.73s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:28<00:08,  8.78s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.738038, 'eval_rouge-2': 7.741174, 'eval_rouge-l': 25.461753999999996, 'eval_bleu-4': 0.0388588539334734, 'eval_runtime': 57.8674, 'eval_samples_per_second': 0.864, 'eval_steps_per_second': 0.069, 'epoch': 0.87}\n",
      " 62%|████████████████████▋            | 12500/20000 [6:39:37<3:36:50,  1.73s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:31<00:00,  6.50s/it]\u001b[A\n",
      "{'loss': 3.3057, 'grad_norm': 9.912164688110352, 'learning_rate': 1.8725e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3052, 'grad_norm': 9.074104309082031, 'learning_rate': 1.87e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2765, 'grad_norm': 8.85905933380127, 'learning_rate': 1.8675e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2523, 'grad_norm': 9.763555526733398, 'learning_rate': 1.865e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3142, 'grad_norm': 9.116138458251953, 'learning_rate': 1.8625000000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 3.274, 'grad_norm': 9.36520767211914, 'learning_rate': 1.86e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3779, 'grad_norm': 9.42896842956543, 'learning_rate': 1.8575e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2923, 'grad_norm': 9.63575267791748, 'learning_rate': 1.855e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2674, 'grad_norm': 8.910633087158203, 'learning_rate': 1.8525e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2614, 'grad_norm': 10.964117050170898, 'learning_rate': 1.85e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3211, 'grad_norm': 9.95592212677002, 'learning_rate': 1.8475000000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 3.305, 'grad_norm': 9.763782501220703, 'learning_rate': 1.845e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2636, 'grad_norm': 10.168180465698242, 'learning_rate': 1.8425e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3157, 'grad_norm': 9.71860122680664, 'learning_rate': 1.84e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3483, 'grad_norm': 9.359020233154297, 'learning_rate': 1.8375e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3395, 'grad_norm': 11.057991981506348, 'learning_rate': 1.8350000000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3786, 'grad_norm': 9.287788391113281, 'learning_rate': 1.8325e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2532, 'grad_norm': 9.896662712097168, 'learning_rate': 1.83e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2803, 'grad_norm': 10.000000953674316, 'learning_rate': 1.8275e-05, 'epoch': 0.89}\n",
      "{'loss': 3.4133, 'grad_norm': 10.23013687133789, 'learning_rate': 1.825e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3069, 'grad_norm': 10.046806335449219, 'learning_rate': 1.8225e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3456, 'grad_norm': 9.50649642944336, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3268, 'grad_norm': 9.090063095092773, 'learning_rate': 1.8175e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2902, 'grad_norm': 10.295595169067383, 'learning_rate': 1.815e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3264, 'grad_norm': 9.62148666381836, 'learning_rate': 1.8125e-05, 'epoch': 0.89}\n",
      "{'loss': 3.237, 'grad_norm': 9.501218795776367, 'learning_rate': 1.81e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3247, 'grad_norm': 9.984148979187012, 'learning_rate': 1.8075e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2669, 'grad_norm': 11.806954383850098, 'learning_rate': 1.805e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3287, 'grad_norm': 9.763014793395996, 'learning_rate': 1.8025e-05, 'epoch': 0.89}\n",
      "{'loss': 3.265, 'grad_norm': 9.551836967468262, 'learning_rate': 1.8e-05, 'epoch': 0.89}\n",
      "{'loss': 3.417, 'grad_norm': 10.371967315673828, 'learning_rate': 1.7975e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2782, 'grad_norm': 9.590555191040039, 'learning_rate': 1.795e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2818, 'grad_norm': 9.127903938293457, 'learning_rate': 1.7925e-05, 'epoch': 0.9}\n",
      "{'loss': 3.302, 'grad_norm': 9.921246528625488, 'learning_rate': 1.79e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3158, 'grad_norm': 8.723220825195312, 'learning_rate': 1.7875e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2701, 'grad_norm': 9.550936698913574, 'learning_rate': 1.785e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3551, 'grad_norm': 9.784112930297852, 'learning_rate': 1.7825e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3372, 'grad_norm': 10.250911712646484, 'learning_rate': 1.78e-05, 'epoch': 0.9}\n",
      "{'loss': 3.284, 'grad_norm': 10.881890296936035, 'learning_rate': 1.7775e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2642, 'grad_norm': 9.386963844299316, 'learning_rate': 1.775e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3757, 'grad_norm': 9.507407188415527, 'learning_rate': 1.7725e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3557, 'grad_norm': 9.838020324707031, 'learning_rate': 1.77e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3438, 'grad_norm': 9.662839889526367, 'learning_rate': 1.7675e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2895, 'grad_norm': 9.46352481842041, 'learning_rate': 1.765e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3243, 'grad_norm': 9.996594429016113, 'learning_rate': 1.7625e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3646, 'grad_norm': 9.69277286529541, 'learning_rate': 1.76e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2744, 'grad_norm': 9.569435119628906, 'learning_rate': 1.7575e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3046, 'grad_norm': 9.85968017578125, 'learning_rate': 1.755e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2647, 'grad_norm': 9.796764373779297, 'learning_rate': 1.7525e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2947, 'grad_norm': 10.302251815795898, 'learning_rate': 1.75e-05, 'epoch': 0.91}\n",
      " 65%|█████████████████████▍           | 13000/20000 [6:54:34<3:23:38,  1.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.16s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:11, 11.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.567851999999995, 'eval_rouge-2': 7.650075999999999, 'eval_rouge-l': 24.872134000000006, 'eval_bleu-4': 0.03611759296618917, 'eval_runtime': 48.4579, 'eval_samples_per_second': 1.032, 'eval_steps_per_second': 0.083, 'epoch': 0.91}\n",
      " 65%|█████████████████████▍           | 13000/20000 [6:55:23<3:23:38,  1.75s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  8.31s/it]\u001b[A\n",
      "{'loss': 3.2509, 'grad_norm': 9.91024112701416, 'learning_rate': 1.7475e-05, 'epoch': 0.91}\n",
      "{'loss': 3.4254, 'grad_norm': 9.76535415649414, 'learning_rate': 1.745e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3115, 'grad_norm': 9.748095512390137, 'learning_rate': 1.7425e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1762, 'grad_norm': 8.98331069946289, 'learning_rate': 1.74e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2916, 'grad_norm': 9.545344352722168, 'learning_rate': 1.7375e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3194, 'grad_norm': 11.09377670288086, 'learning_rate': 1.7349999999999998e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3376, 'grad_norm': 9.542826652526855, 'learning_rate': 1.7325e-05, 'epoch': 0.91}\n",
      "{'loss': 3.44, 'grad_norm': 9.944870948791504, 'learning_rate': 1.73e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3576, 'grad_norm': 9.79713249206543, 'learning_rate': 1.7275e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3192, 'grad_norm': 9.045331001281738, 'learning_rate': 1.725e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3328, 'grad_norm': 10.410002708435059, 'learning_rate': 1.7225e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2533, 'grad_norm': 10.237512588500977, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.92}\n",
      "{'loss': 3.4339, 'grad_norm': 9.652435302734375, 'learning_rate': 1.7175e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2014, 'grad_norm': 10.197832107543945, 'learning_rate': 1.7150000000000004e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3716, 'grad_norm': 10.385782241821289, 'learning_rate': 1.7125000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3572, 'grad_norm': 8.7098970413208, 'learning_rate': 1.7100000000000002e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2826, 'grad_norm': 9.489703178405762, 'learning_rate': 1.7075e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3422, 'grad_norm': 9.953068733215332, 'learning_rate': 1.705e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2643, 'grad_norm': 10.01008129119873, 'learning_rate': 1.7025e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3545, 'grad_norm': 9.145894050598145, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3747, 'grad_norm': 10.2590913772583, 'learning_rate': 1.6975000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 3.36, 'grad_norm': 8.804110527038574, 'learning_rate': 1.6950000000000002e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3415, 'grad_norm': 9.378249168395996, 'learning_rate': 1.6925e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3264, 'grad_norm': 10.76900577545166, 'learning_rate': 1.69e-05, 'epoch': 0.92}\n",
      "{'loss': 3.388, 'grad_norm': 10.778407096862793, 'learning_rate': 1.6875000000000004e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2642, 'grad_norm': 9.829941749572754, 'learning_rate': 1.6850000000000003e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2858, 'grad_norm': 10.02332592010498, 'learning_rate': 1.6825000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3095, 'grad_norm': 11.450743675231934, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2988, 'grad_norm': 8.780508041381836, 'learning_rate': 1.6775e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3642, 'grad_norm': 10.870894432067871, 'learning_rate': 1.675e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2712, 'grad_norm': 10.007346153259277, 'learning_rate': 1.6725000000000003e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3668, 'grad_norm': 8.548048973083496, 'learning_rate': 1.6700000000000003e-05, 'epoch': 0.93}\n",
      "{'loss': 3.374, 'grad_norm': 9.856144905090332, 'learning_rate': 1.6675000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2339, 'grad_norm': 10.801660537719727, 'learning_rate': 1.665e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3051, 'grad_norm': 9.202658653259277, 'learning_rate': 1.6625e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2832, 'grad_norm': 8.519753456115723, 'learning_rate': 1.66e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2443, 'grad_norm': 9.711618423461914, 'learning_rate': 1.6575000000000003e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3661, 'grad_norm': 10.491671562194824, 'learning_rate': 1.6550000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2477, 'grad_norm': 9.420388221740723, 'learning_rate': 1.6525000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3313, 'grad_norm': 9.756265640258789, 'learning_rate': 1.65e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2558, 'grad_norm': 9.622689247131348, 'learning_rate': 1.6475e-05, 'epoch': 0.94}\n",
      "{'loss': 3.257, 'grad_norm': 9.641464233398438, 'learning_rate': 1.645e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2641, 'grad_norm': 10.8447265625, 'learning_rate': 1.6425000000000003e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2638, 'grad_norm': 10.080615997314453, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3382, 'grad_norm': 10.395230293273926, 'learning_rate': 1.6375e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2792, 'grad_norm': 9.823192596435547, 'learning_rate': 1.635e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3629, 'grad_norm': 10.284668922424316, 'learning_rate': 1.6325e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2965, 'grad_norm': 9.620620727539062, 'learning_rate': 1.63e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2909, 'grad_norm': 10.294609069824219, 'learning_rate': 1.6275000000000003e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3427, 'grad_norm': 10.307546615600586, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.94}\n",
      " 68%|██████████████████████▎          | 13500/20000 [7:10:14<3:05:02,  1.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:09,  9.23s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.494454, 'eval_rouge-2': 7.488278, 'eval_rouge-l': 25.214104000000003, 'eval_bleu-4': 0.03703565249516831, 'eval_runtime': 36.9177, 'eval_samples_per_second': 1.354, 'eval_steps_per_second': 0.108, 'epoch': 0.94}\n",
      " 68%|██████████████████████▎          | 13500/20000 [7:10:51<3:05:02,  1.71s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.81s/it]\u001b[A\n",
      "{'loss': 3.3235, 'grad_norm': 10.066788673400879, 'learning_rate': 1.6225e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2765, 'grad_norm': 9.815445899963379, 'learning_rate': 1.62e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3303, 'grad_norm': 10.413047790527344, 'learning_rate': 1.6175e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2585, 'grad_norm': 10.20921802520752, 'learning_rate': 1.6150000000000003e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2863, 'grad_norm': 9.383695602416992, 'learning_rate': 1.6125000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3015, 'grad_norm': 9.128820419311523, 'learning_rate': 1.6100000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.304, 'grad_norm': 8.801519393920898, 'learning_rate': 1.6075e-05, 'epoch': 0.95}\n",
      "{'loss': 3.267, 'grad_norm': 9.517328262329102, 'learning_rate': 1.605e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3256, 'grad_norm': 9.411216735839844, 'learning_rate': 1.6025e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2693, 'grad_norm': 10.339079856872559, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2594, 'grad_norm': 10.051506042480469, 'learning_rate': 1.5975000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3554, 'grad_norm': 10.923595428466797, 'learning_rate': 1.595e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3275, 'grad_norm': 10.012003898620605, 'learning_rate': 1.5925e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3223, 'grad_norm': 9.906033515930176, 'learning_rate': 1.59e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1707, 'grad_norm': 9.451977729797363, 'learning_rate': 1.5875e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2467, 'grad_norm': 10.374595642089844, 'learning_rate': 1.5850000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2969, 'grad_norm': 9.617182731628418, 'learning_rate': 1.5825000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1925, 'grad_norm': 9.485260963439941, 'learning_rate': 1.58e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3305, 'grad_norm': 9.8563232421875, 'learning_rate': 1.5775e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3674, 'grad_norm': 9.921329498291016, 'learning_rate': 1.575e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3316, 'grad_norm': 10.463959693908691, 'learning_rate': 1.5725e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2561, 'grad_norm': 9.881003379821777, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3946, 'grad_norm': 9.677943229675293, 'learning_rate': 1.5675e-05, 'epoch': 0.96}\n",
      "{'loss': 3.289, 'grad_norm': 9.905243873596191, 'learning_rate': 1.565e-05, 'epoch': 0.96}\n",
      "{'loss': 3.295, 'grad_norm': 10.293347358703613, 'learning_rate': 1.5625e-05, 'epoch': 0.96}\n",
      "{'loss': 3.234, 'grad_norm': 10.144431114196777, 'learning_rate': 1.56e-05, 'epoch': 0.96}\n",
      "{'loss': 3.4641, 'grad_norm': 10.567963600158691, 'learning_rate': 1.5575e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2957, 'grad_norm': 9.60225772857666, 'learning_rate': 1.5550000000000002e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3415, 'grad_norm': 9.693567276000977, 'learning_rate': 1.5525e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2126, 'grad_norm': 11.025491714477539, 'learning_rate': 1.55e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3136, 'grad_norm': 9.94996166229248, 'learning_rate': 1.5475e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2831, 'grad_norm': 9.633100509643555, 'learning_rate': 1.545e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2676, 'grad_norm': 9.738037109375, 'learning_rate': 1.5425000000000002e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2979, 'grad_norm': 9.43574333190918, 'learning_rate': 1.54e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1875, 'grad_norm': 8.93725872039795, 'learning_rate': 1.5375e-05, 'epoch': 0.97}\n",
      "{'loss': 3.4227, 'grad_norm': 10.55844783782959, 'learning_rate': 1.535e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3536, 'grad_norm': 9.238805770874023, 'learning_rate': 1.5325e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2259, 'grad_norm': 9.542451858520508, 'learning_rate': 1.53e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2922, 'grad_norm': 9.68125057220459, 'learning_rate': 1.5275000000000002e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2591, 'grad_norm': 9.043646812438965, 'learning_rate': 1.525e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2652, 'grad_norm': 10.139739036560059, 'learning_rate': 1.5225e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1852, 'grad_norm': 9.452787399291992, 'learning_rate': 1.52e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2876, 'grad_norm': 9.251323699951172, 'learning_rate': 1.5175e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3227, 'grad_norm': 10.086432456970215, 'learning_rate': 1.515e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3376, 'grad_norm': 8.864779472351074, 'learning_rate': 1.5125e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2128, 'grad_norm': 8.948346138000488, 'learning_rate': 1.51e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2868, 'grad_norm': 9.333161354064941, 'learning_rate': 1.5075e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2859, 'grad_norm': 10.347017288208008, 'learning_rate': 1.505e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2623, 'grad_norm': 10.725629806518555, 'learning_rate': 1.5025000000000001e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3128, 'grad_norm': 9.326202392578125, 'learning_rate': 1.5e-05, 'epoch': 0.98}\n",
      " 70%|███████████████████████          | 14000/20000 [7:25:48<3:06:28,  1.86s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:09,  9.11s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.295596, 'eval_rouge-2': 8.310858000000001, 'eval_rouge-l': 25.6738, 'eval_bleu-4': 0.037733611296576104, 'eval_runtime': 36.8471, 'eval_samples_per_second': 1.357, 'eval_steps_per_second': 0.109, 'epoch': 0.98}\n",
      " 70%|███████████████████████          | 14000/20000 [7:26:25<3:06:28,  1.86s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.72s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-14000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.1691, 'grad_norm': 9.531155586242676, 'learning_rate': 1.4975e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2986, 'grad_norm': 9.747937202453613, 'learning_rate': 1.4950000000000001e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3176, 'grad_norm': 11.549593925476074, 'learning_rate': 1.4925e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2589, 'grad_norm': 14.705448150634766, 'learning_rate': 1.49e-05, 'epoch': 0.98}\n",
      "{'loss': 3.252, 'grad_norm': 9.350604057312012, 'learning_rate': 1.4875e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2657, 'grad_norm': 9.475957870483398, 'learning_rate': 1.485e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3426, 'grad_norm': 10.205389976501465, 'learning_rate': 1.4825e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3886, 'grad_norm': 9.674574851989746, 'learning_rate': 1.48e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2907, 'grad_norm': 10.160734176635742, 'learning_rate': 1.4775e-05, 'epoch': 0.98}\n",
      "{'loss': 3.324, 'grad_norm': 9.859288215637207, 'learning_rate': 1.475e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2713, 'grad_norm': 10.777536392211914, 'learning_rate': 1.4725e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2543, 'grad_norm': 10.44544792175293, 'learning_rate': 1.47e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3377, 'grad_norm': 10.764427185058594, 'learning_rate': 1.4675e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3231, 'grad_norm': 9.463408470153809, 'learning_rate': 1.465e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3295, 'grad_norm': 10.317044258117676, 'learning_rate': 1.4625e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2182, 'grad_norm': 10.905965805053711, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2252, 'grad_norm': 10.775406837463379, 'learning_rate': 1.4575e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3463, 'grad_norm': 9.114428520202637, 'learning_rate': 1.455e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2769, 'grad_norm': 10.006430625915527, 'learning_rate': 1.4524999999999999e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3151, 'grad_norm': 11.038701057434082, 'learning_rate': 1.45e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3692, 'grad_norm': 9.892518043518066, 'learning_rate': 1.4475e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2563, 'grad_norm': 11.110848426818848, 'learning_rate': 1.4449999999999999e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3121, 'grad_norm': 10.34235668182373, 'learning_rate': 1.4425e-05, 'epoch': 0.99}\n",
      "{'loss': 3.306, 'grad_norm': 10.013562202453613, 'learning_rate': 1.44e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3707, 'grad_norm': 9.754073143005371, 'learning_rate': 1.4374999999999999e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3576, 'grad_norm': 9.742229461669922, 'learning_rate': 1.435e-05, 'epoch': 1.0}\n",
      "{'loss': 3.3377, 'grad_norm': 9.295638084411621, 'learning_rate': 1.4325e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2968, 'grad_norm': 9.612388610839844, 'learning_rate': 1.43e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2409, 'grad_norm': 11.59341812133789, 'learning_rate': 1.4275e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2473, 'grad_norm': 10.85304069519043, 'learning_rate': 1.4249999999999999e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2872, 'grad_norm': 10.620100021362305, 'learning_rate': 1.4225e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2897, 'grad_norm': 9.951943397521973, 'learning_rate': 1.42e-05, 'epoch': 1.0}\n",
      " 72%|███████████████████████▋         | 14325/20000 [7:36:05<2:54:31,  1.85s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "{'loss': 3.2472, 'grad_norm': 10.352850914001465, 'learning_rate': 1.4174999999999999e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2302, 'grad_norm': 9.536287307739258, 'learning_rate': 1.415e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2767, 'grad_norm': 9.9573335647583, 'learning_rate': 1.4125e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2404, 'grad_norm': 9.238701820373535, 'learning_rate': 1.4099999999999999e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2512, 'grad_norm': 10.112046241760254, 'learning_rate': 1.4075e-05, 'epoch': 1.0}\n",
      "{'loss': 3.3023, 'grad_norm': 10.23147201538086, 'learning_rate': 1.4050000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 3.336, 'grad_norm': 9.750383377075195, 'learning_rate': 1.4025000000000002e-05, 'epoch': 1.0}\n",
      "{'loss': 3.3075, 'grad_norm': 10.01672077178955, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 3.3137, 'grad_norm': 10.161158561706543, 'learning_rate': 1.3975000000000003e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2235, 'grad_norm': 9.836533546447754, 'learning_rate': 1.3950000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.3101, 'grad_norm': 10.025880813598633, 'learning_rate': 1.3925000000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2738, 'grad_norm': 9.60312557220459, 'learning_rate': 1.3900000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2127, 'grad_norm': 10.236396789550781, 'learning_rate': 1.3875000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2515, 'grad_norm': 9.49902057647705, 'learning_rate': 1.3850000000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2164, 'grad_norm': 9.467670440673828, 'learning_rate': 1.3825000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2209, 'grad_norm': 9.061349868774414, 'learning_rate': 1.3800000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2325, 'grad_norm': 9.373162269592285, 'learning_rate': 1.3775000000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2301, 'grad_norm': 10.058039665222168, 'learning_rate': 1.3750000000000002e-05, 'epoch': 1.01}\n",
      " 72%|███████████████████████▉         | 14500/20000 [7:41:17<2:46:34,  1.82s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.88s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:12, 12.03s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.805718, 'eval_rouge-2': 7.679934, 'eval_rouge-l': 24.26208, 'eval_bleu-4': 0.03384422020284352, 'eval_runtime': 59.6688, 'eval_samples_per_second': 0.838, 'eval_steps_per_second': 0.067, 'epoch': 1.01}\n",
      " 72%|███████████████████████▉         | 14500/20000 [7:42:17<2:46:34,  1.82s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.54s/it]\u001b[A\n",
      "{'loss': 3.2073, 'grad_norm': 9.714966773986816, 'learning_rate': 1.3725000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.3132, 'grad_norm': 8.958824157714844, 'learning_rate': 1.3700000000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 3.3769, 'grad_norm': 9.51257610321045, 'learning_rate': 1.3675000000000002e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2024, 'grad_norm': 9.911367416381836, 'learning_rate': 1.3650000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2951, 'grad_norm': 9.23801040649414, 'learning_rate': 1.3625e-05, 'epoch': 1.02}\n",
      "{'loss': 3.3909, 'grad_norm': 10.570663452148438, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1513, 'grad_norm': 10.108662605285645, 'learning_rate': 1.3575000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2815, 'grad_norm': 8.77465534210205, 'learning_rate': 1.3550000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1958, 'grad_norm': 11.731242179870605, 'learning_rate': 1.3525000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.292, 'grad_norm': 14.094212532043457, 'learning_rate': 1.3500000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.3496, 'grad_norm': 10.08504581451416, 'learning_rate': 1.3475000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.132, 'grad_norm': 9.584554672241211, 'learning_rate': 1.3450000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2944, 'grad_norm': 10.211788177490234, 'learning_rate': 1.3425000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2433, 'grad_norm': 10.614302635192871, 'learning_rate': 1.3400000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1908, 'grad_norm': 10.47071647644043, 'learning_rate': 1.3375000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2488, 'grad_norm': 9.15246295928955, 'learning_rate': 1.3350000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2645, 'grad_norm': 9.636497497558594, 'learning_rate': 1.3325000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2185, 'grad_norm': 11.644577980041504, 'learning_rate': 1.3300000000000001e-05, 'epoch': 1.02}\n",
      "{'loss': 3.314, 'grad_norm': 9.241490364074707, 'learning_rate': 1.3275e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2853, 'grad_norm': 10.23393726348877, 'learning_rate': 1.3250000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.3434, 'grad_norm': 10.494844436645508, 'learning_rate': 1.3225000000000001e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2506, 'grad_norm': 9.538562774658203, 'learning_rate': 1.32e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2405, 'grad_norm': 9.877090454101562, 'learning_rate': 1.3175000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2856, 'grad_norm': 10.148353576660156, 'learning_rate': 1.3150000000000001e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2154, 'grad_norm': 9.994345664978027, 'learning_rate': 1.3125e-05, 'epoch': 1.03}\n",
      "{'loss': 3.3038, 'grad_norm': 10.018641471862793, 'learning_rate': 1.3100000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.3771, 'grad_norm': 10.054303169250488, 'learning_rate': 1.3075000000000001e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2779, 'grad_norm': 10.469223022460938, 'learning_rate': 1.305e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2759, 'grad_norm': 9.525673866271973, 'learning_rate': 1.3025000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.3, 'grad_norm': 9.449202537536621, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2302, 'grad_norm': 10.234289169311523, 'learning_rate': 1.2975e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1627, 'grad_norm': 10.483590126037598, 'learning_rate': 1.2950000000000001e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2574, 'grad_norm': 9.94469165802002, 'learning_rate': 1.2925e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2756, 'grad_norm': 11.25253963470459, 'learning_rate': 1.29e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1449, 'grad_norm': 10.520256042480469, 'learning_rate': 1.2875000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2562, 'grad_norm': 11.194883346557617, 'learning_rate': 1.285e-05, 'epoch': 1.04}\n",
      "{'loss': 3.286, 'grad_norm': 11.137709617614746, 'learning_rate': 1.2825000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 3.3139, 'grad_norm': 9.987399101257324, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2823, 'grad_norm': 10.967824935913086, 'learning_rate': 1.2775e-05, 'epoch': 1.04}\n",
      "{'loss': 3.3406, 'grad_norm': 12.410964012145996, 'learning_rate': 1.2750000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 3.3806, 'grad_norm': 10.785634994506836, 'learning_rate': 1.2725000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1964, 'grad_norm': 9.526782989501953, 'learning_rate': 1.27e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2328, 'grad_norm': 10.33443832397461, 'learning_rate': 1.2675000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2833, 'grad_norm': 11.055702209472656, 'learning_rate': 1.2650000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.316, 'grad_norm': 10.766284942626953, 'learning_rate': 1.2625e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2746, 'grad_norm': 9.325887680053711, 'learning_rate': 1.2600000000000001e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2361, 'grad_norm': 10.54432487487793, 'learning_rate': 1.2575e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1971, 'grad_norm': 11.912323951721191, 'learning_rate': 1.255e-05, 'epoch': 1.05}\n",
      "{'loss': 3.345, 'grad_norm': 9.757758140563965, 'learning_rate': 1.2525000000000001e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2346, 'grad_norm': 10.451215744018555, 'learning_rate': 1.25e-05, 'epoch': 1.05}\n",
      " 75%|████████████████████████▊        | 15000/20000 [7:57:18<2:27:46,  1.77s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:08<00:08,  4.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:03,  3.91s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.50812, 'eval_rouge-2': 8.144276, 'eval_rouge-l': 25.690364, 'eval_bleu-4': 0.03934119016333733, 'eval_runtime': 18.0079, 'eval_samples_per_second': 2.777, 'eval_steps_per_second': 0.222, 'epoch': 1.05}\n",
      " 75%|████████████████████████▊        | 15000/20000 [7:57:36<2:27:46,  1.77s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.27s/it]\u001b[A\n",
      "{'loss': 3.3203, 'grad_norm': 10.506669998168945, 'learning_rate': 1.2475e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1856, 'grad_norm': 10.450650215148926, 'learning_rate': 1.2450000000000001e-05, 'epoch': 1.05}\n",
      "{'loss': 3.381, 'grad_norm': 10.948080062866211, 'learning_rate': 1.2425e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2279, 'grad_norm': 9.669376373291016, 'learning_rate': 1.24e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2815, 'grad_norm': 9.063690185546875, 'learning_rate': 1.2375000000000001e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2058, 'grad_norm': 9.779428482055664, 'learning_rate': 1.235e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2919, 'grad_norm': 10.720625877380371, 'learning_rate': 1.2325e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2311, 'grad_norm': 9.99144458770752, 'learning_rate': 1.23e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2532, 'grad_norm': 10.203168869018555, 'learning_rate': 1.2275e-05, 'epoch': 1.05}\n",
      "{'loss': 3.3011, 'grad_norm': 9.549912452697754, 'learning_rate': 1.225e-05, 'epoch': 1.05}\n",
      "{'loss': 3.311, 'grad_norm': 9.559918403625488, 'learning_rate': 1.2225e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2146, 'grad_norm': 9.988554000854492, 'learning_rate': 1.22e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3647, 'grad_norm': 10.350525856018066, 'learning_rate': 1.2175e-05, 'epoch': 1.06}\n",
      "{'loss': 3.352, 'grad_norm': 9.925078392028809, 'learning_rate': 1.215e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2685, 'grad_norm': 10.218859672546387, 'learning_rate': 1.2125e-05, 'epoch': 1.06}\n",
      "{'loss': 3.283, 'grad_norm': 9.613143920898438, 'learning_rate': 1.2100000000000001e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3475, 'grad_norm': 9.668062210083008, 'learning_rate': 1.2075e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3582, 'grad_norm': 11.283974647521973, 'learning_rate': 1.205e-05, 'epoch': 1.06}\n",
      "{'loss': 3.208, 'grad_norm': 13.249058723449707, 'learning_rate': 1.2025000000000001e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2127, 'grad_norm': 10.558611869812012, 'learning_rate': 1.2e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2262, 'grad_norm': 9.589672088623047, 'learning_rate': 1.1975e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3056, 'grad_norm': 10.87055778503418, 'learning_rate': 1.195e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3081, 'grad_norm': 10.577816009521484, 'learning_rate': 1.1925e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2784, 'grad_norm': 9.478418350219727, 'learning_rate': 1.19e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3061, 'grad_norm': 9.560147285461426, 'learning_rate': 1.1875e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2509, 'grad_norm': 9.854796409606934, 'learning_rate': 1.185e-05, 'epoch': 1.07}\n",
      "{'loss': 3.3111, 'grad_norm': 9.577096939086914, 'learning_rate': 1.1825e-05, 'epoch': 1.07}\n",
      "{'loss': 3.3166, 'grad_norm': 10.161537170410156, 'learning_rate': 1.18e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2508, 'grad_norm': 10.134380340576172, 'learning_rate': 1.1775e-05, 'epoch': 1.07}\n",
      "{'loss': 3.3102, 'grad_norm': 10.090445518493652, 'learning_rate': 1.175e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1711, 'grad_norm': 11.298315048217773, 'learning_rate': 1.1725e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2863, 'grad_norm': 9.934947967529297, 'learning_rate': 1.1700000000000001e-05, 'epoch': 1.07}\n",
      "{'loss': 3.23, 'grad_norm': 10.534303665161133, 'learning_rate': 1.1675000000000001e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2965, 'grad_norm': 10.268310546875, 'learning_rate': 1.1650000000000002e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2807, 'grad_norm': 11.306475639343262, 'learning_rate': 1.1625000000000001e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2479, 'grad_norm': 9.664436340332031, 'learning_rate': 1.16e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2833, 'grad_norm': 10.193756103515625, 'learning_rate': 1.1575000000000002e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2625, 'grad_norm': 10.02364730834961, 'learning_rate': 1.1550000000000001e-05, 'epoch': 1.07}\n",
      "{'loss': 3.3273, 'grad_norm': 10.261688232421875, 'learning_rate': 1.1525e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2739, 'grad_norm': 10.159210205078125, 'learning_rate': 1.1500000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2811, 'grad_norm': 9.3231840133667, 'learning_rate': 1.1475000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 3.223, 'grad_norm': 9.66285228729248, 'learning_rate': 1.145e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2544, 'grad_norm': 9.86991024017334, 'learning_rate': 1.1425000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2713, 'grad_norm': 10.047510147094727, 'learning_rate': 1.1400000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2721, 'grad_norm': 10.5181303024292, 'learning_rate': 1.1375e-05, 'epoch': 1.08}\n",
      "{'loss': 3.3032, 'grad_norm': 10.730199813842773, 'learning_rate': 1.1350000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 3.312, 'grad_norm': 12.29158878326416, 'learning_rate': 1.1325e-05, 'epoch': 1.08}\n",
      "{'loss': 3.3024, 'grad_norm': 9.768688201904297, 'learning_rate': 1.13e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2475, 'grad_norm': 9.606396675109863, 'learning_rate': 1.1275000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1874, 'grad_norm': 10.108983993530273, 'learning_rate': 1.125e-05, 'epoch': 1.08}\n",
      " 78%|█████████████████████████▌       | 15500/20000 [8:12:32<2:06:14,  1.68s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.740860000000005, 'eval_rouge-2': 7.911541999999998, 'eval_rouge-l': 23.633316000000004, 'eval_bleu-4': 0.034821439559654326, 'eval_runtime': 79.802, 'eval_samples_per_second': 0.627, 'eval_steps_per_second': 0.05, 'epoch': 1.08}\n",
      " 78%|█████████████████████████▌       | 15500/20000 [8:13:52<2:06:14,  1.68s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.26s/it]\u001b[A\n",
      "{'loss': 3.3166, 'grad_norm': 11.034900665283203, 'learning_rate': 1.1225e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1186, 'grad_norm': 10.49906063079834, 'learning_rate': 1.1200000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 3.3252, 'grad_norm': 9.99167251586914, 'learning_rate': 1.1175e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1985, 'grad_norm': 10.444883346557617, 'learning_rate': 1.115e-05, 'epoch': 1.08}\n",
      "{'loss': 3.3128, 'grad_norm': 10.519878387451172, 'learning_rate': 1.1125000000000001e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2556, 'grad_norm': 10.889826774597168, 'learning_rate': 1.11e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2273, 'grad_norm': 10.305375099182129, 'learning_rate': 1.1075e-05, 'epoch': 1.09}\n",
      "{'loss': 3.229, 'grad_norm': 9.909808158874512, 'learning_rate': 1.1050000000000001e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2786, 'grad_norm': 10.510493278503418, 'learning_rate': 1.1025e-05, 'epoch': 1.09}\n",
      "{'loss': 3.3099, 'grad_norm': 10.521686553955078, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.09}\n",
      "{'loss': 3.3239, 'grad_norm': 11.58481216430664, 'learning_rate': 1.0975e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2139, 'grad_norm': 9.925522804260254, 'learning_rate': 1.095e-05, 'epoch': 1.09}\n",
      "{'loss': 3.3386, 'grad_norm': 10.698068618774414, 'learning_rate': 1.0925000000000001e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2309, 'grad_norm': 9.578010559082031, 'learning_rate': 1.09e-05, 'epoch': 1.09}\n",
      "{'loss': 3.269, 'grad_norm': 11.172069549560547, 'learning_rate': 1.0875e-05, 'epoch': 1.09}\n",
      "{'loss': 3.3393, 'grad_norm': 9.980175018310547, 'learning_rate': 1.0850000000000001e-05, 'epoch': 1.09}\n",
      "{'loss': 3.224, 'grad_norm': 9.647331237792969, 'learning_rate': 1.0825e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2793, 'grad_norm': 9.980193138122559, 'learning_rate': 1.08e-05, 'epoch': 1.09}\n",
      "{'loss': 3.3896, 'grad_norm': 10.236950874328613, 'learning_rate': 1.0775000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2973, 'grad_norm': 9.842890739440918, 'learning_rate': 1.075e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2481, 'grad_norm': 10.361900329589844, 'learning_rate': 1.0725e-05, 'epoch': 1.1}\n",
      "{'loss': 3.3202, 'grad_norm': 9.6487455368042, 'learning_rate': 1.0700000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2931, 'grad_norm': 10.065742492675781, 'learning_rate': 1.0675e-05, 'epoch': 1.1}\n",
      "{'loss': 3.3284, 'grad_norm': 10.495052337646484, 'learning_rate': 1.065e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2783, 'grad_norm': 9.419417381286621, 'learning_rate': 1.0625e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2083, 'grad_norm': 10.032422065734863, 'learning_rate': 1.06e-05, 'epoch': 1.1}\n",
      "{'loss': 3.3863, 'grad_norm': 10.392583847045898, 'learning_rate': 1.0575e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0951, 'grad_norm': 10.17306137084961, 'learning_rate': 1.055e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2211, 'grad_norm': 10.653640747070312, 'learning_rate': 1.0525e-05, 'epoch': 1.1}\n",
      "{'loss': 3.3677, 'grad_norm': 9.929423332214355, 'learning_rate': 1.05e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2006, 'grad_norm': 10.57955551147461, 'learning_rate': 1.0475e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1321, 'grad_norm': 9.857004165649414, 'learning_rate': 1.045e-05, 'epoch': 1.1}\n",
      "{'loss': 3.284, 'grad_norm': 10.210325241088867, 'learning_rate': 1.0425e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2967, 'grad_norm': 10.818408966064453, 'learning_rate': 1.04e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2488, 'grad_norm': 11.498946189880371, 'learning_rate': 1.0375e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3198, 'grad_norm': 10.166845321655273, 'learning_rate': 1.035e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3162, 'grad_norm': 9.267169952392578, 'learning_rate': 1.0325e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2627, 'grad_norm': 9.831440925598145, 'learning_rate': 1.03e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2976, 'grad_norm': 11.042062759399414, 'learning_rate': 1.0275e-05, 'epoch': 1.11}\n",
      "{'loss': 3.4146, 'grad_norm': 9.934983253479004, 'learning_rate': 1.025e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3434, 'grad_norm': 9.609278678894043, 'learning_rate': 1.0225e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1895, 'grad_norm': 10.09444522857666, 'learning_rate': 1.02e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3278, 'grad_norm': 9.695651054382324, 'learning_rate': 1.0175e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3045, 'grad_norm': 9.719005584716797, 'learning_rate': 1.0150000000000001e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2703, 'grad_norm': 9.509814262390137, 'learning_rate': 1.0125e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3383, 'grad_norm': 10.936872482299805, 'learning_rate': 1.0100000000000002e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2883, 'grad_norm': 10.321914672851562, 'learning_rate': 1.0075000000000001e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1693, 'grad_norm': 10.33919620513916, 'learning_rate': 1.005e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2898, 'grad_norm': 11.29067611694336, 'learning_rate': 1.0025000000000001e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2443, 'grad_norm': 10.5823392868042, 'learning_rate': 1e-05, 'epoch': 1.12}\n",
      " 80%|██████████████████████████▍      | 16000/20000 [8:28:51<1:58:35,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:08,  9.00s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.295264, 'eval_rouge-2': 8.099932, 'eval_rouge-l': 24.735016, 'eval_bleu-4': 0.03736314294232503, 'eval_runtime': 36.7144, 'eval_samples_per_second': 1.362, 'eval_steps_per_second': 0.109, 'epoch': 1.12}\n",
      " 80%|██████████████████████████▍      | 16000/20000 [8:29:27<1:58:35,  1.78s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  6.85s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-16000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.2566, 'grad_norm': 10.076472282409668, 'learning_rate': 9.975e-06, 'epoch': 1.12}\n",
      "{'loss': 3.3213, 'grad_norm': 10.22731876373291, 'learning_rate': 9.950000000000001e-06, 'epoch': 1.12}\n",
      "{'loss': 3.24, 'grad_norm': 10.084632873535156, 'learning_rate': 9.925e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2828, 'grad_norm': 9.903268814086914, 'learning_rate': 9.900000000000002e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2639, 'grad_norm': 11.011792182922363, 'learning_rate': 9.875000000000001e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2705, 'grad_norm': 10.638463973999023, 'learning_rate': 9.85e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2922, 'grad_norm': 10.939386367797852, 'learning_rate': 9.825000000000002e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2107, 'grad_norm': 10.710464477539062, 'learning_rate': 9.800000000000001e-06, 'epoch': 1.12}\n",
      "{'loss': 3.3034, 'grad_norm': 9.97602367401123, 'learning_rate': 9.775e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2604, 'grad_norm': 9.0872163772583, 'learning_rate': 9.750000000000002e-06, 'epoch': 1.12}\n",
      "{'loss': 3.3134, 'grad_norm': 9.415559768676758, 'learning_rate': 9.725000000000001e-06, 'epoch': 1.12}\n",
      "{'loss': 3.2698, 'grad_norm': 9.841157913208008, 'learning_rate': 9.7e-06, 'epoch': 1.13}\n",
      "{'loss': 3.4371, 'grad_norm': 10.72244930267334, 'learning_rate': 9.675000000000001e-06, 'epoch': 1.13}\n",
      "{'loss': 3.3485, 'grad_norm': 10.253728866577148, 'learning_rate': 9.65e-06, 'epoch': 1.13}\n",
      "{'loss': 3.3215, 'grad_norm': 10.682186126708984, 'learning_rate': 9.625e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2614, 'grad_norm': 9.64588737487793, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2575, 'grad_norm': 9.752280235290527, 'learning_rate': 9.575e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2605, 'grad_norm': 10.617873191833496, 'learning_rate': 9.55e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2708, 'grad_norm': 11.036458969116211, 'learning_rate': 9.525000000000001e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2513, 'grad_norm': 10.219526290893555, 'learning_rate': 9.5e-06, 'epoch': 1.13}\n",
      "{'loss': 3.3173, 'grad_norm': 10.263143539428711, 'learning_rate': 9.475e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2636, 'grad_norm': 11.122788429260254, 'learning_rate': 9.450000000000001e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2303, 'grad_norm': 10.477100372314453, 'learning_rate': 9.425e-06, 'epoch': 1.13}\n",
      "{'loss': 3.252, 'grad_norm': 10.610889434814453, 'learning_rate': 9.4e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2007, 'grad_norm': 9.362702369689941, 'learning_rate': 9.375000000000001e-06, 'epoch': 1.13}\n",
      "{'loss': 3.2978, 'grad_norm': 10.272480964660645, 'learning_rate': 9.35e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3705, 'grad_norm': 10.40832805633545, 'learning_rate': 9.325e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3367, 'grad_norm': 10.197702407836914, 'learning_rate': 9.3e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3021, 'grad_norm': 9.760083198547363, 'learning_rate': 9.275e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2265, 'grad_norm': 10.88643741607666, 'learning_rate': 9.25e-06, 'epoch': 1.14}\n",
      "{'loss': 3.1942, 'grad_norm': 9.811345100402832, 'learning_rate': 9.225e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3368, 'grad_norm': 9.963783264160156, 'learning_rate': 9.2e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3482, 'grad_norm': 10.154352188110352, 'learning_rate': 9.175000000000001e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3041, 'grad_norm': 11.305092811584473, 'learning_rate': 9.15e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2802, 'grad_norm': 10.878857612609863, 'learning_rate': 9.125e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2717, 'grad_norm': 10.362269401550293, 'learning_rate': 9.100000000000001e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2575, 'grad_norm': 10.623725891113281, 'learning_rate': 9.075e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2107, 'grad_norm': 10.258023262023926, 'learning_rate': 9.05e-06, 'epoch': 1.14}\n",
      "{'loss': 3.2418, 'grad_norm': 10.723625183105469, 'learning_rate': 9.025e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3391, 'grad_norm': 9.883929252624512, 'learning_rate': 9e-06, 'epoch': 1.14}\n",
      "{'loss': 3.3108, 'grad_norm': 10.573227882385254, 'learning_rate': 8.975e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2872, 'grad_norm': 11.710644721984863, 'learning_rate': 8.95e-06, 'epoch': 1.15}\n",
      "{'loss': 3.252, 'grad_norm': 10.399621963500977, 'learning_rate': 8.925e-06, 'epoch': 1.15}\n",
      "{'loss': 3.301, 'grad_norm': 10.710458755493164, 'learning_rate': 8.9e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2912, 'grad_norm': 9.944022178649902, 'learning_rate': 8.875e-06, 'epoch': 1.15}\n",
      "{'loss': 3.3478, 'grad_norm': 10.594647407531738, 'learning_rate': 8.85e-06, 'epoch': 1.15}\n",
      "{'loss': 3.202, 'grad_norm': 11.03569507598877, 'learning_rate': 8.825e-06, 'epoch': 1.15}\n",
      "{'loss': 3.3402, 'grad_norm': 9.762737274169922, 'learning_rate': 8.8e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2455, 'grad_norm': 11.444602012634277, 'learning_rate': 8.775e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2675, 'grad_norm': 11.185876846313477, 'learning_rate': 8.75e-06, 'epoch': 1.15}\n",
      " 82%|███████████████████████████▏     | 16500/20000 [8:44:24<1:42:04,  1.75s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:11<00:11,  5.57s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:36<00:13, 13.61s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.840574000000004, 'eval_rouge-2': 7.7160660000000005, 'eval_rouge-l': 25.39715, 'eval_bleu-4': 0.035751897314920844, 'eval_runtime': 43.8931, 'eval_samples_per_second': 1.139, 'eval_steps_per_second': 0.091, 'epoch': 1.15}\n",
      " 82%|███████████████████████████▏     | 16500/20000 [8:45:08<1:42:04,  1.75s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:39<00:00,  9.72s/it]\u001b[A\n",
      "{'loss': 3.3265, 'grad_norm': 10.14814281463623, 'learning_rate': 8.725e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2663, 'grad_norm': 12.103601455688477, 'learning_rate': 8.7e-06, 'epoch': 1.15}\n",
      "{'loss': 3.2004, 'grad_norm': 10.681243896484375, 'learning_rate': 8.674999999999999e-06, 'epoch': 1.15}\n",
      "{'loss': 3.1877, 'grad_norm': 10.767311096191406, 'learning_rate': 8.65e-06, 'epoch': 1.15}\n",
      "{'loss': 3.3417, 'grad_norm': 10.87724781036377, 'learning_rate': 8.625e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2487, 'grad_norm': 10.07546329498291, 'learning_rate': 8.599999999999999e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3011, 'grad_norm': 10.56029987335205, 'learning_rate': 8.575000000000002e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2538, 'grad_norm': 10.528986930847168, 'learning_rate': 8.550000000000001e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2934, 'grad_norm': 11.01703929901123, 'learning_rate': 8.525e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3491, 'grad_norm': 11.01191234588623, 'learning_rate': 8.500000000000002e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3255, 'grad_norm': 11.027786254882812, 'learning_rate': 8.475000000000001e-06, 'epoch': 1.16}\n",
      "{'loss': 3.311, 'grad_norm': 10.897920608520508, 'learning_rate': 8.45e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2224, 'grad_norm': 11.203113555908203, 'learning_rate': 8.425000000000001e-06, 'epoch': 1.16}\n",
      "{'loss': 3.287, 'grad_norm': 11.293512344360352, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2341, 'grad_norm': 10.126218795776367, 'learning_rate': 8.375e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3429, 'grad_norm': 11.353812217712402, 'learning_rate': 8.350000000000001e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3646, 'grad_norm': 10.078951835632324, 'learning_rate': 8.325e-06, 'epoch': 1.16}\n",
      "{'loss': 3.2983, 'grad_norm': 10.488044738769531, 'learning_rate': 8.3e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3327, 'grad_norm': 10.005437850952148, 'learning_rate': 8.275000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3185, 'grad_norm': 10.585933685302734, 'learning_rate': 8.25e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2318, 'grad_norm': 9.723509788513184, 'learning_rate': 8.225e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3037, 'grad_norm': 10.44767951965332, 'learning_rate': 8.200000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2895, 'grad_norm': 10.485260963439941, 'learning_rate': 8.175e-06, 'epoch': 1.17}\n",
      "{'loss': 3.4079, 'grad_norm': 11.565523147583008, 'learning_rate': 8.15e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3475, 'grad_norm': 10.090027809143066, 'learning_rate': 8.125000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3526, 'grad_norm': 10.313276290893555, 'learning_rate': 8.1e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2682, 'grad_norm': 10.017049789428711, 'learning_rate': 8.075000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2303, 'grad_norm': 9.149457931518555, 'learning_rate': 8.050000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.1772, 'grad_norm': 9.480610847473145, 'learning_rate': 8.025e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2629, 'grad_norm': 9.961283683776855, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2122, 'grad_norm': 9.993849754333496, 'learning_rate': 7.975e-06, 'epoch': 1.17}\n",
      "{'loss': 3.3718, 'grad_norm': 9.746110916137695, 'learning_rate': 7.95e-06, 'epoch': 1.17}\n",
      "{'loss': 3.337, 'grad_norm': 10.041630744934082, 'learning_rate': 7.925000000000001e-06, 'epoch': 1.17}\n",
      "{'loss': 3.2888, 'grad_norm': 10.561729431152344, 'learning_rate': 7.9e-06, 'epoch': 1.18}\n",
      "{'loss': 3.25, 'grad_norm': 10.01372241973877, 'learning_rate': 7.875e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2689, 'grad_norm': 9.934992790222168, 'learning_rate': 7.850000000000001e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2207, 'grad_norm': 9.986120223999023, 'learning_rate': 7.825e-06, 'epoch': 1.18}\n",
      "{'loss': 3.3212, 'grad_norm': 9.639222145080566, 'learning_rate': 7.8e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2364, 'grad_norm': 10.457318305969238, 'learning_rate': 7.775000000000001e-06, 'epoch': 1.18}\n",
      "{'loss': 3.289, 'grad_norm': 10.715839385986328, 'learning_rate': 7.75e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2188, 'grad_norm': 10.022965431213379, 'learning_rate': 7.725e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2977, 'grad_norm': 11.107076644897461, 'learning_rate': 7.7e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2972, 'grad_norm': 11.98568344116211, 'learning_rate': 7.675e-06, 'epoch': 1.18}\n",
      "{'loss': 3.3414, 'grad_norm': 10.330138206481934, 'learning_rate': 7.65e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2684, 'grad_norm': 10.25019359588623, 'learning_rate': 7.625e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2908, 'grad_norm': 9.436949729919434, 'learning_rate': 7.6e-06, 'epoch': 1.18}\n",
      "{'loss': 3.3598, 'grad_norm': 10.465523719787598, 'learning_rate': 7.575e-06, 'epoch': 1.18}\n",
      "{'loss': 3.2288, 'grad_norm': 10.187835693359375, 'learning_rate': 7.55e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2072, 'grad_norm': 10.434556007385254, 'learning_rate': 7.525e-06, 'epoch': 1.19}\n",
      "{'loss': 3.1988, 'grad_norm': 11.413975715637207, 'learning_rate': 7.5e-06, 'epoch': 1.19}\n",
      " 85%|████████████████████████████     | 17000/20000 [9:00:03<1:30:15,  1.81s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:28<00:08,  8.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.988504, 'eval_rouge-2': 7.7312259999999995, 'eval_rouge-l': 25.363772000000004, 'eval_bleu-4': 0.0374218892885577, 'eval_runtime': 58.1463, 'eval_samples_per_second': 0.86, 'eval_steps_per_second': 0.069, 'epoch': 1.19}\n",
      " 85%|████████████████████████████     | 17000/20000 [9:01:01<1:30:15,  1.81s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:31<00:00,  6.60s/it]\u001b[A\n",
      "{'loss': 3.2487, 'grad_norm': 9.325082778930664, 'learning_rate': 7.4750000000000004e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2879, 'grad_norm': 10.508495330810547, 'learning_rate': 7.45e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2779, 'grad_norm': 9.726175308227539, 'learning_rate': 7.425e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2152, 'grad_norm': 10.82050895690918, 'learning_rate': 7.4e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2929, 'grad_norm': 10.496984481811523, 'learning_rate': 7.375e-06, 'epoch': 1.19}\n",
      "{'loss': 3.341, 'grad_norm': 10.568764686584473, 'learning_rate': 7.35e-06, 'epoch': 1.19}\n",
      "{'loss': 3.3474, 'grad_norm': 11.043119430541992, 'learning_rate': 7.325e-06, 'epoch': 1.19}\n",
      "{'loss': 3.3647, 'grad_norm': 11.284006118774414, 'learning_rate': 7.2999999999999996e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2263, 'grad_norm': 10.680895805358887, 'learning_rate': 7.275e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2996, 'grad_norm': 10.542892456054688, 'learning_rate': 7.25e-06, 'epoch': 1.19}\n",
      "{'loss': 3.274, 'grad_norm': 10.54758071899414, 'learning_rate': 7.2249999999999994e-06, 'epoch': 1.19}\n",
      "{'loss': 3.2727, 'grad_norm': 10.39729118347168, 'learning_rate': 7.2e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2474, 'grad_norm': 10.950623512268066, 'learning_rate': 7.175e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2787, 'grad_norm': 10.859079360961914, 'learning_rate': 7.15e-06, 'epoch': 1.2}\n",
      "{'loss': 3.4701, 'grad_norm': 10.196551322937012, 'learning_rate': 7.1249999999999995e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2411, 'grad_norm': 11.338123321533203, 'learning_rate': 7.1e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2804, 'grad_norm': 11.52881145477295, 'learning_rate': 7.075e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2332, 'grad_norm': 9.667980194091797, 'learning_rate': 7.049999999999999e-06, 'epoch': 1.2}\n",
      "{'loss': 3.4722, 'grad_norm': 9.99365234375, 'learning_rate': 7.025000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.311, 'grad_norm': 10.287115097045898, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2471, 'grad_norm': 10.90215015411377, 'learning_rate': 6.975000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2153, 'grad_norm': 10.292387962341309, 'learning_rate': 6.950000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2845, 'grad_norm': 10.864143371582031, 'learning_rate': 6.925000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2036, 'grad_norm': 10.234210968017578, 'learning_rate': 6.900000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.196, 'grad_norm': 10.387589454650879, 'learning_rate': 6.875000000000001e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2683, 'grad_norm': 10.773223876953125, 'learning_rate': 6.8500000000000005e-06, 'epoch': 1.2}\n",
      "{'loss': 3.2699, 'grad_norm': 9.573342323303223, 'learning_rate': 6.825000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.4033, 'grad_norm': 10.203888893127441, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2323, 'grad_norm': 10.090109825134277, 'learning_rate': 6.775000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2875, 'grad_norm': 10.384538650512695, 'learning_rate': 6.750000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.3708, 'grad_norm': 10.76308536529541, 'learning_rate': 6.725000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2859, 'grad_norm': 10.705641746520996, 'learning_rate': 6.700000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2344, 'grad_norm': 10.171672821044922, 'learning_rate': 6.6750000000000005e-06, 'epoch': 1.21}\n",
      "{'loss': 3.3403, 'grad_norm': 11.112329483032227, 'learning_rate': 6.650000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.3089, 'grad_norm': 9.975630760192871, 'learning_rate': 6.625000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.249, 'grad_norm': 10.125783920288086, 'learning_rate': 6.6e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2728, 'grad_norm': 10.622087478637695, 'learning_rate': 6.5750000000000006e-06, 'epoch': 1.21}\n",
      "{'loss': 3.374, 'grad_norm': 10.691405296325684, 'learning_rate': 6.550000000000001e-06, 'epoch': 1.21}\n",
      "{'loss': 3.2863, 'grad_norm': 9.752349853515625, 'learning_rate': 6.525e-06, 'epoch': 1.21}\n",
      "{'loss': 3.321, 'grad_norm': 14.96101188659668, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.21}\n",
      "{'loss': 3.4101, 'grad_norm': 10.402650833129883, 'learning_rate': 6.475000000000001e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3108, 'grad_norm': 11.583393096923828, 'learning_rate': 6.45e-06, 'epoch': 1.22}\n",
      "{'loss': 3.2397, 'grad_norm': 9.778804779052734, 'learning_rate': 6.425e-06, 'epoch': 1.22}\n",
      "{'loss': 3.2915, 'grad_norm': 10.699361801147461, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.22}\n",
      "{'loss': 3.2322, 'grad_norm': 10.345932960510254, 'learning_rate': 6.375000000000001e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3782, 'grad_norm': 11.731849670410156, 'learning_rate': 6.35e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3047, 'grad_norm': 10.224952697753906, 'learning_rate': 6.3250000000000004e-06, 'epoch': 1.22}\n",
      "{'loss': 3.128, 'grad_norm': 11.452922821044922, 'learning_rate': 6.300000000000001e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3009, 'grad_norm': 13.16211986541748, 'learning_rate': 6.275e-06, 'epoch': 1.22}\n",
      "{'loss': 3.2472, 'grad_norm': 11.031328201293945, 'learning_rate': 6.25e-06, 'epoch': 1.22}\n",
      " 88%|████████████████████████████▉    | 17500/20000 [9:15:56<1:12:52,  1.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.78s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:28<00:08,  8.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.593140000000005, 'eval_rouge-2': 7.76024, 'eval_rouge-l': 25.368688, 'eval_bleu-4': 0.037177027479624855, 'eval_runtime': 57.7949, 'eval_samples_per_second': 0.865, 'eval_steps_per_second': 0.069, 'epoch': 1.22}\n",
      " 88%|████████████████████████████▉    | 17500/20000 [9:16:54<1:12:52,  1.75s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:31<00:00,  6.48s/it]\u001b[A\n",
      "{'loss': 3.2738, 'grad_norm': 10.670165061950684, 'learning_rate': 6.2250000000000005e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3123, 'grad_norm': 10.950908660888672, 'learning_rate': 6.2e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3528, 'grad_norm': 10.670258522033691, 'learning_rate': 6.175e-06, 'epoch': 1.22}\n",
      "{'loss': 3.187, 'grad_norm': 11.047943115234375, 'learning_rate': 6.15e-06, 'epoch': 1.22}\n",
      "{'loss': 3.3461, 'grad_norm': 11.103484153747559, 'learning_rate': 6.125e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2089, 'grad_norm': 11.626446723937988, 'learning_rate': 6.1e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2055, 'grad_norm': 10.881719589233398, 'learning_rate': 6.075e-06, 'epoch': 1.23}\n",
      "{'loss': 3.3485, 'grad_norm': 11.520298957824707, 'learning_rate': 6.0500000000000005e-06, 'epoch': 1.23}\n",
      "{'loss': 3.3317, 'grad_norm': 10.659991264343262, 'learning_rate': 6.025e-06, 'epoch': 1.23}\n",
      "{'loss': 3.3011, 'grad_norm': 12.57900333404541, 'learning_rate': 6e-06, 'epoch': 1.23}\n",
      "{'loss': 3.3206, 'grad_norm': 11.64780330657959, 'learning_rate': 5.975e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2478, 'grad_norm': 9.895014762878418, 'learning_rate': 5.95e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2753, 'grad_norm': 10.476640701293945, 'learning_rate': 5.925e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2526, 'grad_norm': 10.630667686462402, 'learning_rate': 5.9e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2227, 'grad_norm': 10.668590545654297, 'learning_rate': 5.875e-06, 'epoch': 1.23}\n",
      "{'loss': 3.3299, 'grad_norm': 12.048798561096191, 'learning_rate': 5.850000000000001e-06, 'epoch': 1.23}\n",
      "{'loss': 3.283, 'grad_norm': 11.551539421081543, 'learning_rate': 5.825000000000001e-06, 'epoch': 1.23}\n",
      "{'loss': 3.324, 'grad_norm': 10.672711372375488, 'learning_rate': 5.8e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2065, 'grad_norm': 10.182236671447754, 'learning_rate': 5.775000000000001e-06, 'epoch': 1.23}\n",
      "{'loss': 3.2997, 'grad_norm': 10.268896102905273, 'learning_rate': 5.750000000000001e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3432, 'grad_norm': 10.030898094177246, 'learning_rate': 5.725e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3404, 'grad_norm': 11.155440330505371, 'learning_rate': 5.7000000000000005e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3127, 'grad_norm': 10.47681713104248, 'learning_rate': 5.675000000000001e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2585, 'grad_norm': 10.258957862854004, 'learning_rate': 5.65e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3718, 'grad_norm': 9.950387954711914, 'learning_rate': 5.625e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2788, 'grad_norm': 11.347655296325684, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2452, 'grad_norm': 9.791828155517578, 'learning_rate': 5.575e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3326, 'grad_norm': 11.293906211853027, 'learning_rate': 5.55e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2446, 'grad_norm': 11.344620704650879, 'learning_rate': 5.5250000000000005e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3049, 'grad_norm': 12.48261833190918, 'learning_rate': 5.500000000000001e-06, 'epoch': 1.24}\n",
      "{'loss': 3.1294, 'grad_norm': 9.46496868133545, 'learning_rate': 5.475e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2321, 'grad_norm': 11.476613998413086, 'learning_rate': 5.45e-06, 'epoch': 1.24}\n",
      "{'loss': 3.3453, 'grad_norm': 10.962678909301758, 'learning_rate': 5.4250000000000006e-06, 'epoch': 1.24}\n",
      "{'loss': 3.1757, 'grad_norm': 10.762781143188477, 'learning_rate': 5.4e-06, 'epoch': 1.25}\n",
      "{'loss': 3.1479, 'grad_norm': 9.883736610412598, 'learning_rate': 5.375e-06, 'epoch': 1.25}\n",
      "{'loss': 3.233, 'grad_norm': 10.421320915222168, 'learning_rate': 5.3500000000000004e-06, 'epoch': 1.25}\n",
      "{'loss': 3.2723, 'grad_norm': 12.021389961242676, 'learning_rate': 5.325e-06, 'epoch': 1.25}\n",
      "{'loss': 3.305, 'grad_norm': 11.996023178100586, 'learning_rate': 5.3e-06, 'epoch': 1.25}\n",
      "{'loss': 3.3998, 'grad_norm': 10.492779731750488, 'learning_rate': 5.275e-06, 'epoch': 1.25}\n",
      "{'loss': 3.2022, 'grad_norm': 10.302337646484375, 'learning_rate': 5.25e-06, 'epoch': 1.25}\n",
      "{'loss': 3.292, 'grad_norm': 12.053168296813965, 'learning_rate': 5.225e-06, 'epoch': 1.25}\n",
      "{'loss': 3.348, 'grad_norm': 10.328975677490234, 'learning_rate': 5.2e-06, 'epoch': 1.25}\n",
      "{'loss': 3.1259, 'grad_norm': 10.150985717773438, 'learning_rate': 5.175e-06, 'epoch': 1.25}\n",
      "{'loss': 3.303, 'grad_norm': 11.1254243850708, 'learning_rate': 5.15e-06, 'epoch': 1.25}\n",
      "{'loss': 3.2743, 'grad_norm': 11.004863739013672, 'learning_rate': 5.125e-06, 'epoch': 1.25}\n",
      "{'loss': 3.2044, 'grad_norm': 10.98576831817627, 'learning_rate': 5.1e-06, 'epoch': 1.25}\n",
      "{'loss': 3.206, 'grad_norm': 10.235943794250488, 'learning_rate': 5.0750000000000005e-06, 'epoch': 1.25}\n",
      "{'loss': 3.2558, 'grad_norm': 10.445938110351562, 'learning_rate': 5.050000000000001e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2088, 'grad_norm': 11.172881126403809, 'learning_rate': 5.025e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2945, 'grad_norm': 10.072369575500488, 'learning_rate': 5e-06, 'epoch': 1.26}\n",
      " 90%|█████████████████████████████▋   | 18000/20000 [9:31:46<1:01:23,  1.84s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.46s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:29<00:11, 11.78s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.666926000000004, 'eval_rouge-2': 8.761531999999999, 'eval_rouge-l': 25.988416, 'eval_bleu-4': 0.04051221038094912, 'eval_runtime': 39.7708, 'eval_samples_per_second': 1.257, 'eval_steps_per_second': 0.101, 'epoch': 1.26}\n",
      " 90%|█████████████████████████████▋   | 18000/20000 [9:32:26<1:01:23,  1.84s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  8.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-18000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3738, 'grad_norm': 12.235466003417969, 'learning_rate': 4.975000000000001e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2299, 'grad_norm': 9.966010093688965, 'learning_rate': 4.950000000000001e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2646, 'grad_norm': 11.89281940460205, 'learning_rate': 4.925e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2327, 'grad_norm': 10.917759895324707, 'learning_rate': 4.9000000000000005e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2779, 'grad_norm': 9.813759803771973, 'learning_rate': 4.875000000000001e-06, 'epoch': 1.26}\n",
      "{'loss': 3.1786, 'grad_norm': 11.866744995117188, 'learning_rate': 4.85e-06, 'epoch': 1.26}\n",
      "{'loss': 3.194, 'grad_norm': 10.518562316894531, 'learning_rate': 4.825e-06, 'epoch': 1.26}\n",
      "{'loss': 3.369, 'grad_norm': 11.339118003845215, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.26}\n",
      "{'loss': 3.3528, 'grad_norm': 12.233359336853027, 'learning_rate': 4.775e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2884, 'grad_norm': 11.742830276489258, 'learning_rate': 4.75e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2837, 'grad_norm': 9.877533912658691, 'learning_rate': 4.7250000000000005e-06, 'epoch': 1.26}\n",
      "{'loss': 3.1612, 'grad_norm': 10.060908317565918, 'learning_rate': 4.7e-06, 'epoch': 1.26}\n",
      "{'loss': 3.2622, 'grad_norm': 11.201995849609375, 'learning_rate': 4.675e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3422, 'grad_norm': 9.772138595581055, 'learning_rate': 4.65e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2635, 'grad_norm': 9.638309478759766, 'learning_rate': 4.625e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2802, 'grad_norm': 11.66706371307373, 'learning_rate': 4.6e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3827, 'grad_norm': 11.45609188079834, 'learning_rate': 4.575e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3019, 'grad_norm': 10.660643577575684, 'learning_rate': 4.5500000000000005e-06, 'epoch': 1.27}\n",
      "{'loss': 3.1179, 'grad_norm': 10.460225105285645, 'learning_rate': 4.525e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3067, 'grad_norm': 9.793413162231445, 'learning_rate': 4.5e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2895, 'grad_norm': 10.313949584960938, 'learning_rate': 4.475e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3173, 'grad_norm': 11.118919372558594, 'learning_rate': 4.45e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2414, 'grad_norm': 10.904556274414062, 'learning_rate': 4.425e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2741, 'grad_norm': 10.364582061767578, 'learning_rate': 4.4e-06, 'epoch': 1.27}\n",
      "{'loss': 3.3446, 'grad_norm': 9.4580078125, 'learning_rate': 4.375e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2414, 'grad_norm': 12.432439804077148, 'learning_rate': 4.35e-06, 'epoch': 1.27}\n",
      "{'loss': 3.2169, 'grad_norm': 10.75355052947998, 'learning_rate': 4.325e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2505, 'grad_norm': 10.67586898803711, 'learning_rate': 4.2999999999999995e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2329, 'grad_norm': 10.938401222229004, 'learning_rate': 4.2750000000000006e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3012, 'grad_norm': 10.940996170043945, 'learning_rate': 4.250000000000001e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3088, 'grad_norm': 9.81904125213623, 'learning_rate': 4.225e-06, 'epoch': 1.28}\n",
      "{'loss': 3.235, 'grad_norm': 10.54637336730957, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3854, 'grad_norm': 10.711915016174316, 'learning_rate': 4.175000000000001e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3376, 'grad_norm': 10.81556224822998, 'learning_rate': 4.15e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2794, 'grad_norm': 10.255681037902832, 'learning_rate': 4.125e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2201, 'grad_norm': 11.005790710449219, 'learning_rate': 4.1000000000000006e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3024, 'grad_norm': 12.34191608428955, 'learning_rate': 4.075e-06, 'epoch': 1.28}\n",
      "{'loss': 3.3399, 'grad_norm': 10.583377838134766, 'learning_rate': 4.05e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2685, 'grad_norm': 10.32100772857666, 'learning_rate': 4.0250000000000004e-06, 'epoch': 1.28}\n",
      "{'loss': 3.2388, 'grad_norm': 10.32855224609375, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.28}\n",
      "{'loss': 3.1019, 'grad_norm': 10.11902904510498, 'learning_rate': 3.975e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2788, 'grad_norm': 10.137430191040039, 'learning_rate': 3.95e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2244, 'grad_norm': 11.339875221252441, 'learning_rate': 3.9250000000000005e-06, 'epoch': 1.29}\n",
      "{'loss': 3.3445, 'grad_norm': 10.696089744567871, 'learning_rate': 3.9e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2199, 'grad_norm': 10.46714973449707, 'learning_rate': 3.875e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2229, 'grad_norm': 12.16008186340332, 'learning_rate': 3.85e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2313, 'grad_norm': 10.460930824279785, 'learning_rate': 3.825e-06, 'epoch': 1.29}\n",
      "{'loss': 3.252, 'grad_norm': 12.014293670654297, 'learning_rate': 3.8e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2313, 'grad_norm': 10.295722961425781, 'learning_rate': 3.775e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2547, 'grad_norm': 10.180567741394043, 'learning_rate': 3.75e-06, 'epoch': 1.29}\n",
      " 92%|████████████████████████████████▍  | 18500/20000 [9:47:16<45:03,  1.80s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:25<00:25, 12.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:50<00:17, 17.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.555156000000004, 'eval_rouge-2': 7.651998000000001, 'eval_rouge-l': 25.944471999999998, 'eval_bleu-4': 0.03656681298340865, 'eval_runtime': 57.7234, 'eval_samples_per_second': 0.866, 'eval_steps_per_second': 0.069, 'epoch': 1.29}\n",
      " 92%|████████████████████████████████▍  | 18500/20000 [9:48:14<45:03,  1.80s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:53<00:00, 12.29s/it]\u001b[A\n",
      "{'loss': 3.1553, 'grad_norm': 11.979656219482422, 'learning_rate': 3.725e-06, 'epoch': 1.29}\n",
      "{'loss': 3.3184, 'grad_norm': 10.730342864990234, 'learning_rate': 3.7e-06, 'epoch': 1.29}\n",
      "{'loss': 3.2093, 'grad_norm': 10.204544067382812, 'learning_rate': 3.675e-06, 'epoch': 1.29}\n",
      "{'loss': 3.336, 'grad_norm': 10.178348541259766, 'learning_rate': 3.6499999999999998e-06, 'epoch': 1.29}\n",
      "{'loss': 3.1832, 'grad_norm': 11.386213302612305, 'learning_rate': 3.625e-06, 'epoch': 1.29}\n",
      "{'loss': 3.283, 'grad_norm': 13.301950454711914, 'learning_rate': 3.6e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2959, 'grad_norm': 9.219401359558105, 'learning_rate': 3.575e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2938, 'grad_norm': 10.736551284790039, 'learning_rate': 3.55e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2477, 'grad_norm': 11.198175430297852, 'learning_rate': 3.5249999999999997e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2253, 'grad_norm': 10.091983795166016, 'learning_rate': 3.5000000000000004e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2533, 'grad_norm': 10.204190254211426, 'learning_rate': 3.4750000000000006e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2458, 'grad_norm': 11.705809593200684, 'learning_rate': 3.4500000000000004e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2954, 'grad_norm': 9.627668380737305, 'learning_rate': 3.4250000000000002e-06, 'epoch': 1.3}\n",
      "{'loss': 3.3641, 'grad_norm': 10.72994613647461, 'learning_rate': 3.4000000000000005e-06, 'epoch': 1.3}\n",
      "{'loss': 3.3199, 'grad_norm': 10.576194763183594, 'learning_rate': 3.3750000000000003e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2579, 'grad_norm': 11.399569511413574, 'learning_rate': 3.3500000000000005e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2014, 'grad_norm': 9.820486068725586, 'learning_rate': 3.3250000000000004e-06, 'epoch': 1.3}\n",
      "{'loss': 3.1872, 'grad_norm': 10.352654457092285, 'learning_rate': 3.3e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2575, 'grad_norm': 11.496384620666504, 'learning_rate': 3.2750000000000004e-06, 'epoch': 1.3}\n",
      "{'loss': 3.2594, 'grad_norm': 10.271245002746582, 'learning_rate': 3.2500000000000002e-06, 'epoch': 1.31}\n",
      "{'loss': 3.1822, 'grad_norm': 9.94224739074707, 'learning_rate': 3.225e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2713, 'grad_norm': 10.790592193603516, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2177, 'grad_norm': 10.941625595092773, 'learning_rate': 3.175e-06, 'epoch': 1.31}\n",
      "{'loss': 3.3277, 'grad_norm': 10.958718299865723, 'learning_rate': 3.1500000000000003e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2157, 'grad_norm': 13.798233032226562, 'learning_rate': 3.125e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2556, 'grad_norm': 10.543850898742676, 'learning_rate': 3.1e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2891, 'grad_norm': 11.055509567260742, 'learning_rate': 3.075e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2368, 'grad_norm': 10.156156539916992, 'learning_rate': 3.05e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2786, 'grad_norm': 10.059282302856445, 'learning_rate': 3.0250000000000003e-06, 'epoch': 1.31}\n",
      "{'loss': 3.3019, 'grad_norm': 10.670690536499023, 'learning_rate': 3e-06, 'epoch': 1.31}\n",
      "{'loss': 3.229, 'grad_norm': 11.595919609069824, 'learning_rate': 2.975e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2792, 'grad_norm': 10.956016540527344, 'learning_rate': 2.95e-06, 'epoch': 1.31}\n",
      "{'loss': 3.1899, 'grad_norm': 10.358284950256348, 'learning_rate': 2.9250000000000004e-06, 'epoch': 1.31}\n",
      "{'loss': 3.2312, 'grad_norm': 11.116808891296387, 'learning_rate': 2.9e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2817, 'grad_norm': 10.400055885314941, 'learning_rate': 2.8750000000000004e-06, 'epoch': 1.32}\n",
      "{'loss': 3.241, 'grad_norm': 11.012544631958008, 'learning_rate': 2.8500000000000002e-06, 'epoch': 1.32}\n",
      "{'loss': 3.253, 'grad_norm': 10.758316040039062, 'learning_rate': 2.825e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2478, 'grad_norm': 10.636685371398926, 'learning_rate': 2.8000000000000003e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3916, 'grad_norm': 10.779390335083008, 'learning_rate': 2.775e-06, 'epoch': 1.32}\n",
      "{'loss': 3.236, 'grad_norm': 10.009954452514648, 'learning_rate': 2.7500000000000004e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2471, 'grad_norm': 10.056300163269043, 'learning_rate': 2.725e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3525, 'grad_norm': 9.90622615814209, 'learning_rate': 2.7e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2513, 'grad_norm': 10.98635196685791, 'learning_rate': 2.6750000000000002e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2929, 'grad_norm': 11.029491424560547, 'learning_rate': 2.65e-06, 'epoch': 1.32}\n",
      "{'loss': 3.2296, 'grad_norm': 10.185382843017578, 'learning_rate': 2.625e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3213, 'grad_norm': 11.371849060058594, 'learning_rate': 2.6e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3102, 'grad_norm': 10.487178802490234, 'learning_rate': 2.575e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3076, 'grad_norm': 10.439305305480957, 'learning_rate': 2.55e-06, 'epoch': 1.32}\n",
      "{'loss': 3.3099, 'grad_norm': 10.258112907409668, 'learning_rate': 2.5250000000000004e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2603, 'grad_norm': 10.826441764831543, 'learning_rate': 2.5e-06, 'epoch': 1.33}\n",
      " 95%|████████████████████████████████▎ | 19000/20000 [10:03:07<28:20,  1.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.29s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.81s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.676048, 'eval_rouge-2': 8.122458, 'eval_rouge-l': 26.117378000000002, 'eval_bleu-4': 0.03909494569258829, 'eval_runtime': 36.9236, 'eval_samples_per_second': 1.354, 'eval_steps_per_second': 0.108, 'epoch': 1.33}\n",
      " 95%|████████████████████████████████▎ | 19000/20000 [10:03:44<28:20,  1.70s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.69s/it]\u001b[A\n",
      "{'loss': 3.2522, 'grad_norm': 11.1271390914917, 'learning_rate': 2.4750000000000004e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2347, 'grad_norm': 11.107172012329102, 'learning_rate': 2.4500000000000003e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2907, 'grad_norm': 10.697492599487305, 'learning_rate': 2.425e-06, 'epoch': 1.33}\n",
      "{'loss': 3.3182, 'grad_norm': 11.038741111755371, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2109, 'grad_norm': 10.991418838500977, 'learning_rate': 2.375e-06, 'epoch': 1.33}\n",
      "{'loss': 3.197, 'grad_norm': 10.405055046081543, 'learning_rate': 2.35e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2392, 'grad_norm': 10.457305908203125, 'learning_rate': 2.325e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2586, 'grad_norm': 11.409502983093262, 'learning_rate': 2.3e-06, 'epoch': 1.33}\n",
      "{'loss': 3.3019, 'grad_norm': 10.848549842834473, 'learning_rate': 2.2750000000000002e-06, 'epoch': 1.33}\n",
      "{'loss': 3.3158, 'grad_norm': 9.88620662689209, 'learning_rate': 2.25e-06, 'epoch': 1.33}\n",
      "{'loss': 3.3374, 'grad_norm': 10.417755126953125, 'learning_rate': 2.225e-06, 'epoch': 1.33}\n",
      "{'loss': 3.2013, 'grad_norm': 10.156959533691406, 'learning_rate': 2.2e-06, 'epoch': 1.33}\n",
      "{'loss': 3.3171, 'grad_norm': 10.785326957702637, 'learning_rate': 2.175e-06, 'epoch': 1.34}\n",
      "{'loss': 3.2216, 'grad_norm': 10.447501182556152, 'learning_rate': 2.1499999999999997e-06, 'epoch': 1.34}\n",
      "{'loss': 3.1975, 'grad_norm': 11.014359474182129, 'learning_rate': 2.1250000000000004e-06, 'epoch': 1.34}\n",
      "{'loss': 3.3655, 'grad_norm': 10.576313018798828, 'learning_rate': 2.1000000000000002e-06, 'epoch': 1.34}\n",
      "{'loss': 3.1902, 'grad_norm': 10.263740539550781, 'learning_rate': 2.075e-06, 'epoch': 1.34}\n",
      "{'loss': 3.1771, 'grad_norm': 10.103690147399902, 'learning_rate': 2.0500000000000003e-06, 'epoch': 1.34}\n",
      "{'loss': 3.2634, 'grad_norm': 10.307394027709961, 'learning_rate': 2.025e-06, 'epoch': 1.34}\n",
      "{'loss': 3.2591, 'grad_norm': 11.132019996643066, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.34}\n",
      "{'loss': 3.4202, 'grad_norm': 10.856959342956543, 'learning_rate': 1.975e-06, 'epoch': 1.34}\n",
      "{'loss': 3.3021, 'grad_norm': 12.212018013000488, 'learning_rate': 1.95e-06, 'epoch': 1.34}\n",
      "{'loss': 3.3074, 'grad_norm': 12.73758316040039, 'learning_rate': 1.925e-06, 'epoch': 1.34}\n",
      "{'loss': 3.1734, 'grad_norm': 10.567501068115234, 'learning_rate': 1.9e-06, 'epoch': 1.34}\n",
      "{'loss': 3.3295, 'grad_norm': 10.732100486755371, 'learning_rate': 1.875e-06, 'epoch': 1.34}\n",
      "{'loss': 3.257, 'grad_norm': 11.275014877319336, 'learning_rate': 1.85e-06, 'epoch': 1.34}\n",
      "{'loss': 3.1185, 'grad_norm': 10.81987190246582, 'learning_rate': 1.8249999999999999e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2382, 'grad_norm': 10.83164119720459, 'learning_rate': 1.8e-06, 'epoch': 1.35}\n",
      "{'loss': 3.3304, 'grad_norm': 12.314800262451172, 'learning_rate': 1.775e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2356, 'grad_norm': 9.931624412536621, 'learning_rate': 1.7500000000000002e-06, 'epoch': 1.35}\n",
      "{'loss': 3.1678, 'grad_norm': 12.036752700805664, 'learning_rate': 1.7250000000000002e-06, 'epoch': 1.35}\n",
      "{'loss': 3.3518, 'grad_norm': 13.583553314208984, 'learning_rate': 1.7000000000000002e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2574, 'grad_norm': 12.936001777648926, 'learning_rate': 1.6750000000000003e-06, 'epoch': 1.35}\n",
      "{'loss': 3.216, 'grad_norm': 12.795628547668457, 'learning_rate': 1.65e-06, 'epoch': 1.35}\n",
      "{'loss': 3.3227, 'grad_norm': 11.672438621520996, 'learning_rate': 1.6250000000000001e-06, 'epoch': 1.35}\n",
      "{'loss': 3.3282, 'grad_norm': 10.3504056930542, 'learning_rate': 1.6000000000000001e-06, 'epoch': 1.35}\n",
      "{'loss': 3.3023, 'grad_norm': 11.982488632202148, 'learning_rate': 1.5750000000000002e-06, 'epoch': 1.35}\n",
      "{'loss': 3.1935, 'grad_norm': 10.424955368041992, 'learning_rate': 1.55e-06, 'epoch': 1.35}\n",
      "{'loss': 3.388, 'grad_norm': 10.438699722290039, 'learning_rate': 1.525e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2564, 'grad_norm': 10.805522918701172, 'learning_rate': 1.5e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2595, 'grad_norm': 10.291069984436035, 'learning_rate': 1.475e-06, 'epoch': 1.35}\n",
      "{'loss': 3.2376, 'grad_norm': 11.119353294372559, 'learning_rate': 1.45e-06, 'epoch': 1.36}\n",
      "{'loss': 3.3767, 'grad_norm': 10.529945373535156, 'learning_rate': 1.4250000000000001e-06, 'epoch': 1.36}\n",
      "{'loss': 3.4111, 'grad_norm': 12.497526168823242, 'learning_rate': 1.4000000000000001e-06, 'epoch': 1.36}\n",
      "{'loss': 3.3059, 'grad_norm': 10.074193000793457, 'learning_rate': 1.3750000000000002e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2678, 'grad_norm': 11.498295783996582, 'learning_rate': 1.35e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2608, 'grad_norm': 10.032225608825684, 'learning_rate': 1.325e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2869, 'grad_norm': 10.841329574584961, 'learning_rate': 1.3e-06, 'epoch': 1.36}\n",
      "{'loss': 3.3096, 'grad_norm': 9.744556427001953, 'learning_rate': 1.275e-06, 'epoch': 1.36}\n",
      "{'loss': 3.3028, 'grad_norm': 11.062294960021973, 'learning_rate': 1.25e-06, 'epoch': 1.36}\n",
      " 98%|█████████████████████████████████▏| 19500/20000 [10:18:40<15:30,  1.86s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:06<00:06,  3.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:31<00:12, 12.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.264478, 'eval_rouge-2': 8.692954, 'eval_rouge-l': 26.298659999999995, 'eval_bleu-4': 0.04004470611110035, 'eval_runtime': 60.0864, 'eval_samples_per_second': 0.832, 'eval_steps_per_second': 0.067, 'epoch': 1.36}\n",
      " 98%|█████████████████████████████████▏| 19500/20000 [10:19:40<15:30,  1.86s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.63s/it]\u001b[A\n",
      "{'loss': 3.0851, 'grad_norm': 10.5592622756958, 'learning_rate': 1.2250000000000001e-06, 'epoch': 1.36}\n",
      "{'loss': 3.3791, 'grad_norm': 11.033915519714355, 'learning_rate': 1.2000000000000002e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2364, 'grad_norm': 9.838501930236816, 'learning_rate': 1.175e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2898, 'grad_norm': 9.947989463806152, 'learning_rate': 1.15e-06, 'epoch': 1.36}\n",
      "{'loss': 3.1688, 'grad_norm': 10.295293807983398, 'learning_rate': 1.125e-06, 'epoch': 1.36}\n",
      "{'loss': 3.2314, 'grad_norm': 10.142358779907227, 'learning_rate': 1.1e-06, 'epoch': 1.37}\n",
      "{'loss': 3.313, 'grad_norm': 10.592622756958008, 'learning_rate': 1.0749999999999999e-06, 'epoch': 1.37}\n",
      "{'loss': 3.2207, 'grad_norm': 10.507253646850586, 'learning_rate': 1.0500000000000001e-06, 'epoch': 1.37}\n",
      "{'loss': 3.3687, 'grad_norm': 10.559762001037598, 'learning_rate': 1.0250000000000001e-06, 'epoch': 1.37}\n",
      "{'loss': 3.2989, 'grad_norm': 10.277057647705078, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.37}\n",
      "{'loss': 3.3382, 'grad_norm': 10.199384689331055, 'learning_rate': 9.75e-07, 'epoch': 1.37}\n",
      "{'loss': 3.306, 'grad_norm': 10.692338943481445, 'learning_rate': 9.5e-07, 'epoch': 1.37}\n",
      "{'loss': 3.3526, 'grad_norm': 10.634507179260254, 'learning_rate': 9.25e-07, 'epoch': 1.37}\n",
      "{'loss': 3.2718, 'grad_norm': 10.790084838867188, 'learning_rate': 9e-07, 'epoch': 1.37}\n",
      "{'loss': 3.2735, 'grad_norm': 10.11281967163086, 'learning_rate': 8.750000000000001e-07, 'epoch': 1.37}\n",
      "{'loss': 3.1999, 'grad_norm': 9.627643585205078, 'learning_rate': 8.500000000000001e-07, 'epoch': 1.37}\n",
      "{'loss': 3.3362, 'grad_norm': 12.091151237487793, 'learning_rate': 8.25e-07, 'epoch': 1.37}\n",
      "{'loss': 3.3062, 'grad_norm': 12.874615669250488, 'learning_rate': 8.000000000000001e-07, 'epoch': 1.37}\n",
      "{'loss': 3.2469, 'grad_norm': 10.780401229858398, 'learning_rate': 7.75e-07, 'epoch': 1.37}\n",
      "{'loss': 3.3038, 'grad_norm': 10.944927215576172, 'learning_rate': 7.5e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2462, 'grad_norm': 11.465524673461914, 'learning_rate': 7.25e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2957, 'grad_norm': 10.792900085449219, 'learning_rate': 7.000000000000001e-07, 'epoch': 1.38}\n",
      "{'loss': 3.111, 'grad_norm': 10.140108108520508, 'learning_rate': 6.75e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2529, 'grad_norm': 11.804705619812012, 'learning_rate': 6.5e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2373, 'grad_norm': 10.79831600189209, 'learning_rate': 6.25e-07, 'epoch': 1.38}\n",
      "{'loss': 3.3417, 'grad_norm': 11.12575912475586, 'learning_rate': 6.000000000000001e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2677, 'grad_norm': 13.474159240722656, 'learning_rate': 5.75e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2437, 'grad_norm': 10.722307205200195, 'learning_rate': 5.5e-07, 'epoch': 1.38}\n",
      "{'loss': 3.237, 'grad_norm': 10.969828605651855, 'learning_rate': 5.250000000000001e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2739, 'grad_norm': 10.520224571228027, 'learning_rate': 5.000000000000001e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2284, 'grad_norm': 11.706355094909668, 'learning_rate': 4.75e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2199, 'grad_norm': 10.77894401550293, 'learning_rate': 4.5e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2563, 'grad_norm': 10.463370323181152, 'learning_rate': 4.2500000000000006e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2108, 'grad_norm': 11.570136070251465, 'learning_rate': 4.0000000000000003e-07, 'epoch': 1.38}\n",
      "{'loss': 3.2021, 'grad_norm': 10.75749683380127, 'learning_rate': 3.75e-07, 'epoch': 1.39}\n",
      "{'loss': 3.169, 'grad_norm': 10.581660270690918, 'learning_rate': 3.5000000000000004e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2539, 'grad_norm': 11.446451187133789, 'learning_rate': 3.25e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2868, 'grad_norm': 10.822022438049316, 'learning_rate': 3.0000000000000004e-07, 'epoch': 1.39}\n",
      "{'loss': 3.1966, 'grad_norm': 10.517337799072266, 'learning_rate': 2.75e-07, 'epoch': 1.39}\n",
      "{'loss': 3.1571, 'grad_norm': 10.991250991821289, 'learning_rate': 2.5000000000000004e-07, 'epoch': 1.39}\n",
      "{'loss': 3.399, 'grad_norm': 11.901219367980957, 'learning_rate': 2.25e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2909, 'grad_norm': 10.209939002990723, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2833, 'grad_norm': 10.958701133728027, 'learning_rate': 1.7500000000000002e-07, 'epoch': 1.39}\n",
      "{'loss': 3.3045, 'grad_norm': 10.569751739501953, 'learning_rate': 1.5000000000000002e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2071, 'grad_norm': 10.862107276916504, 'learning_rate': 1.2500000000000002e-07, 'epoch': 1.39}\n",
      "{'loss': 3.1954, 'grad_norm': 10.570624351501465, 'learning_rate': 1.0000000000000001e-07, 'epoch': 1.39}\n",
      "{'loss': 3.2412, 'grad_norm': 10.247922897338867, 'learning_rate': 7.500000000000001e-08, 'epoch': 1.39}\n",
      "{'loss': 3.3048, 'grad_norm': 10.620966911315918, 'learning_rate': 5.0000000000000004e-08, 'epoch': 1.39}\n",
      "{'loss': 3.2771, 'grad_norm': 10.917610168457031, 'learning_rate': 2.5000000000000002e-08, 'epoch': 1.4}\n",
      "{'loss': 3.2345, 'grad_norm': 14.107661247253418, 'learning_rate': 0.0, 'epoch': 1.4}\n",
      "100%|██████████████████████████████████| 20000/20000 [10:34:34<00:00,  1.87s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.52s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:34<00:13, 13.05s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.350559999999994, 'eval_rouge-2': 8.481152000000002, 'eval_rouge-l': 25.409054, 'eval_bleu-4': 0.03804105549654653, 'eval_runtime': 63.1502, 'eval_samples_per_second': 0.792, 'eval_steps_per_second': 0.063, 'epoch': 1.4}\n",
      "100%|██████████████████████████████████| 20000/20000 [10:35:37<00:00,  1.87s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:36<00:00,  9.19s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-20000\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../../chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 38138.09, 'train_samples_per_second': 4.195, 'train_steps_per_second': 0.524, 'train_loss': 3.35061484375, 'epoch': 1.4}\n",
      "100%|██████████████████████████████████| 20000/20000 [10:35:38<00:00,  1.91s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [08:38<00:00,  7.74s/it]\n"
     ]
    }
   ],
   "source": [
    "!python finetune_hf.py  data/AdvertiseGen_fix ../../chatglm3-6b configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-04T03:53:15.637707Z",
     "iopub.status.busy": "2024-04-04T03:53:15.637375Z",
     "iopub.status.idle": "2024-04-04T03:53:15.833237Z",
     "shell.execute_reply": "2024-04-04T03:53:15.832512Z",
     "shell.execute_reply.started": "2024-04-04T03:53:15.637687Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 行 1: /media/zr/Data/Code/ChatGLM3/venv/bin/python3: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 /media/zr/Data/Code/ChatGLM3/venv/bin/python3 finetune_hf.py  data/AdvertiseGen_fix  /media/zr/Data/Models/LLM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {},
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-04T17:45:22.393051Z",
     "iopub.status.busy": "2024-04-04T17:45:22.392734Z",
     "iopub.status.idle": "2024-04-04T17:45:22.592160Z",
     "shell.execute_reply": "2024-04-04T17:45:22.591560Z",
     "shell.execute_reply.started": "2024-04-04T17:45:22.393033Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-10000  checkpoint-16000  checkpoint-20000  checkpoint-8000\n",
      "checkpoint-12000  checkpoint-18000  checkpoint-4000   runs\n",
      "checkpoint-14000  checkpoint-2000   checkpoint-6000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-04T17:45:35.893307Z",
     "iopub.status.busy": "2024-04-04T17:45:35.892958Z",
     "iopub.status.idle": "2024-04-04T17:45:36.089175Z",
     "shell.execute_reply": "2024-04-04T17:45:36.088502Z",
     "shell.execute_reply.started": "2024-04-04T17:45:35.893286Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 行 1: /media/zr/Data/Code/ChatGLM3/venv/bin/python3: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  /media/zr/Data/Code/ChatGLM3/venv/bin/python3 inference_hf.py output/checkpoint-3000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140bf8c7-7b24-4794-b6c7-c10770e118d3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-04T17:48:21.287648Z",
     "iopub.status.busy": "2024-04-04T17:48:21.287323Z",
     "iopub.status.idle": "2024-04-04T17:48:48.168848Z",
     "shell.execute_reply": "2024-04-04T17:48:48.168243Z",
     "shell.execute_reply.started": "2024-04-04T17:48:21.287630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 01:48:25.781920: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-05 01:48:25.784288: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 01:48:25.816872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-05 01:48:25.816920: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-05 01:48:25.816948: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-05 01:48:25.823141: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 01:48:25.823362: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 01:48:26.593075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:13<00:00,  1.89s/it]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "这款连衣裙采用木耳边拼接的网纱设计，穿起来更具有气质感，而且透视网纱设计更性感。不规则的裙摆拼接，更显灵动与优雅，腰间压褶的版型，穿起来更显瘦。后背拉链的设计，穿脱更方便。\n"
     ]
    }
   ],
   "source": [
    "!python inference_hf.py output/checkpoint-20000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
